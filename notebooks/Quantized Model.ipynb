{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49165a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the directory exists\n",
    "import os\n",
    "if os.path.exists(\"./Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting\"):\n",
    "    pass\n",
    "else:\n",
    "    !git clone https://github.com/abekek/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea23a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/alibek_2/Quantized-SHO-Fitting\n",
      "Cloning into 'Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting'...\n",
      "remote: Enumerating objects: 521, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 521 (delta 6), reused 10 (delta 2), pack-reused 501\u001b[K\n",
      "Receiving objects: 100% (521/521), 139.87 MiB | 72.37 MiB/s, done.\n",
      "Resolving deltas: 100% (271/271), done.\n"
     ]
    }
   ],
   "source": [
    "# Checks if the directory exists\n",
    "import os\n",
    "if os.path.exists(\"./Quantized-SHO-Fitting\"):\n",
    "    %cd Quantized-SHO-Fitting\n",
    "else:\n",
    "    %mkdir Quantized-SHO-Fitting\n",
    "    %cd Quantized-SHO-Fitting\n",
    "    !git clone https://github.com/abekek/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d2c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/alibek_2/Quantized-SHO-Fitting/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting\n"
     ]
    }
   ],
   "source": [
    "# moves to the right directory\n",
    "%cd Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca15c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Pulling without specifying how to reconcile divergent branches is\n",
      "discouraged. You can squelch this message by running one of the following\n",
      "commands sometime before your next pull:\n",
      "\n",
      "  git config pull.rebase false  # merge (the default strategy)\n",
      "  git config pull.rebase true   # rebase\n",
      "  git config pull.ff only       # fast-forward only\n",
      "\n",
      "You can replace \"git config\" with \"git config --global\" to set a default\n",
      "preference for all repositories. You can also pass --rebase, --no-rebase,\n",
      "or --ff-only on the command line to override the configured default per\n",
      "invocation.\n",
      "\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "# checks if the directory is up to date\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f682eb2",
   "metadata": {},
   "source": [
    "## Installation & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542d089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting atomicwrites==1.4.0\n",
      "  Downloading atomicwrites-1.4.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting autopep8==1.5.4\n",
      "  Downloading autopep8-1.5.4.tar.gz (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi==2020.12.5\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 65.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet==4.0.0\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 98.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama==0.4.4\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting coverage==5.3.1\n",
      "  Downloading coverage-5.3.1-cp38-cp38-manylinux2010_x86_64.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 105.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docopt==0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting idna==2.10\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting iniconfig==1.1.1\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: numpy==1.19.5 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.19.5)\n",
      "Collecting packaging==20.8\n",
      "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
      "Collecting Pillow==8.1.0\n",
      "  Downloading Pillow-8.1.0-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 99.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pipreqs==0.4.10\n",
      "  Downloading pipreqs-0.4.10-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pluggy==0.13.1\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting py==1.10.0\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 12.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pycodestyle==2.6.0\n",
      "  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 506 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing==2.4.7 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (2.4.7)\n",
      "Collecting pytest==6.2.1\n",
      "  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
      "\u001b[K     |████████████████████████████████| 279 kB 98.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest-cov==2.11.1\n",
      "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting requests==2.25.1\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 8.1 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting toml==0.10.2\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting gdown==3.12.2\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions==3.7.4.3 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (3.7.4.3)\n",
      "Collecting urllib3==1.26.2\n",
      "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 106.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarg==0.1.9\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy_groupies==0.9.7 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 27)) (0.9.7)\n",
      "Collecting dask==2.12.0\n",
      "  Downloading dask-2.12.0-py3-none-any.whl (789 kB)\n",
      "\u001b[K     |████████████████████████████████| 789 kB 60.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xlrd==1.1.0\n",
      "  Downloading xlrd-1.1.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 105.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==2.4.1\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "\u001b[K     |███████████████████████▏        | 285.2 MB 135.3 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 394.4 MB 86 kB/s \n",
      "\u001b[?25hRequirement already satisfied: sidpy==0.0.5 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (0.0.5)\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (7.6.3)\n",
      "Collecting scikit_image==0.16.2\n",
      "  Downloading scikit_image-0.16.2-cp38-cp38-manylinux1_x86_64.whl (26.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.5 MB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib==1.0.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (1.0.1)\n",
      "Collecting pip==19.3.1\n",
      "  Downloading pip-19.3.1-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 105.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipython==7.22.0\n",
      "  Downloading ipython-7.22.0-py3-none-any.whl (785 kB)\n",
      "\u001b[K     |████████████████████████████████| 785 kB 112.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyqtgraph==0.12.1\n",
      "  Downloading pyqtgraph-0.12.1-py3-none-any.whl (939 kB)\n",
      "\u001b[K     |████████████████████████████████| 939 kB 111.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyUSID==0.0.10 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (0.0.10)\n",
      "Collecting scikit_learn==0.24.1\n",
      "  Downloading scikit_learn-0.24.1-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.9 MB 100.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinx_rtd_theme==0.5.2\n",
      "  Downloading sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 104.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wget==3.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 41)) (3.2)\n",
      "Collecting BGlib==0.0.3\n",
      "  Downloading BGlib-0.0.3-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[K     |████████████████████████████████| 189 kB 111.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pycroscopy==0.60.7 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (0.60.7)\n",
      "Collecting matplotlib==3.2.2\n",
      "  Downloading matplotlib-3.2.2-cp38-cp38-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 35.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting moviepy==0.2.3.5\n",
      "  Downloading moviepy-0.2.3.5.tar.gz (372 kB)\n",
      "\u001b[K     |████████████████████████████████| 372 kB 113.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py==2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 40.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pygame==2.0.1\n",
      "  Downloading pygame-2.0.1-cp38-cp38-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 99.2 MB/s eta 0:00:01\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0.dev20210415+cu101 (from -r requirements.txt (line 49)) (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==1.9.0.dev20210415+cu101 (from -r requirements.txt (line 49))\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Installs all of the requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8488bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |█████████████▌                  | 317.9 MB 132.0 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |█████████████████████████████   | 680.7 MB 5.3 MB/s eta 0:00:14"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 750.6 MB 54 kB/s \n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 40.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: requests in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from torchvision) (9.0.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->torchvision) (1.26.6)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->torchvision) (2021.5.30)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu111\n",
      "    Uninstalling torch-1.9.0+cu111:\n",
      "      Successfully uninstalled torch-1.9.0+cu111\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.10.0+cu111\n",
      "    Uninstalling torchvision-0.10.0+cu111:\n",
      "      Successfully uninstalled torchvision-0.10.0+cu111\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "torchaudio 0.9.0 requires torch==1.9.0, but you'll have torch 1.11.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.11.0 torchvision-0.12.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pycroscopy==0.60.7 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (0.60.7)\n",
      "Requirement already satisfied: pyUSID>=0.0.8 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.0.10)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (3.4.3)\n",
      "Requirement already satisfied: six in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (1.15.0)\n",
      "Requirement already satisfied: sidpy>=0.0.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.0.5)\n",
      "Requirement already satisfied: pillow in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (9.0.0)\n",
      "Requirement already satisfied: joblib>=0.11.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (1.0.1)\n",
      "Requirement already satisfied: ipython>=6.0; python_version >= \"3.3\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (7.26.0)\n",
      "Requirement already satisfied: ipywidgets>=5.2.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (7.6.3)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (1.7.1)\n",
      "Requirement already satisfied: gwyfile in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (1.19.5)\n",
      "Requirement already satisfied: psutil in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (5.8.0)\n",
      "Requirement already satisfied: igor in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.3)\n",
      "Requirement already satisfied: scikit-image>=0.12.3 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.18.2)\n",
      "Requirement already satisfied: xlrd>=1.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (2.0.1)\n",
      "Requirement already satisfied: numpy-groupies==0.9.7 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.9.7)\n",
      "Requirement already satisfied: h5py>=2.6.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pycroscopy==0.60.7) (0.24.2)\n",
      "Requirement already satisfied: dask>=0.10 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (2021.8.0)\n",
      "Requirement already satisfied: cytoolz in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (0.11.0)\n",
      "Requirement already satisfied: toolz in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (0.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (2.4.7)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (4.8.0)\n",
      "Requirement already satisfied: decorator in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (3.0.19)\n",
      "Requirement already satisfied: backcall in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (49.2.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (5.0.5)\n",
      "Requirement already satisfied: matplotlib-inline in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.1.2)\n",
      "Requirement already satisfied: pygments in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.0.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (6.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.1.3)\n",
      "Requirement already satisfied: networkx>=2.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.6.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2021.8.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-learn>=0.17.1->pycroscopy==0.60.7) (2.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from dask>=0.10->pyUSID>=0.0.8->pycroscopy==0.60.7) (2021.7.0)\n",
      "Requirement already satisfied: pyyaml in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from dask>=0.10->pyUSID>=0.0.8->pycroscopy==0.60.7) (5.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from dask>=0.10->pyUSID>=0.0.8->pycroscopy==0.60.7) (1.6.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from dask>=0.10->pyUSID>=0.0.8->pycroscopy==0.60.7) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from dask>=0.10->pyUSID>=0.0.8->pycroscopy==0.60.7) (21.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from traitlets>=4.2->ipython>=6.0; python_version >= \"3.3\"->pycroscopy==0.60.7) (0.2.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (7.0.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.4.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (6.4.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.7.1)\n",
      "Requirement already satisfied: locket in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from partd>=0.3.10->dask>=0.10->pyUSID>=0.0.8->pycroscopy==0.60.7) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.5.1)\n",
      "Requirement already satisfied: entrypoints in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (22.2.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (20.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.11.1)\n",
      "Requirement already satisfied: prometheus-client in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.11.0)\n",
      "Requirement already satisfied: nbconvert in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (6.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (2.0.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.14.6)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.8.4)\n",
      "Requirement already satisfied: bleach in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.0.0)\n",
      "Requirement already satisfied: defusedxml in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.1.2)\n",
      "Requirement already satisfied: testpath in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.5.4)\n",
      "Requirement already satisfied: pycparser in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (2.20)\n",
      "Requirement already satisfied: webencodings in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Cloning into 'BGlib'...\n",
      "remote: Enumerating objects: 2165, done.\u001b[K\n",
      "remote: Counting objects: 100% (689/689), done.\u001b[K\n",
      "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
      "remote: Total 2165 (delta 539), reused 658 (delta 529), pack-reused 1476\u001b[K\n",
      "Receiving objects: 100% (2165/2165), 11.78 MiB | 21.02 MiB/s, done.\n",
      "Resolving deltas: 100% (1375/1375), done.\n",
      "/scratch/alibek_2/Quantized-SHO-Fitting/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/BGlib\n",
      "0.0.2\n",
      "0.0.3\n",
      "v0.0.0\n",
      "v0.0.1\n",
      "v0.0.3\n",
      "Note: switching to '0.0.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at ea3a41d Micro version bump\n",
      "Deleted branch master (was bd1eb5b).\n",
      "Switched to a new branch 'master'\n",
      "/scratch/alibek_2/Quantized-SHO-Fitting/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting\n",
      "Collecting h5py==2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting numpy>=1.7\n",
      "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.9 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: six, numpy, h5py\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.6.0 requires h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\n",
      "tensorflow 2.6.0 requires numpy~=1.19.2, but you'll have numpy 1.22.4 which is incompatible.\n",
      "tensorflow 2.6.0 requires six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
      "Successfully installed h5py-2.10.0 numpy-1.22.4 six-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing PyTorch's Nightly version\n",
    "!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html -U\n",
    "\n",
    "!pip install pycroscopy==0.60.7\n",
    "\n",
    "if os.path.exists(\"./BGlib\"):\n",
    "    pass\n",
    "else:\n",
    "    !git clone https://github.com/pycroscopy/BGlib.git\n",
    "    %cd BGlib/\n",
    "    !git tag -l\n",
    "    !git checkout 0.0.3\n",
    "    !git branch -D master\n",
    "    !git checkout -b master\n",
    "    %cd ..\n",
    "\n",
    "# downgrading the h5py version\n",
    "!pip install 'h5py==2.10.0' --force-reinstal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5448f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hls4ml\n",
      "  Downloading hls4ml-0.6.0-py3-none-any.whl (295 kB)\n",
      "\u001b[K     |████████████████████████████████| 295 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from hls4ml) (2.10.0)\n",
      "Requirement already satisfied: numpy in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from hls4ml) (1.22.4)\n",
      "Collecting onnx>=1.4.0\n",
      "  Downloading onnx-1.11.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 38.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from hls4ml) (5.4.1)\n",
      "Requirement already satisfied: six in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from hls4ml) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from onnx>=1.4.0->hls4ml) (3.17.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from onnx>=1.4.0->hls4ml) (3.7.4.3)\n",
      "Installing collected packages: onnx, hls4ml\n",
      "Successfully installed hls4ml-0.6.0 onnx-1.11.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hls4ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2295d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.55.2-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from numba) (49.2.1)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 33.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.23,>=1.18 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from numba) (1.22.4)\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.38.1 numba-0.55.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d22a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 12:32:55.396052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:32:55.396099: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numba import jit\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from scipy.signal import resample\n",
    "from scipy import fftpack\n",
    "from scipy import io\n",
    "from scipy import special\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.layers import (Attention, Dense, Conv1D, Convolution2D, \n",
    "                                     GRU, LSTM, Bidirectional, TimeDistributed,\n",
    "                                     Dropout, Flatten, LayerNormalization, \n",
    "                                     RepeatVector, Reshape, MaxPooling1D, \n",
    "                                     UpSampling1D, BatchNormalization, Activation)\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from multiprocessing import Pool, Process\n",
    "import multiprocessing as mp\n",
    "from moviepy.editor import *\n",
    "import glob\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "import sidpy\n",
    "from BGlib.BGlib import be as belib\n",
    " \n",
    "# set up notebook to show plots within the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "# Import necessary libraries:\n",
    "# General utilities:\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Computation:\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from IPython.display import clear_output\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# Finally, pycroscopy itself\n",
    "sys.path.append('../../../')\n",
    "import pyUSID as usid\n",
    "from codes.util.preprocessing_global_standard_scaler import global_standard_scaler\n",
    "from sidpy.hdf.hdf_utils import write_simple_attrs, get_attr\n",
    "from pyUSID.io.hdf_utils import create_results_group, write_main_dataset, write_reduced_anc_dsets, create_empty_dataset, reshape_to_n_dims, get_auxiliary_datasets\n",
    "from pyUSID.io.usi_data import USIDataset\n",
    "from pyUSID.io import Dimension\n",
    "\n",
    "from codes.util.file import print_tree\n",
    "from codes.util.core import SHO_fit_func_torch, loop_fitting_function, loop_fitting_function_tf, computeDotProducts, normOfVar, fit_loop_function, computeTime, conventional_fit_loop_function\n",
    "from codes.viz.plot import plot_best_worst_SHO, make_movie, plot_best_worst_loops, plot_reconstruction_comparison_SHO, plot_reconstruction_comparison_loops\n",
    "from codes.util.postprocessing import transform_params, convert_real_imag\n",
    "from codes.util.preprocessing_global_scaler import global_scaler\n",
    "from codes.processing.filters import range_filter, clean_interpolate, interpolate_missing_points\n",
    "from codes.algorithm.TRPCGOptimizerv2 import TRPCGOptimizerv2\n",
    "from codes.algorithm.AdaHessian import AdaHessian\n",
    "\n",
    "import numpy.lib.recfunctions as rfn\n",
    "\n",
    "import hls4ml\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Activation, MaxPool1D, AvgPool1D, Flatten, Dense\n",
    "from tensorflow.nn import selu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d4d8a",
   "metadata": {},
   "source": [
    "## Setting defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e638df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the number of CPU cores\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42f1c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 15 12:34:15 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      Off  | 00000000:2F:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    59W / 400W |  28316MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-SXM4-40GB      Off  | 00000000:30:00.0 Off |                    0 |\n",
      "| N/A   24C    P0    55W / 400W |  34986MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-SXM4-40GB      Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   51C    P0   320W / 400W |  36226MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-SXM4-40GB      Off  | 00000000:B0:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    77W / 400W |  22889MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    358348      C   ...onda3/envs/gyc/bin/python     2879MiB |\n",
      "|    0   N/A  N/A    925041      C   ..._2/alibek_env/bin/python3     2031MiB |\n",
      "|    0   N/A  N/A   3068144      C   ...onda3/envs/gyc/bin/python     1899MiB |\n",
      "|    0   N/A  N/A   3087085      C   ...onda3/envs/gyc/bin/python    18439MiB |\n",
      "|    1   N/A  N/A   1906463      C   ...da3/envs/shuyu/bin/python    34983MiB |\n",
      "|    2   N/A  N/A     65260      C   ...da3/envs/shuyu/bin/python    36223MiB |\n",
      "|    3   N/A  N/A   1638178      C   ...onda3/envs/gyc/bin/python     7647MiB |\n",
      "|    3   N/A  N/A   3211742      C   ...onda3/envs/gyc/bin/python    15239MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# shows the GPU that is available and the resources\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10bf5ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 12:34:18.087915: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.096563: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.102790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.123646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.130708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.136585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.142759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.149413: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/Apps/lusoft/opt/spack/linux-centos8-haswell/gcc-8.3.1/python/3.8.6-lesvfst/lib:/share/Apps/lusoft/opt/spack/linux-centos8-haswell/intel-20.0.3/mpich/3.3.2-n7f36fo/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/compiler/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/libfabric/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib/release:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mpi/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/ipp/lib/intel64:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/mkl/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/tbb/lib/intel64/gcc4.8:/share/Apps/intel/2020/debugger_2020/python/intel64/lib:/share/Apps/intel/2020/debugger_2020/libipt/intel64/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/lib/intel64_lin:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/gcc-8.3.1/python/3.8.6-cjy2mt3/lib:/share/Apps/lusoft/opt/spack/linux-centos8-ivybridge/intel-20.0.3/mvapich2/2.3.4-5p7hm6w/lib:/share/Apps/lusoft/opt/spack/linux-centos8-skylake_avx512/intel-20.0.3/mpich/3.3.2-u7dnjec/lib:/share/Apps/intel/2020/compilers_and_libraries_2020.3.275/linux/lib\n",
      "2022-06-15 12:34:18.149433: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a395ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes the random seed for reproducible training\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a007a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting default seaborn style\n",
    "sns.reset_orig()\n",
    "\n",
    "# setting default plotting params\n",
    "plt.rcParams['image.cmap'] = 'magma'\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.top'] = True\n",
    "plt.rcParams['ytick.right'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccef33",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f240a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /scratch/alibek_2/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/data_file.h5 /scratch/alibek_2/Quantized-SHO-Fitting/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/data_file.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8823161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "├ Measurement_000\n",
      "  ---------------\n",
      "  ├ Channel_000\n",
      "    -----------\n",
      "    ├ Bin_FFT\n",
      "    ├ Bin_Frequencies\n",
      "    ├ Bin_Indices\n",
      "    ├ Bin_Step\n",
      "    ├ Bin_Wfm_Type\n",
      "    ├ Excitation_Waveform\n",
      "    ├ Noise_Floor\n",
      "    ├ Position_Indices\n",
      "    ├ Position_Values\n",
      "    ├ Raw_Data\n",
      "    ├ Raw_Data-SHO_Fit_000\n",
      "      --------------------\n",
      "      ├ Fit\n",
      "      ├ Guess\n",
      "      ├ Spectroscopic_Indices\n",
      "      ├ Spectroscopic_Values\n",
      "      ├ completed_fit_positions\n",
      "      ├ completed_guess_positions\n",
      "    ├ Spatially_Averaged_Plot_Group_000\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Max_Response\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Min_Response\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spatially_Averaged_Plot_Group_001\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Max_Response\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Min_Response\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spectroscopic_Indices\n",
      "    ├ Spectroscopic_Values\n",
      "    ├ UDVS\n",
      "    ├ UDVS_Indices\n"
     ]
    }
   ],
   "source": [
    "# Opens the translated file\n",
    "h5_f = h5py.File('./data_file.h5', 'r+')\n",
    "\n",
    "#Inspects the h5 file\n",
    "usid.hdf_utils.print_tree(h5_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b43f7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and datagroups within the file:\n",
      "------------------------------------\n",
      "/\n",
      "/Measurement_000\n",
      "/Measurement_000/Channel_000\n",
      "/Measurement_000/Channel_000/Bin_FFT\n",
      "/Measurement_000/Channel_000/Bin_Frequencies\n",
      "/Measurement_000/Channel_000/Bin_Indices\n",
      "/Measurement_000/Channel_000/Bin_Step\n",
      "/Measurement_000/Channel_000/Bin_Wfm_Type\n",
      "/Measurement_000/Channel_000/Excitation_Waveform\n",
      "/Measurement_000/Channel_000/Noise_Floor\n",
      "/Measurement_000/Channel_000/Position_Indices\n",
      "/Measurement_000/Channel_000/Position_Values\n",
      "/Measurement_000/Channel_000/Raw_Data\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Fit\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Indices\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_fit_positions\n",
      "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_guess_positions\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Bin_Frequencies\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Max_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Mean_Spectrogram\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Min_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Spectroscopic_Parameter\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Step_Averaged_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Bin_Frequencies\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Max_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Mean_Spectrogram\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Min_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Spectroscopic_Parameter\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Step_Averaged_Response\n",
      "/Measurement_000/Channel_000/Spectroscopic_Indices\n",
      "/Measurement_000/Channel_000/Spectroscopic_Values\n",
      "/Measurement_000/Channel_000/UDVS\n",
      "/Measurement_000/Channel_000/UDVS_Indices\n",
      "\n",
      "The main dataset:\n",
      "------------------------------------\n",
      "<HDF5 file \"data_file.h5\" (mode r+)>\n",
      "\n",
      "The ancillary datasets:\n",
      "------------------------------------\n",
      "<HDF5 dataset \"Position_Indices\": shape (3600, 2), type \"<u4\">\n",
      "<HDF5 dataset \"Position_Values\": shape (3600, 2), type \"<f4\">\n",
      "<HDF5 dataset \"Spectroscopic_Indices\": shape (4, 63360), type \"<u4\">\n",
      "<HDF5 dataset \"Spectroscopic_Values\": shape (4, 63360), type \"<f4\">\n",
      "\n",
      "Metadata or attributes in a datagroup\n",
      "------------------------------------\n",
      "BE_actual_duration_[s] : 0.004\n",
      "BE_amplitude_[V] : 1\n",
      "BE_auto_smoothing : auto smoothing on\n",
      "BE_band_edge_smoothing_[s] : 4832.1\n",
      "BE_band_edge_trim : 0.094742\n",
      "BE_band_width_[Hz] : 200000\n",
      "BE_bins_per_band : 0\n",
      "BE_center_frequency_[Hz] : 1310000\n",
      "BE_desired_duration_[s] : 0.004\n",
      "BE_phase_content : chirp-sinc hybrid\n",
      "BE_phase_variation : 1\n",
      "BE_points_per_BE_wave : 0\n",
      "BE_repeats : 4\n",
      "FORC_V_high1_[V] : 1\n",
      "FORC_V_high2_[V] : 10\n",
      "FORC_V_low1_[V] : -1\n",
      "FORC_V_low2_[V] : -10\n",
      "FORC_num_of_FORC_cycles : 1\n",
      "FORC_num_of_FORC_repeats : 1\n",
      "File_MDAQ_version : MDAQ_VS_090915_01\n",
      "File_date_and_time : 18-Sep-2015 18:32:14\n",
      "File_file_name : SP128_NSO\n",
      "File_file_path : C:\\Users\\Asylum User\\Documents\\Users\\Agar\\SP128_NSO\\\n",
      "File_file_suffix : 99\n",
      "IO_AO_amplifier : 10\n",
      "IO_AO_range_[V] : +/- 10\n",
      "IO_Analog_Input_1 : +/- .1V, FFT\n",
      "IO_Analog_Input_2 : off\n",
      "IO_Analog_Input_3 : off\n",
      "IO_Analog_Input_4 : off\n",
      "IO_DAQ_platform : NI 6115\n",
      "IO_rate_[Hz] : 4000000\n",
      "VS_amplitude_[V] : 16\n",
      "VS_cycle_fraction : full\n",
      "VS_cycle_phase_shift : 0\n",
      "VS_measure_in_field_loops : in and out-of-field\n",
      "VS_mode : DC modulation mode\n",
      "VS_number_of_cycles : 2\n",
      "VS_offset_[V] : 0\n",
      "VS_read_voltage_[V] : 0\n",
      "VS_set_pulse_amplitude[V] : 0\n",
      "VS_set_pulse_duration[s] : 0.002\n",
      "VS_step_edge_smoothing_[s] : 0.001\n",
      "VS_steps_per_full_cycle : 96\n",
      "data_type : BEPSData\n",
      "grid_/single : grid\n",
      "grid_contact_set_point_[V] : 1\n",
      "grid_current_col : 1\n",
      "grid_current_row : 1\n",
      "grid_cycle_time_[s] : 10\n",
      "grid_measuring : 0\n",
      "grid_moving : 0\n",
      "grid_num_cols : 60\n",
      "grid_num_rows : 60\n",
      "grid_settle_time_[s] : 0.15\n",
      "grid_time_remaining_[h;m;s] : 10\n",
      "grid_total_time_[h;m;s] : 10\n",
      "grid_transit_set_point_[V] : 0.1\n",
      "grid_transit_time_[s] : 0.15\n",
      "num_bins : 165\n",
      "num_pix : 3600\n",
      "num_udvs_steps : 384\n"
     ]
    }
   ],
   "source": [
    "print('Datasets and datagroups within the file:\\n------------------------------------')\n",
    "print_tree(h5_f.file)\n",
    " \n",
    "print('\\nThe main dataset:\\n------------------------------------')\n",
    "print(h5_f)\n",
    "print('\\nThe ancillary datasets:\\n------------------------------------')\n",
    "print(h5_f.file['/Measurement_000/Channel_000/Position_Indices'])\n",
    "print(h5_f.file['/Measurement_000/Channel_000/Position_Values'])\n",
    "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Indices'])\n",
    "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Values'])\n",
    "\n",
    "print('\\nMetadata or attributes in a datagroup\\n------------------------------------')\n",
    "for key in h5_f.file['/Measurement_000'].attrs:\n",
    "    print('{} : {}'.format(key, h5_f.file['/Measurement_000'].attrs[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ef928e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples per SHO fit\n",
    "num_bins = h5_f['Measurement_000'].attrs['num_bins'] \n",
    "\n",
    "# number of pixels in the image\n",
    "num_pix = h5_f['Measurement_000'].attrs['num_pix'] \n",
    "\n",
    "# number of pixels in x and y dimensions\n",
    "num_pix_1d = int(np.sqrt(num_pix)) \n",
    "\n",
    "# number of DC voltage steps \n",
    "voltage_steps = h5_f['Measurement_000'].attrs['num_udvs_steps']\n",
    "\n",
    "# sampling rate\n",
    "sampling_rate = h5_f['Measurement_000'].attrs['IO_rate_[Hz]']\n",
    "\n",
    "# BE bandwidth\n",
    "be_bandwidth = h5_f['Measurement_000'].attrs['BE_band_width_[Hz]']\n",
    "\n",
    "# BE center frequency\n",
    "be_center_frequency = h5_f['Measurement_000'].attrs['BE_center_frequency_[Hz]']\n",
    "\n",
    "# Frequency Vector in Hz\n",
    "frequency_bin = h5_f['Measurement_000']['Channel_000']['Bin_Frequencies'][:]\n",
    "\n",
    "# Resampled frequency vector\n",
    "wvec_freq = resample(frequency_bin, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "944ebbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data (real and imaginary combined)\n",
    "raw_data = h5_f['Measurement_000']['Channel_000']['Raw_Data']\n",
    "# resampling it from 165 to 80 frequency steps\n",
    "raw_data_resampled = resample(np.array(raw_data).reshape(-1 , 165), 80, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb1ba4",
   "metadata": {},
   "source": [
    "## Conversion of Real/Imaginary to Magnitude/Phase Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f85b0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of raw data (both resampled and full)\n",
    "magnitude_graph_initial_full, phase_graph_initial_full = convert_real_imag(raw_data)\n",
    "magnitude_graph_initial, phase_graph_initial = convert_real_imag(raw_data_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ce769",
   "metadata": {},
   "source": [
    "## Resampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d63f0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get real and imaginary components from raw data\n",
    "real = np.real(h5_f['Measurement_000']['Channel_000']['Raw_Data'])\n",
    "imag = np.imag(h5_f['Measurement_000']['Channel_000']['Raw_Data'])\n",
    "\n",
    "# resample both real and imaginary components \n",
    "real_resample = resample(real.reshape(num_pix, -1, num_bins), 80, axis=2)\n",
    "imag_resample = resample(imag.reshape(num_pix, -1, num_bins), 80, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5932a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up the RAM\n",
    "del real\n",
    "del imag\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c435249",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd752db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for parameters\n",
    "fit_results_list = []\n",
    "for sublist in np.array(h5_f['Measurement_000']['Channel_000']['Raw_Data-SHO_Fit_000']['Fit']):\n",
    "    for item in sublist:\n",
    "        for i in item:\n",
    "          fit_results_list.append(i)\n",
    "\n",
    "# flatten parameters list into numpy array\n",
    "fit_results_list = np.array(fit_results_list).reshape(num_pix,voltage_steps,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85cd0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the fit results with Standard Scaler\n",
    "fit_results_scaler = StandardScaler()\n",
    "scaled_fit_results = fit_results_scaler.fit_transform(fit_results_list.reshape(-1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7122e3",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64f86b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean =  -6.855169e-06 STD =  0.0026878386\n",
      "mean =  0.00013161483 STD =  0.0027575183\n",
      "(1382400, 4)\n"
     ]
    }
   ],
   "source": [
    "# scale the real component of input data\n",
    "scaler_real = global_standard_scaler()\n",
    "scaled_data_real = scaler_real.fit_transform(real_resample).reshape(-1, 80)\n",
    "\n",
    "# scale the imaginary component of input data\n",
    "scaler_imag = global_standard_scaler()\n",
    "scaled_data_imag = scaler_imag.fit_transform(imag_resample).reshape(-1, 80)\n",
    "\n",
    "# stack both components\n",
    "data_ = np.stack((scaled_data_real, scaled_data_imag),axis=2)\n",
    "\n",
    "# scale the parameters (now takes only 4 parameters, excluding the R2)\n",
    "params_scaler = StandardScaler()\n",
    "scaled_params = params_scaler.fit_transform(fit_results_list.reshape(-1,5)[:,0:4])\n",
    "\n",
    "# exclude the R2 parameter\n",
    "params = fit_results_list.reshape(-1,5)[:,0:4]\n",
    "print(params.shape)\n",
    "\n",
    "del real_resample\n",
    "del imag_resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818aaa98",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67d05855",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, params_train, params_test = train_test_split(data_, \n",
    "                                                                    scaled_params, \n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4816ca7",
   "metadata": {},
   "source": [
    "## Tensorflow/HLS4ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "efbe6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce3281",
   "metadata": {},
   "source": [
    "### AdaHessian Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c2b85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.eager import def_function\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend_config\n",
    "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import training_ops\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "import abc\n",
    "import contextlib\n",
    "import functools\n",
    "\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.distribute import distribution_strategy_context as distribute_ctx\n",
    "from tensorflow.python.distribute import parameter_server_strategy\n",
    "from tensorflow.python.distribute import reduce_util as ds_reduce_util\n",
    "from tensorflow.python.distribute import values as ds_values\n",
    "from tensorflow.python.eager import backprop\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras.engine import base_layer_utils\n",
    "from tensorflow.python.keras.optimizer_v2 import learning_rate_schedule\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import gradients\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.saved_model import revived_types\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util import tf_inspect\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "class AdaHessian(optimizer_v2.OptimizerV2):\n",
    "\n",
    "    _HAS_AGGREGATE_GRAD = True\n",
    "\n",
    "    def __init__(self,\n",
    "               learning_rate=0.1,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.999,\n",
    "               epsilon=1e-4,\n",
    "               weight_decay = 0.,\n",
    "               hessian_power=1.0,\n",
    "               name='AdaHessian',\n",
    "               average_size_1d=None,\n",
    "               average_size_2d=None,\n",
    "               average_size_3d=-1,\n",
    "               average_size_4d=-1,\n",
    "               **kwargs):\n",
    "        \"\"\"Construct a new AdaHessian optimizer.\n",
    "        Args:\n",
    "            learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
    "            `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that\n",
    "            takes no arguments and returns the actual value to use, The learning\n",
    "            rate. Defaults to 0.1.\n",
    "            beta_1: A float value or a constant float tensor, or a callable that takes\n",
    "            no arguments and returns the actual value to use. The exponential decay\n",
    "            rate for the 1st moment estimates. Defaults to 0.9.\n",
    "            beta_2: A float value or a constant float tensor, or a callable that takes\n",
    "            no arguments and returns the actual value to use, The exponential decay\n",
    "            rate for the 2nd moment estimates. Defaults to 0.999.\n",
    "            epsilon: A small constant for numerical stability. This epsilon is\n",
    "            \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "            Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
    "            1e-7.\n",
    "            weight_decay: We are using AdamW's weight decay scheme. Defaults to 0.\n",
    "            name: Optional name for the operations created when applying gradients.\n",
    "            Defaults to \"Adam\".\n",
    "            hessian_power: Hessian power to control the optimizer more similar to first/second \n",
    "            order method (default: 1). You can also try 0.5. For some tasks we found this \n",
    "            to result in better performance.\n",
    "            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "            `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "            gradients by value, `decay` is included for backward compatibility to\n",
    "            allow time inverse decay of learning rate. `lr` is included for backward\n",
    "            compatibility, recommended to use `learning_rate` instead.\n",
    "            # average_size_{1,2,3,4}d: \n",
    "                        None: use no spatial averaging\n",
    "                        -1: use suggested spatial averaging (recommended for conv kernels)\n",
    "                        >= 1: use customized size\n",
    "        \"\"\"\n",
    "\n",
    "        super(AdaHessian, self).__init__(name, **kwargs)\n",
    "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "        self._set_hyper('decay', self._initial_decay)\n",
    "        self._set_hyper('beta_1', beta_1)\n",
    "        self._set_hyper('beta_2', beta_2)\n",
    "        self.epsilon = epsilon or backend_config.epsilon()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.hessian_power = hessian_power\n",
    "        self.average_size_1d = average_size_1d\n",
    "        self.average_size_2d = average_size_2d\n",
    "        self.average_size_3d = average_size_3d\n",
    "        self.average_size_4d = average_size_4d\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        # Create slots for the first and second moments.\n",
    "        # Separate for-loops to respect the ordering of slot variables from v1.\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v')\n",
    "\n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(AdaHessian, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\n",
    "        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "        lr = (\n",
    "            apply_state[(var_device, var_dtype)]['lr_t'] *\n",
    "            (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\n",
    "        apply_state[(var_device, var_dtype)].update(\n",
    "            dict(\n",
    "                lr=lr,\n",
    "                epsilon=ops.convert_to_tensor_v2(self.epsilon, var_dtype),\n",
    "                beta_1_t=beta_1_t,\n",
    "                beta_1_power=beta_1_power,\n",
    "                one_minus_beta_1_t=1 - beta_1_t,\n",
    "                beta_2_t=beta_2_t,\n",
    "                beta_2_power=beta_2_power,\n",
    "                one_minus_beta_2_t=1 - beta_2_t))\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        # If the weights are generated by Keras V1 optimizer, it includes vhats\n",
    "        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n",
    "        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[:len(params)]\n",
    "        super(AdaHessian, self).set_weights(weights)\n",
    "\n",
    "\n",
    "    def get_gradients_hessian(self, loss, params):\n",
    "        \"\"\"Returns gradients and Hessian of `loss` with respect to `params`.\n",
    "        Arguments:\n",
    "            loss: Loss tensor.\n",
    "            params: List of variables.\n",
    "        Returns:\n",
    "            List of gradient and Hessian tensors.\n",
    "        Raises:\n",
    "            ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
    "            function not implemented).\n",
    "        \"\"\"\n",
    "        params = nest.flatten(params)\n",
    "        with backend.get_graph().as_default(), backend.name_scope(self._name +\n",
    "                                                                    \"/gradients\"):\n",
    "            grads = gradients.gradients(loss, params)\n",
    "            for grad, param in zip(grads, params):\n",
    "                if grad is None:\n",
    "                    raise ValueError(\"Variable {} has `None` for gradient. \"\n",
    "                                    \"Please make sure that all of your ops have a \"\n",
    "                                    \"gradient defined (i.e. are differentiable). \"\n",
    "                                    \"Common ops without gradient: \"\n",
    "                                    \"K.argmax, K.round, K.eval.\".format(param))\n",
    "\n",
    "            # WARNING: for now we do not support gradient clip\n",
    "            # grads = self._clip_gradients(grads)\n",
    "\n",
    "            v = [np.random.uniform(0, 1, size = p.shape) for p in params]\n",
    "            for vi in v:\n",
    "                vi[ vi < 0.5] =  -1 \n",
    "                vi[ vi >= 0.5] =  1 \n",
    "            v = [tf.convert_to_tensor(vi, dtype = tf.dtypes.float32) for vi in v]\n",
    "\n",
    "            vprod = tf.reduce_sum([ tf.reduce_sum(vi * grad) for vi, grad in zip(v, grads)])\n",
    "\n",
    "            Hv = gradients.gradients(vprod, params)\n",
    "\n",
    "            Hd = [ tf.abs(Hvi * vi) for Hvi, vi in zip(Hv, v)]\n",
    "\n",
    "        return grads, Hd\n",
    "\n",
    "    def _filter_grads_hessian(self, grads_hessian_and_vars):\n",
    "        \"\"\"Filter out iterable with grad equal to None.\"\"\"\n",
    "        grads_hessian_and_vars = tuple(grads_hessian_and_vars)\n",
    "        if not grads_hessian_and_vars:\n",
    "            return grads_hessian_and_vars\n",
    "        filtered = []\n",
    "        vars_with_empty_grads = []\n",
    "        for grad, hessian, var in grads_hessian_and_vars:\n",
    "            if grad is None:\n",
    "                vars_with_empty_grads.append(var)\n",
    "            else:\n",
    "                filtered.append((grad, hessian, var))\n",
    "        filtered = tuple(filtered)\n",
    "\n",
    "        if not filtered:\n",
    "            raise ValueError(\"No gradients provided for any variable: %s.\" %\n",
    "                            ([v.name for _, v in grads_and_vars],))\n",
    "        if vars_with_empty_grads:\n",
    "            logging.warning(\n",
    "                (\"Gradients do not exist for variables %s when minimizing the loss.\"),\n",
    "                ([v.name for v in vars_with_empty_grads]))\n",
    "        return filtered\n",
    "\n",
    "    def apply_gradients_hessian(self,\n",
    "                      grads_hessian_and_vars,\n",
    "                      name=None,\n",
    "                      experimental_aggregate_gradients=True):\n",
    "        grads_hessian_and_vars = self._filter_grads_hessian(grads_hessian_and_vars)\n",
    "        var_list = [v for (_, _, v) in grads_hessian_and_vars]\n",
    "\n",
    "        with backend.name_scope(self._name):\n",
    "            # Create iteration if necessary.\n",
    "            with ops.init_scope():\n",
    "                self._create_all_weights(var_list)\n",
    "\n",
    "        if not grads_hessian_and_vars:\n",
    "            # Distribution strategy does not support reducing an empty list of\n",
    "            # gradients\n",
    "            return control_flow_ops.no_op()\n",
    "\n",
    "        if distribute_ctx.in_cross_replica_context():\n",
    "            raise RuntimeError(\n",
    "                \"`apply_gradients() cannot be called in cross-replica context. \"\n",
    "                \"Use `tf.distribute.Strategy.run` to enter replica \"\n",
    "                \"context.\")\n",
    "\n",
    "        strategy = distribute_ctx.get_strategy()\n",
    "        if (not experimental_aggregate_gradients and strategy and isinstance(\n",
    "            strategy.extended,\n",
    "            parameter_server_strategy.ParameterServerStrategyExtended)):\n",
    "            raise NotImplementedError(\n",
    "                \"`experimental_aggregate_gradients=False is not supported for \"\n",
    "                \"ParameterServerStrategy and CentralStorageStrategy\")\n",
    "\n",
    "        apply_state = self._prepare(var_list)\n",
    "        if experimental_aggregate_gradients:\n",
    "            reduced_grads, reduced_hessian = self._aggregate_gradients_hessian(grads_hessian_and_vars)\n",
    "            var_list = [v for _, _, v in grads_hessian_and_vars]\n",
    "            grads_hessian_and_vars = list(zip(reduced_grads, reduced_hessian, var_list))\n",
    "\n",
    "\n",
    "        return distribute_ctx.get_replica_context().merge_call(\n",
    "            functools.partial(self._distributed_apply, apply_state=apply_state),\n",
    "            args=(grads_hessian_and_vars,),\n",
    "            kwargs={\n",
    "                \"name\": name,\n",
    "            })\n",
    "\n",
    "    def _aggregate_gradients_hessian(self, grads_hessian_and_vars):\n",
    "        \"\"\"Returns all-reduced gradients.\n",
    "        Args:\n",
    "        grads_and_vars: List of (gradient, hessian, variable) pairs.\n",
    "        Returns:\n",
    "        Two lists of all-reduced gradients and Hessian.\n",
    "        \"\"\"\n",
    "        grads_hessian_and_vars = list(grads_hessian_and_vars)\n",
    "        filtered_grads_hessian_and_vars = self._filter_grads_hessian(grads_hessian_and_vars)\n",
    "\n",
    "        # split the list so that we can use the all_recude_fn\n",
    "        filtered_grads_and_vars = tuple([(g, v) for (g, h, v) in filtered_grads_hessian_and_vars])\n",
    "        filtered_hessian_and_vars = tuple([(h, v) for (g, h, v) in filtered_grads_hessian_and_vars])\n",
    "\n",
    "\n",
    "        def all_reduce_fn(distribution, grads_hessian_and_vars):\n",
    "            # WARNING: this ReduceOp.SUM can only support two entries, for now we have three.\n",
    "            # So far now, we do it for two steps to make life easier.\n",
    "            return distribution.extended.batch_reduce_to(\n",
    "                ds_reduce_util.ReduceOp.SUM, grads_hessian_and_vars)\n",
    "\n",
    "        if filtered_grads_hessian_and_vars:\n",
    "            reduced_part1 = distribute_ctx.get_replica_context().merge_call(\n",
    "                all_reduce_fn, args=(filtered_grads_and_vars,))\n",
    "            reduced_part2 = distribute_ctx.get_replica_context().merge_call(\n",
    "                all_reduce_fn, args=(filtered_hessian_and_vars,))\n",
    "        else:\n",
    "            reduced = []\n",
    "\n",
    "        # Copy 'reduced' but add None gradients back in\n",
    "        reduced_with_nones_grads = []\n",
    "        reduced_with_nones_hessian = []\n",
    "\n",
    "        reduced_pos = 0\n",
    "        for g, h, _ in grads_hessian_and_vars:\n",
    "            if g is None:\n",
    "                reduced_with_nones_grads.append( None )\n",
    "                reduced_with_nones_hessian.append( None )\n",
    "            else:\n",
    "                reduced_with_nones_grads.append(reduced_part1[reduced_pos])\n",
    "                reduced_with_nones_hessian.append(reduced_part2[reduced_pos])\n",
    "                reduced_pos += 1\n",
    "\n",
    "        return reduced_with_nones_grads, reduced_with_nones_hessian\n",
    "\n",
    "\n",
    "    @def_function.function(experimental_compile=True)\n",
    "    def _resource_apply_dense(self, grad, hess, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                        self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "\n",
    "        m.assign_add((grad - m) * (1 - coefficients['beta_1_t']))\n",
    "        # this part need to be changed for spatial averaging\n",
    "\n",
    "        if len(v.shape) == 1:\n",
    "            resize = self.average_size_1d\n",
    "        elif len(v.shape) == 2:\n",
    "            resize = self.average_size_2d\n",
    "        elif len(v.shape) == 3:\n",
    "            resize = self.average_size_3d\n",
    "        elif len(v.shape) == 4:\n",
    "            resize = self.average_size_4d\n",
    "        else:\n",
    "            raise Exception('You need to define the spatial average size by yourself!')\n",
    "\n",
    "        if resize == None:\n",
    "            v.assign_add((math_ops.square(hess) - v) * (1 - coefficients['beta_2_t']))\n",
    "        elif resize == -1:\n",
    "            if len(v.shape) == 1:\n",
    "                v.assign_add((math_ops.square(hess) - v) * (1 - coefficients['beta_2_t']))\n",
    "            elif len(v.shape) == 2:\n",
    "                hess_average = tf.reduce_mean(hess, [0], keepdims=True)\n",
    "                v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t'])) \n",
    "            elif len(v.shape) == 3:\n",
    "                hess_average = tf.reduce_mean(hess, [0], keepdims=True)\n",
    "                v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t'])) \n",
    "            elif len(v.shape) == 4:\n",
    "                hess_average = tf.reduce_mean(hess, [0, 1], keepdims=True)\n",
    "                v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t'])) \n",
    "        else:\n",
    "            if resize <= 0:\n",
    "                raise Exception('You need to define the spatial average size >= 1!')\n",
    "            hess_average = tf.reshape(hess, [resize, -1])\n",
    "            hess_average = tf.reduce_mean(hess_average, [0])\n",
    "            hess_average = tf.repeat(hess_average, resize)\n",
    "            hess_average = tf.reshape(hess_average, v.shape)\n",
    "            v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t']))\n",
    "        \n",
    "        bias_correct1 = 1 - coefficients['beta_1_power']\n",
    "        bias_correct2 = 1 - coefficients['beta_2_power']\n",
    "\n",
    "        if self.weight_decay != 0:\n",
    "            var.assign_sub(coefficients['lr_t'] * self.weight_decay * var)\n",
    "\n",
    "        # denom = np.power(math_ops.sqrt(v / bias_correct2), self.hessian_power) + coefficients['epsilon']\n",
    "        denom = tf.math.pow(math_ops.sqrt(v / bias_correct2), self.hessian_power) + coefficients['epsilon']\n",
    "\n",
    "\n",
    "        var.assign_sub( coefficients['lr_t'] * m / bias_correct1 / denom  )\n",
    "\n",
    "    @def_function.function(experimental_compile=True)\n",
    "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "        raise Exception('For now, we do not support sparse update yet.')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(AdaHessian, self).get_config()\n",
    "        config.update({\n",
    "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "            'decay': self._serialize_hyperparameter('decay'),\n",
    "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "            'epsilon': self.epsilon,\n",
    "            'weight_decay': self.weight_decay\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def _distributed_apply(self, distribution, grads_hessian_and_vars, name, apply_state):\n",
    "        \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n",
    "\n",
    "        def apply_grad_to_update_var(var, grad, hess):\n",
    "            \"\"\"Apply gradient to variable.\"\"\"\n",
    "            if isinstance(var, ops.Tensor):\n",
    "                raise NotImplementedError(\"Trying to update a Tensor \", var)\n",
    "\n",
    "            apply_kwargs = {}\n",
    "\n",
    "            if \"apply_state\" in self._dense_apply_args:\n",
    "                apply_kwargs[\"apply_state\"] = apply_state\n",
    "            update_op = self._resource_apply_dense(grad, hess, var, **apply_kwargs)\n",
    "\n",
    "            if var.constraint is not None:\n",
    "                with ops.control_dependencies([update_op]):\n",
    "                    return var.assign(var.constraint(var))\n",
    "            else:\n",
    "                return update_op\n",
    "\n",
    "        eagerly_outside_functions = ops.executing_eagerly_outside_functions()\n",
    "        update_ops = []\n",
    "        with ops.name_scope(name or self._name, skip_on_eager=True):\n",
    "            for grad, hess, var in grads_hessian_and_vars:\n",
    "\n",
    "                def _assume_mirrored(grad, hess):\n",
    "                    if isinstance(grad, ds_values.PerReplica):\n",
    "                        return ds_values.Mirrored(grad.values), ds_values.Mirrored(hess.values)\n",
    "                    return grad, hess\n",
    "\n",
    "                grad, hess = nest.map_structure(_assume_mirrored, grad, hess)\n",
    "                # Colocate the update with variables to avoid unnecessary communication\n",
    "                # delays. See b/136304694.\n",
    "                with distribution.extended.colocate_vars_with(var):\n",
    "                    with ops.name_scope(\"update\" if eagerly_outside_functions else\n",
    "                                  \"update_\" + var.op.name, skip_on_eager=True):\n",
    "                        update_ops.extend(distribution.extended.update(\n",
    "                                var, apply_grad_to_update_var, args=(grad, hess), group=False))\n",
    "\n",
    "            any_symbolic = any(isinstance(i, ops.Operation) or\n",
    "                             tf_utils.is_symbolic_tensor(i) for i in update_ops)\n",
    "            if not context.executing_eagerly() or any_symbolic:\n",
    "                # If the current context is graph mode or any of the update ops are\n",
    "                # symbolic then the step update should be carried out under a graph\n",
    "                # context. (eager updates execute immediately)\n",
    "                with ops._get_graph_from_inputs(update_ops).as_default():  # pylint: disable=protected-access\n",
    "                    with ops.control_dependencies(update_ops):\n",
    "                        return self._iterations.assign_add(1, read_value=False)\n",
    "\n",
    "            return self._iterations.assign_add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c09851",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b1887",
   "metadata": {},
   "source": [
    "This model is created using OOP; therefore, config file cannot be obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95a6e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Hidden_X1(keras.layers.Layer):\n",
    "#   def __init__(self):\n",
    "#     super(Hidden_X1, self).__init__()\n",
    "#     self.conv1_a = Conv1D(8, 7, padding='same')\n",
    "#     self.selu_a = Activation(selu)\n",
    "#     self.conv1_b = Conv1D(6, 7, padding='same')\n",
    "#     self.selu_b = Activation(selu)\n",
    "#     self.conv1_c = Conv1D(4, 5, padding='same')\n",
    "#     self.selu_c = Activation(selu)\n",
    "  \n",
    "#   def call(self, x):\n",
    "#     x = self.conv1_a(x)\n",
    "#     x = self.selu_a(x)\n",
    "#     x = self.conv1_b(x)\n",
    "#     x = self.selu_b(x)\n",
    "#     x = self.conv1_c(x)\n",
    "#     x = self.selu_c(x)\n",
    "#     return x\n",
    "  \n",
    "# class Hidden_XFC(keras.layers.Layer):\n",
    "#   def __init__(self):\n",
    "#     super(Hidden_XFC, self).__init__()\n",
    "#     self.dense_a = Dense(20, selu)\n",
    "#     self.dense_b = Dense(20, selu)\n",
    "\n",
    "#   def call(self, x):\n",
    "#     x = self.dense_a(x)\n",
    "#     x = self.dense_b(x)\n",
    "#     return x\n",
    "\n",
    "# class Hidden_X2(keras.layers.Layer):\n",
    "#   def __init__(self):\n",
    "#     super(Hidden_X2, self).__init__()\n",
    "#     self.max_pool_a = MaxPool1D(2, padding='same')\n",
    "#     self.conv1_d = Conv1D(2, 5, padding='same')\n",
    "#     self.selu_d = Activation(selu)\n",
    "#     self.conv1_e = Conv1D(4, 5, padding='same')\n",
    "#     self.selu_e = Activation(selu)\n",
    "#     self.conv1_f = Conv1D(4, 5, padding='same')\n",
    "#     self.selu_f = Activation(selu)\n",
    "#     self.conv1_g = Conv1D(4, 5, padding='same')\n",
    "#     self.selu_g = Activation(selu)\n",
    "#     self.conv1_h = Conv1D(4, 5, padding='same')\n",
    "#     self.selu_h = Activation(selu)\n",
    "#     self.conv1_i = Conv1D(4, 5, padding='same')\n",
    "#     self.selu_i = Activation(selu)\n",
    "#     self.avg_pool_a = AvgPool1D(2, padding='same')\n",
    "#     self.conv1_j = Conv1D(4, 3, padding='same')\n",
    "#     self.selu_j = Activation(selu)\n",
    "#     self.avg_pool_b = AvgPool1D(2, padding='same')\n",
    "#     self.conv1_k = Conv1D(4, 3, padding='same')\n",
    "#     self.selu_k = Activation(selu)\n",
    "#     self.avg_pool_c = AvgPool1D(2, padding='same')\n",
    "  \n",
    "#   def call(self, x):\n",
    "#     x = self.max_pool_a(x)\n",
    "#     x = self.conv1_d(x)\n",
    "#     x = self.selu_d(x)\n",
    "#     x = self.conv1_e(x)\n",
    "#     x = self.selu_e(x)\n",
    "#     x = self.conv1_f(x)\n",
    "#     x = self.selu_f(x)\n",
    "#     x = self.conv1_g(x)\n",
    "#     x = self.selu_g(x)\n",
    "#     x = self.conv1_h(x)\n",
    "#     x = self.selu_h(x)\n",
    "#     x = self.conv1_i(x)\n",
    "#     x = self.selu_i(x)\n",
    "#     x = self.avg_pool_a(x)\n",
    "#     x = self.conv1_j(x)\n",
    "#     x = self.selu_j(x)\n",
    "#     x = self.avg_pool_b(x)\n",
    "#     x = self.conv1_k(x)\n",
    "#     x = self.selu_k(x)\n",
    "#     x = self.avg_pool_c(x)\n",
    "#     return x\n",
    "  \n",
    "# class Hidden_Embedding(keras.layers.Layer):\n",
    "#   def __init__(self):\n",
    "#     super(Hidden_Embedding, self).__init__()\n",
    "#     self.dense_c = Dense(20, selu)\n",
    "#     self.dense_d = Dense(8, selu)\n",
    "#     self.dense_e = Dense(4, selu)\n",
    "\n",
    "#   def call(self, x):\n",
    "#     x = self.dense_c(x)\n",
    "#     x = self.dense_d(x)\n",
    "#     x = self.dense_e(x)\n",
    "#     return x\n",
    "\n",
    "# class SHO_Model(keras.Model):\n",
    "#   def __init__(self):\n",
    "#     super(SHO_Model, self).__init__()\n",
    "#     self.hidden_x1 = Hidden_X1()\n",
    "#     self.hidden_xfc = Hidden_XFC()\n",
    "#     self.hidden_x2 = Hidden_X2()\n",
    "#     self.flatten = Flatten()\n",
    "#     self.hidden_embedding = Hidden_Embedding()\n",
    "\n",
    "#   def call(self, x):\n",
    "#     # x = tf.transpose(x, perm=[0, 2, 1])\n",
    "#     x = self.hidden_x1(x)\n",
    "#     # print(x.shape)\n",
    "#     xfc = tf.reshape(x, [-1, 320])\n",
    "#     xfc = self.hidden_xfc(xfc)\n",
    "#     # x = tf.reshape(x, [-1, 2, 128])\n",
    "#     # print(xfc.shape)\n",
    "#     x = self.hidden_x2(x)\n",
    "#     cnn_flat = self.flatten(x)\n",
    "#     # print(cnn_flat.shape)\n",
    "#     # print(xfc.shape)\n",
    "#     encoded = tf.concat([cnn_flat, xfc], 1)\n",
    "#     embedding = self.hidden_embedding(encoded)\n",
    "#     # unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(np.sqrt(params_scaler.var_))),\\\n",
    "#     #                     tf.convert_to_tensor(params_scaler.mean_))\n",
    "    \n",
    "#     return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f8b88ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qkeras\n",
      "  Downloading QKeras-0.9.0-py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from qkeras) (1.22.4)\n",
      "Collecting pyparser\n",
      "  Downloading pyparser-1.0.tar.gz (4.0 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from qkeras) (0.24.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from qkeras) (49.2.1)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from qkeras) (4.62.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from qkeras) (1.7.1)\n",
      "Collecting keras-tuner>=1.0.1\n",
      "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 80.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.2.1\n",
      "  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 104.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from qkeras) (2.6.2)\n",
      "Collecting parse==1.6.5\n",
      "  Downloading parse-1.6.5.tar.gz (24 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-learn>=0.23.1->qkeras) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from scikit-learn>=0.23.1->qkeras) (2.2.0)\n",
      "Requirement already satisfied: ipython in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from keras-tuner>=1.0.1->qkeras) (7.26.0)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from keras-tuner>=1.0.1->qkeras) (21.0)\n",
      "Requirement already satisfied: tensorboard in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from keras-tuner>=1.0.1->qkeras) (2.6.0)\n",
      "Requirement already satisfied: requests in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from keras-tuner>=1.0.1->qkeras) (2.26.0)\n",
      "Requirement already satisfied: six~=1.10 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (1.16.0)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 107.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: backcall in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.18.0)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (4.8.0)\n",
      "Requirement already satisfied: decorator in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (4.4.2)\n",
      "Requirement already satisfied: pygments in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (5.0.5)\n",
      "Requirement already satisfied: matplotlib-inline in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.1.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (3.0.19)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from packaging->keras-tuner>=1.0.1->qkeras) (2.4.7)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.35.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.13.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (3.3.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.39.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.4.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (2.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.37.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (3.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from jedi>=0.16->ipython->keras-tuner>=1.0.1->qkeras) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython->keras-tuner>=1.0.1->qkeras) (0.7.0)\n",
      "Requirement already satisfied: ipython-genutils in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from traitlets>=4.2->ipython->keras-tuner>=1.0.1->qkeras) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner>=1.0.1->qkeras) (0.2.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.1->qkeras) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /scratch/alibek_2/alibek_env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.1->qkeras) (3.1.1)\n",
      "Building wheels for collected packages: pyparser, parse\n",
      "  Building wheel for pyparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyparser: filename=pyparser-1.0-py3-none-any.whl size=4942 sha256=95ac3e154752cba91f1f520b9369735ff7150b38a6226df6c0287d95606b1800\n",
      "  Stored in directory: /home/alk224/.cache/pip/wheels/ae/b3/5f/80b312fc29a80854bd8b1dfe04205fc526baa16c60b87edbff\n",
      "  Building wheel for parse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for parse: filename=parse-1.6.5-py3-none-any.whl size=18175 sha256=400be602c4aa64b47a046f17b0262291084a43c5c5fdf669c34de450498cd1f1\n",
      "  Stored in directory: /home/alk224/.cache/pip/wheels/76/44/b9/72f2a52beaaa26f6e946984a29cc26a031f5c792be55945cb2\n",
      "Successfully built pyparser parse\n",
      "Installing collected packages: parse, pyparser, kt-legacy, keras-tuner, dm-tree, tensorflow-model-optimization, qkeras\n",
      "Successfully installed dm-tree-0.1.7 keras-tuner-1.1.2 kt-legacy-1.0.4 parse-1.6.5 pyparser-1.0 qkeras-0.9.0 tensorflow-model-optimization-0.7.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/scratch/alibek_2/alibek_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install qkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3209aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "de089672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Conv1D_Block(x, time_step, kernel_size):\n",
    "#     x = Conv1D(time_step, kernel_size, padding='same')(x)\n",
    "#     x = Activation(selu)(x)\n",
    "#     return x\n",
    "\n",
    "# def SHO_Model(n_step=80):\n",
    "#     original_input = tf.keras.Input(shape=(n_step, 2))\n",
    "#     x = Conv1D_Block(original_input, 8, 7)\n",
    "#     x = Conv1D_Block(x, 6, 7)\n",
    "#     x = Conv1D_Block(x, 4, 5)\n",
    "    \n",
    "# #     xfc = tf.reshape(x, [-1, 320])\n",
    "#     xfc = Flatten()(x)\n",
    "#     xfc = Dense(100, selu)(xfc)\n",
    "#     xfc = Dense(20, selu)(xfc)\n",
    "    \n",
    "#     x = MaxPool1D(2, padding='same')(x)\n",
    "#     x = Conv1D_Block(x, 2, 5)\n",
    "#     x = Conv1D_Block(x, 4, 5)\n",
    "#     x = Conv1D_Block(x, 4, 5)\n",
    "#     x = Conv1D_Block(x, 4, 5)\n",
    "#     x = Conv1D_Block(x, 4, 5)\n",
    "#     x = Conv1D_Block(x, 4, 5)\n",
    "#     x = AvgPool1D(2, padding='same')(x)\n",
    "#     x = Conv1D_Block(x, 4, 3)\n",
    "#     x = AvgPool1D(2, padding='same')(x)\n",
    "#     x = Conv1D_Block(x, 4, 3)\n",
    "#     x = AvgPool1D(2, padding='same')(x)\n",
    "    \n",
    "#     cnn_flat = Flatten()(x)\n",
    "#     encoded = tf.concat([cnn_flat, xfc], 1)\n",
    "#     encoded = tf.keras.layers.concatenate([cnn_flat, xfc], 1)\n",
    "#     embedding = Dense(20, selu)(encoded)\n",
    "#     embedding = Dense(8, selu)(embedding)\n",
    "#     embedding = Dense(4, selu)(embedding)\n",
    "    \n",
    "#     model = Model(original_input, embedding, name='SHO_Model')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "fc48dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1D_Block(x, time_step, kernel_size):\n",
    "    x = QConv1D(time_step, kernel_size, padding='same',\n",
    "                kernel_quantizer=\"stochastic_ternary\", bias_quantizer=\"ternary\")(x)\n",
    "    x = QActivation(selu)(x)\n",
    "    return x\n",
    "\n",
    "def SHO_Model(n_step=80):\n",
    "    original_input = tf.keras.Input(shape=(n_step, 2))\n",
    "    x = Conv1D_Block(original_input, 8, 7)\n",
    "    x = Conv1D_Block(x, 6, 7)\n",
    "    x = Conv1D_Block(x, 4, 5)\n",
    "    \n",
    "#     xfc = tf.reshape(x, [-1, 320])\n",
    "    xfc = Flatten()(x)\n",
    "    xfc = QDense(100, selu)(xfc)\n",
    "    xfc = QDense(20, selu)(xfc)\n",
    "    \n",
    "    x = MaxPool1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, 2, 5)\n",
    "    x = Conv1D_Block(x, 4, 5)\n",
    "    x = Conv1D_Block(x, 4, 5)\n",
    "    x = Conv1D_Block(x, 4, 5)\n",
    "    x = Conv1D_Block(x, 4, 5)\n",
    "    x = Conv1D_Block(x, 4, 5)\n",
    "    x = AvgPool1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, 4, 3)\n",
    "    x = AvgPool1D(2, padding='same')(x)\n",
    "    x = Conv1D_Block(x, 4, 3)\n",
    "    x = AvgPool1D(2, padding='same')(x)\n",
    "    \n",
    "    cnn_flat = Flatten()(x)\n",
    "    encoded = tf.concat([cnn_flat, xfc], 1)\n",
    "    encoded = tf.keras.layers.concatenate([cnn_flat, xfc], 1)\n",
    "    embedding = Dense(20, selu)(encoded)\n",
    "    embedding = Dense(8, selu)(embedding)\n",
    "    embedding = Dense(4, selu)(embedding)\n",
    "    \n",
    "    model = Model(original_input, embedding, name='SHO_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b55e0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((data_train, params_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((data_test, params_test))\n",
    "BATCH_SIZE = 256\n",
    "SHUFFLE_BUFFER_SIZE = 512\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "44c84e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SHO_Model()\n",
    "adahessian = AdaHessian(0.1)\n",
    "adam = Adam(3e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=adam, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "29ca7bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SHO_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_36 (InputLayer)           [(None, 80, 2)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_14 (QConv1D)           (None, 80, 8)        120         input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_12 (QActivation)   (None, 80, 8)        0           q_conv1d_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_15 (QConv1D)           (None, 80, 6)        342         q_activation_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_13 (QActivation)   (None, 80, 6)        0           q_conv1d_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_16 (QConv1D)           (None, 80, 4)        124         q_activation_13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_14 (QActivation)   (None, 80, 4)        0           q_conv1d_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 40, 4)        0           q_activation_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_17 (QConv1D)           (None, 40, 2)        42          max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_15 (QActivation)   (None, 40, 2)        0           q_conv1d_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_18 (QConv1D)           (None, 40, 4)        44          q_activation_15[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_16 (QActivation)   (None, 40, 4)        0           q_conv1d_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_19 (QConv1D)           (None, 40, 4)        84          q_activation_16[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_17 (QActivation)   (None, 40, 4)        0           q_conv1d_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_20 (QConv1D)           (None, 40, 4)        84          q_activation_17[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_18 (QActivation)   (None, 40, 4)        0           q_conv1d_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_21 (QConv1D)           (None, 40, 4)        84          q_activation_18[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_19 (QActivation)   (None, 40, 4)        0           q_conv1d_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_22 (QConv1D)           (None, 40, 4)        84          q_activation_19[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_20 (QActivation)   (None, 40, 4)        0           q_conv1d_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_98 (AveragePo (None, 20, 4)        0           q_activation_20[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_23 (QConv1D)           (None, 20, 4)        52          average_pooling1d_98[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_21 (QActivation)   (None, 20, 4)        0           q_conv1d_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_99 (AveragePo (None, 10, 4)        0           q_activation_21[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_conv1d_24 (QConv1D)           (None, 10, 4)        52          average_pooling1d_99[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "q_activation_22 (QActivation)   (None, 10, 4)        0           q_conv1d_24[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_42 (Flatten)            (None, 320)          0           q_activation_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_100 (AverageP (None, 5, 4)         0           q_activation_22[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_dense_4 (QDense)              (None, 100)          32100       flatten_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_43 (Flatten)            (None, 20)           0           average_pooling1d_100[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "q_dense_5 (QDense)              (None, 20)           2020        q_dense_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 40)           0           flatten_43[0][0]                 \n",
      "                                                                 q_dense_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_168 (Dense)               (None, 20)           820         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_169 (Dense)               (None, 8)            168         dense_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_170 (Dense)               (None, 4)            36          dense_169[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,256\n",
      "Trainable params: 36,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "24696f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4320/4320 [==============================] - 70s 15ms/step - loss: 0.4790\n",
      "Epoch 2/20\n",
      "4320/4320 [==============================] - 66s 15ms/step - loss: 0.2347\n",
      "Epoch 3/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.2027\n",
      "Epoch 4/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1828\n",
      "Epoch 5/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1765\n",
      "Epoch 6/20\n",
      "4320/4320 [==============================] - 66s 15ms/step - loss: 0.1648\n",
      "Epoch 7/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1606\n",
      "Epoch 8/20\n",
      "4320/4320 [==============================] - 66s 15ms/step - loss: 0.1600\n",
      "Epoch 9/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1581\n",
      "Epoch 10/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1563\n",
      "Epoch 11/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1591\n",
      "Epoch 12/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1541\n",
      "Epoch 13/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1669\n",
      "Epoch 14/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1632\n",
      "Epoch 15/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1622\n",
      "Epoch 16/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1565\n",
      "Epoch 17/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1541\n",
      "Epoch 18/20\n",
      "4320/4320 [==============================] - 66s 15ms/step - loss: 0.1546\n",
      "Epoch 19/20\n",
      "4320/4320 [==============================] - 65s 15ms/step - loss: 0.1546\n",
      "Epoch 20/20\n",
      "4320/4320 [==============================] - 64s 15ms/step - loss: 0.1473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d8b7ec790>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=20, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "93a55a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/tf_sho_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/tf_sho_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_models/tf_sho_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4e5a7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "061323ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_31, layer type: Input\n",
      "Layer name: conv1d_389, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_389\n",
      "Layer name: activation_386, layer type: Activation\n",
      "Layer name: conv1d_390, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_390\n",
      "Layer name: activation_387, layer type: Activation\n",
      "Layer name: conv1d_391, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_391\n",
      "Layer name: activation_388, layer type: Activation\n",
      "Layer name: max_pooling1d_45, layer type: MaxPooling1D\n",
      "Layer name: conv1d_392, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_392\n",
      "Layer name: activation_389, layer type: Activation\n",
      "Layer name: conv1d_393, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_393\n",
      "Layer name: activation_390, layer type: Activation\n",
      "Layer name: conv1d_394, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_394\n",
      "Layer name: activation_391, layer type: Activation\n",
      "Layer name: conv1d_395, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_395\n",
      "Layer name: activation_392, layer type: Activation\n",
      "Layer name: conv1d_396, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_396\n",
      "Layer name: activation_393, layer type: Activation\n",
      "Layer name: conv1d_397, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_397\n",
      "Layer name: activation_394, layer type: Activation\n",
      "Layer name: average_pooling1d_95, layer type: AveragePooling1D\n",
      "Layer name: conv1d_398, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_398\n",
      "Layer name: activation_395, layer type: Activation\n",
      "Layer name: average_pooling1d_96, layer type: AveragePooling1D\n",
      "Layer name: conv1d_399, layer type: Conv1D\n",
      "  -> Activation (linear), layer name: conv1d_399\n",
      "Layer name: activation_396, layer type: Activation\n",
      "Layer name: average_pooling1d_97, layer type: AveragePooling1D\n",
      "Layer name: dense_163, layer type: Dense\n",
      "  -> Activation (selu), layer name: dense_163\n",
      "Layer name: dense_164, layer type: Dense\n",
      "  -> Activation (selu), layer name: dense_164\n",
      "Layer name: concatenate_5, layer type: Concatenate\n",
      "Layer name: dense_165, layer type: Dense\n",
      "  -> Activation (selu), layer name: dense_165\n",
      "Layer name: dense_166, layer type: Dense\n",
      "  -> Activation (selu), layer name: dense_166\n",
      "Layer name: dense_167, layer type: Dense\n",
      "  -> Activation (selu), layer name: dense_167\n"
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "eb297545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_31, layer type: InputLayer, input shapes: [[None, 80, 2]], output shape: [None, 80, 2]\n",
      "Layer name: conv1d_389, layer type: Conv1D, input shapes: [[None, 80, 2]], output shape: [None, 80, 8]\n",
      "Layer name: activation_386, layer type: Activation, input shapes: [[None, 80, 8]], output shape: [None, 80, 8]\n",
      "Layer name: conv1d_390, layer type: Conv1D, input shapes: [[None, 80, 8]], output shape: [None, 80, 6]\n",
      "Layer name: activation_387, layer type: Activation, input shapes: [[None, 80, 6]], output shape: [None, 80, 6]\n",
      "Layer name: conv1d_391, layer type: Conv1D, input shapes: [[None, 80, 6]], output shape: [None, 80, 4]\n",
      "Layer name: activation_388, layer type: Activation, input shapes: [[None, 80, 4]], output shape: [None, 80, 4]\n",
      "Layer name: max_pooling1d_45, layer type: MaxPooling1D, input shapes: [[None, 80, 4]], output shape: [None, 40, 4]\n",
      "Layer name: conv1d_392, layer type: Conv1D, input shapes: [[None, 40, 4]], output shape: [None, 40, 2]\n",
      "Layer name: activation_389, layer type: Activation, input shapes: [[None, 40, 2]], output shape: [None, 40, 2]\n",
      "Layer name: conv1d_393, layer type: Conv1D, input shapes: [[None, 40, 2]], output shape: [None, 40, 4]\n",
      "Layer name: activation_390, layer type: Activation, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: conv1d_394, layer type: Conv1D, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: activation_391, layer type: Activation, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: conv1d_395, layer type: Conv1D, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: activation_392, layer type: Activation, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: conv1d_396, layer type: Conv1D, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: activation_393, layer type: Activation, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: conv1d_397, layer type: Conv1D, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: activation_394, layer type: Activation, input shapes: [[None, 40, 4]], output shape: [None, 40, 4]\n",
      "Layer name: average_pooling1d_95, layer type: AveragePooling1D, input shapes: [[None, 40, 4]], output shape: [None, 20, 4]\n",
      "Layer name: conv1d_398, layer type: Conv1D, input shapes: [[None, 20, 4]], output shape: [None, 20, 4]\n",
      "Layer name: activation_395, layer type: Activation, input shapes: [[None, 20, 4]], output shape: [None, 20, 4]\n",
      "Layer name: average_pooling1d_96, layer type: AveragePooling1D, input shapes: [[None, 20, 4]], output shape: [None, 10, 4]\n",
      "Layer name: conv1d_399, layer type: Conv1D, input shapes: [[None, 10, 4]], output shape: [None, 10, 4]\n",
      "Layer name: activation_396, layer type: Activation, input shapes: [[None, 10, 4]], output shape: [None, 10, 4]\n",
      "Layer name: flatten_38, layer type: Reshape, input shapes: [[None, 80, 4]], output shape: [None, 320]\n",
      "Layer name: average_pooling1d_97, layer type: AveragePooling1D, input shapes: [[None, 10, 4]], output shape: [None, 5, 4]\n",
      "Layer name: dense_163, layer type: Dense, input shapes: [[None, 320]], output shape: [None, 100]\n",
      "Layer name: flatten_39, layer type: Reshape, input shapes: [[None, 5, 4]], output shape: [None, 20]\n",
      "Layer name: dense_164, layer type: Dense, input shapes: [[None, 100]], output shape: [None, 20]\n",
      "Layer name: concatenate_5, layer type: Concatenate, input shapes: [[None, 20], [None, 20]], output shape: [None, 40]\n",
      "Layer name: dense_165, layer type: Dense, input shapes: [[None, 40]], output shape: [None, 20]\n",
      "Layer name: dense_166, layer type: Dense, input shapes: [[None, 20]], output shape: [None, 8]\n",
      "Layer name: dense_167, layer type: Dense, input shapes: [[None, 8]], output shape: [None, 4]\n",
      "Creating HLS model\n"
     ]
    }
   ],
   "source": [
    "# Convert to an hls model\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir='test_prj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d820527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4db0fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6dffe1ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Vivado HLS installation not found. Make sure \"vivado_hls\" is on PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_622692/2340382613.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhls_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/hls4ml/model/hls_model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, reset, csim, synth, cosim, validation, export, vsynth)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'command -v vivado_hls > /dev/null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vivado HLS installation not found. Make sure \"vivado_hls\" is on PATH.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Intel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Vivado HLS installation not found. Make sure \"vivado_hls\" is on PATH."
     ]
    }
   ],
   "source": [
    "hls_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c082b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.predict(data_train[:500]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5837361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "19f7ab8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '': No schema supplied. Perhaps you meant http://?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_622692/1996836064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/uc?id=1SHBWmHHnEfUeQok92dShrxpEq6UmctHD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Xilinx_Unified_2022.1_0420_0327_Lin64.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProxyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"An error has occurred using proxy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/alibek_2/alibek_env/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '': No schema supplied. Perhaps you meant http://?"
     ]
    }
   ],
   "source": [
    "gdown.download('https://drive.google.com/uc?id=1SHBWmHHnEfUeQok92dShrxpEq6UmctHD', 'Xilinx_Unified_2022.1_0420_0327_Lin64.bin', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47f3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
