{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quantized Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPESFujvvm/jjb4xhQvhAJk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abekek/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/blob/main/notebooks/Quantized_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_bMEJp3TVU9"
      },
      "source": [
        "## Initialization Code (code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BYn5s4SZNKd"
      },
      "source": [
        "### Mounting Google Drive (code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkM0wMn0gm9X",
        "outputId": "e4ea55b3-d437-4205-fc05-8140e679dbc7"
      },
      "source": [
        "# if running on collaboratory set = True\n",
        "collaboratory = True\n",
        "\n",
        "if collaboratory:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else: \n",
        "    print('Running on local systems, if running on collaboratory please change above')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Xu_2zghGiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d2d816-3041-49c0-cf00-1d6c1029b735"
      },
      "source": [
        "# changes directory to your main google drive folder\n",
        "%cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKMGpb4-hh0y"
      },
      "source": [
        "# Checks if the directory exists\n",
        "import os\n",
        "if os.path.exists(\"./Quantized-SHO-Fitting\"):\n",
        "    pass\n",
        "else:\n",
        "    %mkdir Quantized-SHO-Fitting\n",
        "    %cd Quantized-SHO-Fitting\n",
        "    !git clone https://github.com/abekek/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting.git"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO3EQvJbhj9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b5f2d8-0984-4eea-931a-f9442428d7d7"
      },
      "source": [
        "# moves to the right directory\n",
        "%cd Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp-2gjiphlCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca96911-b81d-4d87-c69d-80f2763dcdc1"
      },
      "source": [
        "# checks if the directory is up to date\n",
        "!git pull"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating 52b267f..20ab9ff\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian.pt\n",
            "Please commit your changes or stash them before you merge.\n",
            "error: The following untracked working tree files would be overwritten by merge:\n",
            "\tAssets/Movies/amplitude_movie.mp4\n",
            "\tAssets/Movies/phase_movie.mp4\n",
            "\tAssets/Movies/q_factor_movie.mp4\n",
            "\tAssets/Movies/resonance_movie.mp4\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian_noise_2.0_bs128.pt\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian_noise_4.0_bs128.pt\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian_noise_7.0_bs128.pt\n",
            "Please move or remove them before you merge.\n",
            "Aborting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVye1WEEZttP"
      },
      "source": [
        "### Installing Packages (code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OlmHtCc-H8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8bd603-6165-4f45-86a8-665ec0f34f8b"
      },
      "source": [
        "# Installs all of the requirements\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: atomicwrites==1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Collecting attrs==20.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting autopep8==1.5.4\n",
            "  Downloading autopep8-1.5.4.tar.gz (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 25.5 MB/s \n",
            "\u001b[?25hCollecting certifi==2020.12.5\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.4\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting coverage==5.3.1\n",
            "  Downloading coverage-5.3.1-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.6.2)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.1.1)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 41.9 MB/s \n",
            "\u001b[?25hCollecting packaging==20.8\n",
            "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
            "Collecting Pillow==8.1.0\n",
            "  Downloading Pillow-8.1.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 53.9 MB/s \n",
            "\u001b[?25hCollecting pipreqs==0.4.10\n",
            "  Downloading pipreqs-0.4.10-py2.py3-none-any.whl (25 kB)\n",
            "Collecting pluggy==0.13.1\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting py==1.10.0\n",
            "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting pycodestyle==2.6.0\n",
            "  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 363 kB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting pytest==6.2.1\n",
            "  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 77.8 MB/s \n",
            "\u001b[?25hCollecting pytest-cov==2.11.1\n",
            "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 10.0 MB/s \n",
            "\u001b[?25hCollecting toml==0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting urllib3==1.26.2\n",
            "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting yarg==0.1.9\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Collecting numpy_groupies==0.9.7\n",
            "  Downloading numpy_groupies-0.9.7.tar.gz (22 kB)\n",
            "Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (2.12.0)\n",
            "Requirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (1.1.0)\n",
            "Collecting tensorflow==2.4.1\n",
            "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3 MB 14 kB/s \n",
            "\u001b[?25hCollecting sidpy==0.0.5\n",
            "  Downloading sidpy-0.0.5-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.4 MB/s \n",
            "\u001b[?25hCollecting ipywidgets==7.6.3\n",
            "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 69.2 MB/s \n",
            "\u001b[?25hCollecting scikit_image==0.16.2\n",
            "  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.5 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting joblib==1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting pip==19.3.1\n",
            "  Downloading pip-19.3.1-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 60.5 MB/s \n",
            "\u001b[?25hCollecting ipython==7.22.0\n",
            "  Downloading ipython-7.22.0-py3-none-any.whl (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting pyqtgraph==0.12.1\n",
            "  Downloading pyqtgraph-0.12.1-py3-none-any.whl (939 kB)\n",
            "\u001b[K     |████████████████████████████████| 939 kB 62.9 MB/s \n",
            "\u001b[?25hCollecting pyUSID==0.0.10\n",
            "  Downloading pyUSID-0.0.10-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting scikit_learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sphinx_rtd_theme==0.5.2\n",
            "  Downloading sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 57.7 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting BGlib==0.0.3\n",
            "  Downloading BGlib-0.0.3-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting pycroscopy==0.60.7\n",
            "  Downloading pycroscopy-0.60.7-py2.py3-none-any.whl (354 kB)\n",
            "\u001b[K     |████████████████████████████████| 354 kB 77.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (3.2.2)\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (0.2.3.5)\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (1.4.1)\n",
            "Collecting pygame==2.0.1\n",
            "  Downloading pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 47.7 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0.dev20210415+cu101 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0.dev20210415+cu101\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me0OrE2gO3Tq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "388ed2f2-d8b6-4af7-9e78-5b69a2827631"
      },
      "source": [
        "# installing PyTorch's Nightly version\n",
        "!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html -U\n",
        "\n",
        "!pip install pycroscopy==0.60.7\n",
        "\n",
        "if os.path.exists(\"./BGlib\"):\n",
        "    pass\n",
        "else:\n",
        "    !git clone https://github.com/pycroscopy/BGlib.git\n",
        "    %cd BGlib/\n",
        "    !git tag -l\n",
        "    !git checkout 0.0.3\n",
        "    !git branch -D master\n",
        "    !git checkout -b master\n",
        "    %cd ..\n",
        "\n",
        "# downgrading the h5py version\n",
        "!pip install 'h5py==2.10.0' --force-reinstal"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycroscopy==0.60.7\n",
            "  Using cached pycroscopy-0.60.7-py2.py3-none-any.whl (354 kB)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (0.18.3)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.4.1)\n",
            "Collecting igor\n",
            "  Downloading igor-0.3.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 279 kB/s \n",
            "\u001b[?25hCollecting ipython>=6.0\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.15.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (5.4.8)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (7.1.2)\n",
            "Requirement already satisfied: ipywidgets>=5.2.2 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (7.7.0)\n",
            "Collecting numpy-groupies==0.9.7\n",
            "  Using cached numpy_groupies-0.9.7.tar.gz (22 kB)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.1.0)\n",
            "Collecting pyUSID>=0.0.8\n",
            "  Using cached pyUSID-0.0.10-py2.py3-none-any.whl (66 kB)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.0.2)\n",
            "Collecting sidpy>=0.0.1\n",
            "  Downloading sidpy-0.0.9-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (3.1.0)\n",
            "Collecting gwyfile\n",
            "  Downloading gwyfile-0.2.0-py2.py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.6.0->pycroscopy==0.60.7) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 31.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.18.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.6.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=6.0->pycroscopy==0.60.7) (0.8.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->pycroscopy==0.60.7) (4.2.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.11.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=6.0->pycroscopy==0.60.7) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->pycroscopy==0.60.7) (0.2.5)\n",
            "Collecting cytoolz\n",
            "  Downloading cytoolz-0.11.2.tar.gz (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 50.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (0.11.2)\n",
            "Requirement already satisfied: dask>=0.10 in /usr/local/lib/python3.7/dist-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (2.12.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->pycroscopy==0.60.7) (3.1.0)\n",
            "Collecting ipyfilechooser>=0.0.6\n",
            "  Downloading ipyfilechooser-0.6.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from sidpy>=0.0.1->pycroscopy==0.60.7) (0.0)\n",
            "Collecting distributed>=2.0.0psutil\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (3.13)\n",
            "Collecting dask>=0.10\n",
            "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (21.3)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.11.3)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.0.4)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.1 MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 54.3 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (23.1.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.5.1)\n",
            "Building wheels for collected packages: numpy-groupies, cytoolz, igor\n",
            "  Building wheel for numpy-groupies (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy-groupies: filename=numpy_groupies-0.9.7-py3-none-any.whl size=21315 sha256=6d3a4378eac69fc5ffaee8525feb0543dbe157e28f7e4e1a9cf23f0205203b88\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/05/e1/250ebec6656f2a0bc1141d5c185876dcd74e7e47740613c9d6\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.2-cp37-cp37m-linux_x86_64.whl size=1236723 sha256=343ade7262a6e1ae4e1c5f724e71c5cc320c4686e369d145885cbc9b89adab8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/70/71/ca13ea3d36ccd0b3d0ec7d7a4ca67522048d695b556bba4f59\n",
            "  Building wheel for igor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for igor: filename=igor-0.3-py3-none-any.whl size=52116 sha256=725da50cbbe55107d3f0487a311c1cea982e9d0cf877333d53ed54df382339ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/27/d2/31051f074caeea50e0d11890508c40e9456af990a9350d0fb6\n",
            "Successfully built numpy-groupies cytoolz igor\n",
            "Installing collected packages: prompt-toolkit, ipython, locket, pyyaml, partd, fsspec, cloudpickle, dask, ipyfilechooser, distributed, cytoolz, sidpy, pyUSID, numpy-groupies, igor, gwyfile, pycroscopy\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.1.0 cytoolz-0.11.2 dask-2022.2.0 distributed-2022.2.0 fsspec-2022.5.0 gwyfile-0.2.0 igor-0.3 ipyfilechooser-0.6.0 ipython-7.34.0 locket-1.0.0 numpy-groupies-0.9.7 partd-1.2.0 prompt-toolkit-3.0.29 pyUSID-0.0.10 pycroscopy-0.60.7 pyyaml-6.0 sidpy-0.0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 14.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.6 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ff9GT0VaP-y"
      },
      "source": [
        "### Importing Packages (code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb6fWK4LaTAw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "dba279ea-80ed-4bb4-857d-311f395876e1"
      },
      "source": [
        "import gdown\n",
        "import multiprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numba import jit\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import argparse\n",
        "import seaborn as sns\n",
        "from scipy.signal import resample\n",
        "from scipy import fftpack\n",
        "from scipy import io\n",
        "from scipy import special\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.layers import (Attention, Dense, Conv1D, Convolution2D, \n",
        "                                     GRU, LSTM, Bidirectional, TimeDistributed,\n",
        "                                     Dropout, Flatten, LayerNormalization, \n",
        "                                     RepeatVector, Reshape, MaxPooling1D, \n",
        "                                     UpSampling1D, BatchNormalization, Activation)\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from multiprocessing import Pool, Process\n",
        "import multiprocessing as mp\n",
        "from moviepy.editor import *\n",
        "import glob\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import gc\n",
        "import sidpy\n",
        "from BGlib.BGlib import be as belib\n",
        " \n",
        "# set up notebook to show plots within the notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "\n",
        "# Import necessary libraries:\n",
        "# General utilities:\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Computation:\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization:\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "# Finally, pycroscopy itself\n",
        "sys.path.append('../../../')\n",
        "import pyUSID as usid\n",
        "from codes.util.preprocessing_global_standard_scaler import global_standard_scaler\n",
        "from sidpy.hdf.hdf_utils import write_simple_attrs, get_attr\n",
        "from pyUSID.io.hdf_utils import create_results_group, write_main_dataset, write_reduced_anc_dsets, create_empty_dataset, reshape_to_n_dims, get_auxiliary_datasets\n",
        "from pyUSID.io.usi_data import USIDataset\n",
        "from pyUSID.io import Dimension\n",
        "\n",
        "from codes.util.file import print_tree\n",
        "from codes.util.core import SHO_fit_func_torch, loop_fitting_function, loop_fitting_function_tf, computeDotProducts, normOfVar, fit_loop_function, computeTime, conventional_fit_loop_function\n",
        "from codes.viz.plot import plot_best_worst_SHO, make_movie, plot_best_worst_loops, plot_reconstruction_comparison_SHO, plot_reconstruction_comparison_loops\n",
        "from codes.util.postprocessing import transform_params, convert_real_imag\n",
        "from codes.util.preprocessing_global_scaler import global_scaler\n",
        "from codes.processing.filters import range_filter, clean_interpolate, interpolate_missing_points\n",
        "from codes.algorithm.TRPCGOptimizerv2 import TRPCGOptimizerv2\n",
        "from codes.algorithm.AdaHessian import AdaHessian\n",
        "\n",
        "import numpy.lib.recfunctions as rfn"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2859008/45929032 bytes (6.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5447680/45929032 bytes (11.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7831552/45929032 bytes (17.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11091968/45929032 bytes (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13443072/45929032 bytes (29.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16662528/45929032 bytes (36.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18980864/45929032 bytes (41.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21848064/45929032 bytes (47.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25559040/45929032 bytes (55.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29278208/45929032 bytes (63.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32284672/45929032 bytes (70.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35454976/45929032 bytes (77.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37806080/45929032 bytes (82.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40370176/45929032 bytes (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42999808/45929032 bytes (93.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44883968/45929032 bytes (97.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8kofx1Ta8tA"
      },
      "source": [
        "### Setting Defaults (code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDCL1OG2OvVU",
        "outputId": "3b0ad5d9-a927-4f1c-c220-56086ad66d67"
      },
      "source": [
        "# shows the number of CPU cores\n",
        "multiprocessing.cpu_count()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsXoBN6cNuRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3283a7-e61d-48cb-c7f4-6a112a9ad7e7"
      },
      "source": [
        "# shows the GPU that is available and the resources\n",
        "!nvidia-smi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 14 21:59:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_n-dq6La9Po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763a0a71-aa62-414c-f7f3-7febc1b76519"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "gpus"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o1NE8BEbD3W"
      },
      "source": [
        "# fixes the random seed for reproducible training\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl5nmD8xFBzh"
      },
      "source": [
        "# resetting default seaborn style\n",
        "sns.reset_orig()\n",
        "\n",
        "# setting default plotting params\n",
        "plt.rcParams['image.cmap'] = 'magma'\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.direction'] = 'in'\n",
        "plt.rcParams['ytick.direction'] = 'in'\n",
        "plt.rcParams['xtick.top'] = True\n",
        "plt.rcParams['ytick.right'] = True"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol1F9fy7Mo1v"
      },
      "source": [
        "### Loading data for SHO fitting (code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rstr734kKLyT"
      },
      "source": [
        "This is a dataset used for the entire notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or1SY2WdMrkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b384fbc0-d925-4dcf-adbf-cfbeda621563"
      },
      "source": [
        "# downloads the original experiment file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1Q2Qo_1VGlCsVOTjQpZlE5tjoIV1etVe2', 'data_file.h5', quiet=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1Q2Qo_1VGlCsVOTjQpZlE5tjoIV1etVe2\n",
            "To: /content/drive/MyDrive/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/data_file.h5\n",
            "100%|██████████| 1.80G/1.80G [00:18<00:00, 94.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data_file.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRIcp94ufPLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3779f0-f727-4b69-893f-b05ff873eb07"
      },
      "source": [
        "# Opens the translated file\n",
        "h5_f = h5py.File('./data_file.h5', 'r+')\n",
        "\n",
        "#Inspects the h5 file\n",
        "usid.hdf_utils.print_tree(h5_f)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "├ Measurement_000\n",
            "  ---------------\n",
            "  ├ Channel_000\n",
            "    -----------\n",
            "    ├ Bin_FFT\n",
            "    ├ Bin_Frequencies\n",
            "    ├ Bin_Indices\n",
            "    ├ Bin_Step\n",
            "    ├ Bin_Wfm_Type\n",
            "    ├ Excitation_Waveform\n",
            "    ├ Noise_Floor\n",
            "    ├ Position_Indices\n",
            "    ├ Position_Values\n",
            "    ├ Raw_Data\n",
            "    ├ Raw_Data-SHO_Fit_000\n",
            "      --------------------\n",
            "      ├ Fit\n",
            "      ├ Guess\n",
            "      ├ Spectroscopic_Indices\n",
            "      ├ Spectroscopic_Values\n",
            "      ├ completed_fit_positions\n",
            "      ├ completed_guess_positions\n",
            "    ├ Spatially_Averaged_Plot_Group_000\n",
            "      ---------------------------------\n",
            "      ├ Bin_Frequencies\n",
            "      ├ Max_Response\n",
            "      ├ Mean_Spectrogram\n",
            "      ├ Min_Response\n",
            "      ├ Spectroscopic_Parameter\n",
            "      ├ Step_Averaged_Response\n",
            "    ├ Spatially_Averaged_Plot_Group_001\n",
            "      ---------------------------------\n",
            "      ├ Bin_Frequencies\n",
            "      ├ Max_Response\n",
            "      ├ Mean_Spectrogram\n",
            "      ├ Min_Response\n",
            "      ├ Spectroscopic_Parameter\n",
            "      ├ Step_Averaged_Response\n",
            "    ├ Spectroscopic_Indices\n",
            "    ├ Spectroscopic_Values\n",
            "    ├ UDVS\n",
            "    ├ UDVS_Indices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49u7-Tiqf1eD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d404204-ac09-4150-ad12-561fd3d57f6f"
      },
      "source": [
        "print('Datasets and datagroups within the file:\\n------------------------------------')\n",
        "print_tree(h5_f.file)\n",
        " \n",
        "print('\\nThe main dataset:\\n------------------------------------')\n",
        "print(h5_f)\n",
        "print('\\nThe ancillary datasets:\\n------------------------------------')\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Position_Indices'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Position_Values'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Indices'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Values'])\n",
        "\n",
        "print('\\nMetadata or attributes in a datagroup\\n------------------------------------')\n",
        "for key in h5_f.file['/Measurement_000'].attrs:\n",
        "    print('{} : {}'.format(key, h5_f.file['/Measurement_000'].attrs[key]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets and datagroups within the file:\n",
            "------------------------------------\n",
            "/\n",
            "/Measurement_000\n",
            "/Measurement_000/Channel_000\n",
            "/Measurement_000/Channel_000/Bin_FFT\n",
            "/Measurement_000/Channel_000/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Bin_Indices\n",
            "/Measurement_000/Channel_000/Bin_Step\n",
            "/Measurement_000/Channel_000/Bin_Wfm_Type\n",
            "/Measurement_000/Channel_000/Excitation_Waveform\n",
            "/Measurement_000/Channel_000/Noise_Floor\n",
            "/Measurement_000/Channel_000/Position_Indices\n",
            "/Measurement_000/Channel_000/Position_Values\n",
            "/Measurement_000/Channel_000/Raw_Data\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Fit\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Indices\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_fit_positions\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_guess_positions\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Max_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Mean_Spectrogram\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Min_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Spectroscopic_Parameter\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Step_Averaged_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Max_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Mean_Spectrogram\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Min_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Spectroscopic_Parameter\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Step_Averaged_Response\n",
            "/Measurement_000/Channel_000/Spectroscopic_Indices\n",
            "/Measurement_000/Channel_000/Spectroscopic_Values\n",
            "/Measurement_000/Channel_000/UDVS\n",
            "/Measurement_000/Channel_000/UDVS_Indices\n",
            "\n",
            "The main dataset:\n",
            "------------------------------------\n",
            "<HDF5 file \"data_file.h5\" (mode r+)>\n",
            "\n",
            "The ancillary datasets:\n",
            "------------------------------------\n",
            "<HDF5 dataset \"Position_Indices\": shape (3600, 2), type \"<u4\">\n",
            "<HDF5 dataset \"Position_Values\": shape (3600, 2), type \"<f4\">\n",
            "<HDF5 dataset \"Spectroscopic_Indices\": shape (4, 63360), type \"<u4\">\n",
            "<HDF5 dataset \"Spectroscopic_Values\": shape (4, 63360), type \"<f4\">\n",
            "\n",
            "Metadata or attributes in a datagroup\n",
            "------------------------------------\n",
            "BE_actual_duration_[s] : 0.004\n",
            "BE_amplitude_[V] : 1\n",
            "BE_auto_smoothing : auto smoothing on\n",
            "BE_band_edge_smoothing_[s] : 4832.1\n",
            "BE_band_edge_trim : 0.094742\n",
            "BE_band_width_[Hz] : 200000\n",
            "BE_bins_per_band : 0\n",
            "BE_center_frequency_[Hz] : 1310000\n",
            "BE_desired_duration_[s] : 0.004\n",
            "BE_phase_content : chirp-sinc hybrid\n",
            "BE_phase_variation : 1\n",
            "BE_points_per_BE_wave : 0\n",
            "BE_repeats : 4\n",
            "FORC_V_high1_[V] : 1\n",
            "FORC_V_high2_[V] : 10\n",
            "FORC_V_low1_[V] : -1\n",
            "FORC_V_low2_[V] : -10\n",
            "FORC_num_of_FORC_cycles : 1\n",
            "FORC_num_of_FORC_repeats : 1\n",
            "File_MDAQ_version : MDAQ_VS_090915_01\n",
            "File_date_and_time : 18-Sep-2015 18:32:14\n",
            "File_file_name : SP128_NSO\n",
            "File_file_path : C:\\Users\\Asylum User\\Documents\\Users\\Agar\\SP128_NSO\\\n",
            "File_file_suffix : 99\n",
            "IO_AO_amplifier : 10\n",
            "IO_AO_range_[V] : +/- 10\n",
            "IO_Analog_Input_1 : +/- .1V, FFT\n",
            "IO_Analog_Input_2 : off\n",
            "IO_Analog_Input_3 : off\n",
            "IO_Analog_Input_4 : off\n",
            "IO_DAQ_platform : NI 6115\n",
            "IO_rate_[Hz] : 4000000\n",
            "VS_amplitude_[V] : 16\n",
            "VS_cycle_fraction : full\n",
            "VS_cycle_phase_shift : 0\n",
            "VS_measure_in_field_loops : in and out-of-field\n",
            "VS_mode : DC modulation mode\n",
            "VS_number_of_cycles : 2\n",
            "VS_offset_[V] : 0\n",
            "VS_read_voltage_[V] : 0\n",
            "VS_set_pulse_amplitude[V] : 0\n",
            "VS_set_pulse_duration[s] : 0.002\n",
            "VS_step_edge_smoothing_[s] : 0.001\n",
            "VS_steps_per_full_cycle : 96\n",
            "data_type : BEPSData\n",
            "grid_/single : grid\n",
            "grid_contact_set_point_[V] : 1\n",
            "grid_current_col : 1\n",
            "grid_current_row : 1\n",
            "grid_cycle_time_[s] : 10\n",
            "grid_measuring : 0\n",
            "grid_moving : 0\n",
            "grid_num_cols : 60\n",
            "grid_num_rows : 60\n",
            "grid_settle_time_[s] : 0.15\n",
            "grid_time_remaining_[h;m;s] : 10\n",
            "grid_total_time_[h;m;s] : 10\n",
            "grid_transit_set_point_[V] : 0.1\n",
            "grid_transit_time_[s] : 0.15\n",
            "num_bins : 165\n",
            "num_pix : 3600\n",
            "num_udvs_steps : 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z2fWl_hPcq1"
      },
      "source": [
        "# Part I. SHO Fitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4gQ0b2rn4kI"
      },
      "source": [
        "## Extracting Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6EgZqZn3dg"
      },
      "source": [
        "# number of samples per SHO fit\n",
        "num_bins = h5_f['Measurement_000'].attrs['num_bins'] \n",
        "\n",
        "# number of pixels in the image\n",
        "num_pix = h5_f['Measurement_000'].attrs['num_pix'] \n",
        "\n",
        "# number of pixels in x and y dimensions\n",
        "num_pix_1d = int(np.sqrt(num_pix)) \n",
        "\n",
        "# number of DC voltage steps \n",
        "voltage_steps = h5_f['Measurement_000'].attrs['num_udvs_steps']\n",
        "\n",
        "# sampling rate\n",
        "sampling_rate = h5_f['Measurement_000'].attrs['IO_rate_[Hz]']\n",
        "\n",
        "# BE bandwidth\n",
        "be_bandwidth = h5_f['Measurement_000'].attrs['BE_band_width_[Hz]']\n",
        "\n",
        "# BE center frequency\n",
        "be_center_frequency = h5_f['Measurement_000'].attrs['BE_center_frequency_[Hz]']\n",
        "\n",
        "# Frequency Vector in Hz\n",
        "frequency_bin = h5_f['Measurement_000']['Channel_000']['Bin_Frequencies'][:]\n",
        "\n",
        "# Resampled frequency vector\n",
        "wvec_freq = resample(frequency_bin, 80)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lWNU9CAl11"
      },
      "source": [
        "# get raw data (real and imaginary combined)\n",
        "raw_data = h5_f['Measurement_000']['Channel_000']['Raw_Data']\n",
        "# resampling it from 165 to 80 frequency steps\n",
        "raw_data_resampled = resample(np.array(raw_data).reshape(-1 , 165), 80, axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHBI4eow4EHT"
      },
      "source": [
        "## Conversion of Real/Imaginary to Magnitude/Phase Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p3J8nmnLAAe"
      },
      "source": [
        "This is convenient for visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csn21xHl4u4L"
      },
      "source": [
        "# conversion of raw data (both resampled and full)\n",
        "magnitude_graph_initial_full, phase_graph_initial_full = convert_real_imag(raw_data)\n",
        "magnitude_graph_initial, phase_graph_initial = convert_real_imag(raw_data_resampled)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQZWpwdQj9aP"
      },
      "source": [
        "## Resampling the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xXMheD5j621"
      },
      "source": [
        "# get real and imaginary components from raw data\n",
        "real = np.real(h5_f['Measurement_000']['Channel_000']['Raw_Data'])\n",
        "imag = np.imag(h5_f['Measurement_000']['Channel_000']['Raw_Data'])\n",
        "\n",
        "# resample both real and imaginary components \n",
        "real_resample = resample(real.reshape(num_pix, -1, num_bins), 80, axis=2)\n",
        "imag_resample = resample(imag.reshape(num_pix, -1, num_bins), 80, axis=2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZgEYPY-nHv9"
      },
      "source": [
        "# free up the RAM\n",
        "del real\n",
        "del imag\n",
        "gc.collect();"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA2L-XM5nYmI"
      },
      "source": [
        "### Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66_aumU-OGza"
      },
      "source": [
        "# create a list for parameters\n",
        "fit_results_list = []\n",
        "for sublist in np.array(h5_f['Measurement_000']['Channel_000']['Raw_Data-SHO_Fit_000']['Fit']):\n",
        "    for item in sublist:\n",
        "        for i in item:\n",
        "          fit_results_list.append(i)\n",
        "\n",
        "# flatten parameters list into numpy array\n",
        "fit_results_list = np.array(fit_results_list).reshape(num_pix,voltage_steps,5)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJlhD9VzwLa"
      },
      "source": [
        "# scale the fit results with Standard Scaler\n",
        "fit_results_scaler = StandardScaler()\n",
        "scaled_fit_results = fit_results_scaler.fit_transform(fit_results_list.reshape(-1,5))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1dAv3WkW7dl"
      },
      "source": [
        "## Scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AahbDJPpW7dm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0bad459-62a2-4510-95b8-2a24441f3265"
      },
      "source": [
        "# scale the real component of input data\n",
        "scaler_real = global_standard_scaler()\n",
        "scaled_data_real = scaler_real.fit_transform(real_resample).reshape(-1, 80)\n",
        "\n",
        "# scale the imaginary component of input data\n",
        "scaler_imag = global_standard_scaler()\n",
        "scaled_data_imag = scaler_imag.fit_transform(imag_resample).reshape(-1, 80)\n",
        "\n",
        "# stack both components\n",
        "data_ = np.stack((scaled_data_real, scaled_data_imag),axis=2)\n",
        "\n",
        "# scale the parameters (now takes only 4 parameters, excluding the R2)\n",
        "params_scaler = StandardScaler()\n",
        "scaled_params = params_scaler.fit_transform(fit_results_list.reshape(-1,5)[:,0:4])\n",
        "\n",
        "# exclude the R2 parameter\n",
        "params = fit_results_list.reshape(-1,5)[:,0:4]\n",
        "print(params.shape)\n",
        "\n",
        "del real_resample\n",
        "del imag_resample"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean =  -6.855169e-06 STD =  0.0026878386\n",
            "mean =  0.00013161483 STD =  0.0027575183\n",
            "(1382400, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8bRBLdgPL0F"
      },
      "source": [
        "### Train/Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsMAq__GrLCR"
      },
      "source": [
        "Feel free to change the train/test split ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYwis0u3PK1e"
      },
      "source": [
        "data_train, data_test, params_train, params_test = train_test_split(data_, \n",
        "                                                                    scaled_params, \n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=42)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow/HLS4ML Model"
      ],
      "metadata": {
        "id": "5-awlb9_f-Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaHessian"
      ],
      "metadata": {
        "id": "6LrguCm80JYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.python.eager import def_function\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.keras import backend_config\n",
        "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.training import training_ops\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "\n",
        "import abc\n",
        "import contextlib\n",
        "import functools\n",
        "\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.distribute import distribution_strategy_context as distribute_ctx\n",
        "from tensorflow.python.distribute import parameter_server_strategy\n",
        "from tensorflow.python.distribute import reduce_util as ds_reduce_util\n",
        "from tensorflow.python.distribute import values as ds_values\n",
        "from tensorflow.python.eager import backprop\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_util\n",
        "from tensorflow.python.keras import backend\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras.engine import base_layer_utils\n",
        "from tensorflow.python.keras.optimizer_v2 import learning_rate_schedule\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import clip_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import gradients\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import resource_variable_ops\n",
        "from tensorflow.python.ops import variables as tf_variables\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.python.saved_model import revived_types\n",
        "from tensorflow.python.training.tracking import base as trackable\n",
        "from tensorflow.python.training.tracking import tracking\n",
        "from tensorflow.python.util import nest\n",
        "from tensorflow.python.util import tf_inspect\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "class AdaHessian(optimizer_v2.OptimizerV2):\n",
        "\n",
        "    _HAS_AGGREGATE_GRAD = True\n",
        "\n",
        "    def __init__(self,\n",
        "               learning_rate=0.1,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-4,\n",
        "               weight_decay = 0.,\n",
        "               hessian_power=1.0,\n",
        "               name='AdaHessian',\n",
        "               average_size_1d=None,\n",
        "               average_size_2d=None,\n",
        "               average_size_3d=-1,\n",
        "               average_size_4d=-1,\n",
        "               **kwargs):\n",
        "        \"\"\"Construct a new AdaHessian optimizer.\n",
        "        Args:\n",
        "            learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
        "            `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that\n",
        "            takes no arguments and returns the actual value to use, The learning\n",
        "            rate. Defaults to 0.1.\n",
        "            beta_1: A float value or a constant float tensor, or a callable that takes\n",
        "            no arguments and returns the actual value to use. The exponential decay\n",
        "            rate for the 1st moment estimates. Defaults to 0.9.\n",
        "            beta_2: A float value or a constant float tensor, or a callable that takes\n",
        "            no arguments and returns the actual value to use, The exponential decay\n",
        "            rate for the 2nd moment estimates. Defaults to 0.999.\n",
        "            epsilon: A small constant for numerical stability. This epsilon is\n",
        "            \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
        "            Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
        "            1e-7.\n",
        "            weight_decay: We are using AdamW's weight decay scheme. Defaults to 0.\n",
        "            name: Optional name for the operations created when applying gradients.\n",
        "            Defaults to \"Adam\".\n",
        "            hessian_power: Hessian power to control the optimizer more similar to first/second \n",
        "            order method (default: 1). You can also try 0.5. For some tasks we found this \n",
        "            to result in better performance.\n",
        "            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
        "            `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
        "            gradients by value, `decay` is included for backward compatibility to\n",
        "            allow time inverse decay of learning rate. `lr` is included for backward\n",
        "            compatibility, recommended to use `learning_rate` instead.\n",
        "            # average_size_{1,2,3,4}d: \n",
        "                        None: use no spatial averaging\n",
        "                        -1: use suggested spatial averaging (recommended for conv kernels)\n",
        "                        >= 1: use customized size\n",
        "        \"\"\"\n",
        "\n",
        "        super(AdaHessian, self).__init__(name, **kwargs)\n",
        "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
        "        self._set_hyper('decay', self._initial_decay)\n",
        "        self._set_hyper('beta_1', beta_1)\n",
        "        self._set_hyper('beta_2', beta_2)\n",
        "        self.epsilon = epsilon or backend_config.epsilon()\n",
        "        self.weight_decay = weight_decay\n",
        "        self.hessian_power = hessian_power\n",
        "        self.average_size_1d = average_size_1d\n",
        "        self.average_size_2d = average_size_2d\n",
        "        self.average_size_3d = average_size_3d\n",
        "        self.average_size_4d = average_size_4d\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        # Create slots for the first and second moments.\n",
        "        # Separate for-loops to respect the ordering of slot variables from v1.\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'm')\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'v')\n",
        "\n",
        "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
        "        super(AdaHessian, self)._prepare_local(var_device, var_dtype, apply_state)\n",
        "\n",
        "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\n",
        "        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\n",
        "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
        "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
        "        lr = (\n",
        "            apply_state[(var_device, var_dtype)]['lr_t'] *\n",
        "            (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\n",
        "        apply_state[(var_device, var_dtype)].update(\n",
        "            dict(\n",
        "                lr=lr,\n",
        "                epsilon=ops.convert_to_tensor_v2(self.epsilon, var_dtype),\n",
        "                beta_1_t=beta_1_t,\n",
        "                beta_1_power=beta_1_power,\n",
        "                one_minus_beta_1_t=1 - beta_1_t,\n",
        "                beta_2_t=beta_2_t,\n",
        "                beta_2_power=beta_2_power,\n",
        "                one_minus_beta_2_t=1 - beta_2_t))\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        params = self.weights\n",
        "        # If the weights are generated by Keras V1 optimizer, it includes vhats\n",
        "        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n",
        "        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n",
        "        num_vars = int((len(params) - 1) / 2)\n",
        "        if len(weights) == 3 * num_vars + 1:\n",
        "            weights = weights[:len(params)]\n",
        "        super(AdaHessian, self).set_weights(weights)\n",
        "\n",
        "\n",
        "    def get_gradients_hessian(self, loss, params):\n",
        "        \"\"\"Returns gradients and Hessian of `loss` with respect to `params`.\n",
        "        Arguments:\n",
        "            loss: Loss tensor.\n",
        "            params: List of variables.\n",
        "        Returns:\n",
        "            List of gradient and Hessian tensors.\n",
        "        Raises:\n",
        "            ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
        "            function not implemented).\n",
        "        \"\"\"\n",
        "        params = nest.flatten(params)\n",
        "        with backend.get_graph().as_default(), backend.name_scope(self._name +\n",
        "                                                                    \"/gradients\"):\n",
        "            grads = gradients.gradients(loss, params)\n",
        "            for grad, param in zip(grads, params):\n",
        "                if grad is None:\n",
        "                    raise ValueError(\"Variable {} has `None` for gradient. \"\n",
        "                                    \"Please make sure that all of your ops have a \"\n",
        "                                    \"gradient defined (i.e. are differentiable). \"\n",
        "                                    \"Common ops without gradient: \"\n",
        "                                    \"K.argmax, K.round, K.eval.\".format(param))\n",
        "\n",
        "            # WARNING: for now we do not support gradient clip\n",
        "            # grads = self._clip_gradients(grads)\n",
        "\n",
        "            v = [np.random.uniform(0, 1, size = p.shape) for p in params]\n",
        "            for vi in v:\n",
        "                vi[ vi < 0.5] =  -1 \n",
        "                vi[ vi >= 0.5] =  1 \n",
        "            v = [tf.convert_to_tensor(vi, dtype = tf.dtypes.float32) for vi in v]\n",
        "\n",
        "            vprod = tf.reduce_sum([ tf.reduce_sum(vi * grad) for vi, grad in zip(v, grads)])\n",
        "\n",
        "            Hv = gradients.gradients(vprod, params)\n",
        "\n",
        "            Hd = [ tf.abs(Hvi * vi) for Hvi, vi in zip(Hv, v)]\n",
        "\n",
        "        return grads, Hd\n",
        "\n",
        "    def _filter_grads_hessian(self, grads_hessian_and_vars):\n",
        "        \"\"\"Filter out iterable with grad equal to None.\"\"\"\n",
        "        grads_hessian_and_vars = tuple(grads_hessian_and_vars)\n",
        "        if not grads_hessian_and_vars:\n",
        "            return grads_hessian_and_vars\n",
        "        filtered = []\n",
        "        vars_with_empty_grads = []\n",
        "        for grad, hessian, var in grads_hessian_and_vars:\n",
        "            if grad is None:\n",
        "                vars_with_empty_grads.append(var)\n",
        "            else:\n",
        "                filtered.append((grad, hessian, var))\n",
        "        filtered = tuple(filtered)\n",
        "\n",
        "        if not filtered:\n",
        "            raise ValueError(\"No gradients provided for any variable: %s.\" %\n",
        "                            ([v.name for _, v in grads_and_vars],))\n",
        "        if vars_with_empty_grads:\n",
        "            logging.warning(\n",
        "                (\"Gradients do not exist for variables %s when minimizing the loss.\"),\n",
        "                ([v.name for v in vars_with_empty_grads]))\n",
        "        return filtered\n",
        "\n",
        "    def apply_gradients_hessian(self,\n",
        "                      grads_hessian_and_vars,\n",
        "                      name=None,\n",
        "                      experimental_aggregate_gradients=True):\n",
        "        grads_hessian_and_vars = self._filter_grads_hessian(grads_hessian_and_vars)\n",
        "        var_list = [v for (_, _, v) in grads_hessian_and_vars]\n",
        "\n",
        "        with backend.name_scope(self._name):\n",
        "            # Create iteration if necessary.\n",
        "            with ops.init_scope():\n",
        "                self._create_all_weights(var_list)\n",
        "\n",
        "        if not grads_hessian_and_vars:\n",
        "            # Distribution strategy does not support reducing an empty list of\n",
        "            # gradients\n",
        "            return control_flow_ops.no_op()\n",
        "\n",
        "        if distribute_ctx.in_cross_replica_context():\n",
        "            raise RuntimeError(\n",
        "                \"`apply_gradients() cannot be called in cross-replica context. \"\n",
        "                \"Use `tf.distribute.Strategy.run` to enter replica \"\n",
        "                \"context.\")\n",
        "\n",
        "        strategy = distribute_ctx.get_strategy()\n",
        "        if (not experimental_aggregate_gradients and strategy and isinstance(\n",
        "            strategy.extended,\n",
        "            parameter_server_strategy.ParameterServerStrategyExtended)):\n",
        "            raise NotImplementedError(\n",
        "                \"`experimental_aggregate_gradients=False is not supported for \"\n",
        "                \"ParameterServerStrategy and CentralStorageStrategy\")\n",
        "\n",
        "        apply_state = self._prepare(var_list)\n",
        "        if experimental_aggregate_gradients:\n",
        "            reduced_grads, reduced_hessian = self._aggregate_gradients_hessian(grads_hessian_and_vars)\n",
        "            var_list = [v for _, _, v in grads_hessian_and_vars]\n",
        "            grads_hessian_and_vars = list(zip(reduced_grads, reduced_hessian, var_list))\n",
        "\n",
        "\n",
        "        return distribute_ctx.get_replica_context().merge_call(\n",
        "            functools.partial(self._distributed_apply, apply_state=apply_state),\n",
        "            args=(grads_hessian_and_vars,),\n",
        "            kwargs={\n",
        "                \"name\": name,\n",
        "            })\n",
        "\n",
        "    def _aggregate_gradients_hessian(self, grads_hessian_and_vars):\n",
        "        \"\"\"Returns all-reduced gradients.\n",
        "        Args:\n",
        "        grads_and_vars: List of (gradient, hessian, variable) pairs.\n",
        "        Returns:\n",
        "        Two lists of all-reduced gradients and Hessian.\n",
        "        \"\"\"\n",
        "        grads_hessian_and_vars = list(grads_hessian_and_vars)\n",
        "        filtered_grads_hessian_and_vars = self._filter_grads_hessian(grads_hessian_and_vars)\n",
        "\n",
        "        # split the list so that we can use the all_recude_fn\n",
        "        filtered_grads_and_vars = tuple([(g, v) for (g, h, v) in filtered_grads_hessian_and_vars])\n",
        "        filtered_hessian_and_vars = tuple([(h, v) for (g, h, v) in filtered_grads_hessian_and_vars])\n",
        "\n",
        "\n",
        "        def all_reduce_fn(distribution, grads_hessian_and_vars):\n",
        "            # WARNING: this ReduceOp.SUM can only support two entries, for now we have three.\n",
        "            # So far now, we do it for two steps to make life easier.\n",
        "            return distribution.extended.batch_reduce_to(\n",
        "                ds_reduce_util.ReduceOp.SUM, grads_hessian_and_vars)\n",
        "\n",
        "        if filtered_grads_hessian_and_vars:\n",
        "            reduced_part1 = distribute_ctx.get_replica_context().merge_call(\n",
        "                all_reduce_fn, args=(filtered_grads_and_vars,))\n",
        "            reduced_part2 = distribute_ctx.get_replica_context().merge_call(\n",
        "                all_reduce_fn, args=(filtered_hessian_and_vars,))\n",
        "        else:\n",
        "            reduced = []\n",
        "\n",
        "        # Copy 'reduced' but add None gradients back in\n",
        "        reduced_with_nones_grads = []\n",
        "        reduced_with_nones_hessian = []\n",
        "\n",
        "        reduced_pos = 0\n",
        "        for g, h, _ in grads_hessian_and_vars:\n",
        "            if g is None:\n",
        "                reduced_with_nones_grads.append( None )\n",
        "                reduced_with_nones_hessian.append( None )\n",
        "            else:\n",
        "                reduced_with_nones_grads.append(reduced_part1[reduced_pos])\n",
        "                reduced_with_nones_hessian.append(reduced_part2[reduced_pos])\n",
        "                reduced_pos += 1\n",
        "\n",
        "        return reduced_with_nones_grads, reduced_with_nones_hessian\n",
        "\n",
        "\n",
        "    @def_function.function(experimental_compile=True)\n",
        "    def _resource_apply_dense(self, grad, hess, var, apply_state=None):\n",
        "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
        "        coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
        "                        self._fallback_apply_state(var_device, var_dtype))\n",
        "\n",
        "        m = self.get_slot(var, 'm')\n",
        "        v = self.get_slot(var, 'v')\n",
        "\n",
        "        m.assign_add((grad - m) * (1 - coefficients['beta_1_t']))\n",
        "        # this part need to be changed for spatial averaging\n",
        "\n",
        "        if len(v.shape) == 1:\n",
        "            resize = self.average_size_1d\n",
        "        elif len(v.shape) == 2:\n",
        "            resize = self.average_size_2d\n",
        "        elif len(v.shape) == 3:\n",
        "            resize = self.average_size_3d\n",
        "        elif len(v.shape) == 4:\n",
        "            resize = self.average_size_4d\n",
        "        else:\n",
        "            raise Exception('You need to define the spatial average size by yourself!')\n",
        "\n",
        "        if resize == None:\n",
        "            v.assign_add((math_ops.square(hess) - v) * (1 - coefficients['beta_2_t']))\n",
        "        elif resize == -1:\n",
        "            if len(v.shape) == 1:\n",
        "                v.assign_add((math_ops.square(hess) - v) * (1 - coefficients['beta_2_t']))\n",
        "            elif len(v.shape) == 2:\n",
        "                hess_average = tf.reduce_mean(hess, [0], keepdims=True)\n",
        "                v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t'])) \n",
        "            elif len(v.shape) == 3:\n",
        "                hess_average = tf.reduce_mean(hess, [0], keepdims=True)\n",
        "                v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t'])) \n",
        "            elif len(v.shape) == 4:\n",
        "                hess_average = tf.reduce_mean(hess, [0, 1], keepdims=True)\n",
        "                v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t'])) \n",
        "        else:\n",
        "            if resize <= 0:\n",
        "                raise Exception('You need to define the spatial average size >= 1!')\n",
        "            hess_average = tf.reshape(hess, [resize, -1])\n",
        "            hess_average = tf.reduce_mean(hess_average, [0])\n",
        "            hess_average = tf.repeat(hess_average, resize)\n",
        "            hess_average = tf.reshape(hess_average, v.shape)\n",
        "            v.assign_add((math_ops.square(hess_average) - v) * (1 - coefficients['beta_2_t']))\n",
        "        \n",
        "        bias_correct1 = 1 - coefficients['beta_1_power']\n",
        "        bias_correct2 = 1 - coefficients['beta_2_power']\n",
        "\n",
        "        if self.weight_decay != 0:\n",
        "            var.assign_sub(coefficients['lr_t'] * self.weight_decay * var)\n",
        "\n",
        "        # denom = np.power(math_ops.sqrt(v / bias_correct2), self.hessian_power) + coefficients['epsilon']\n",
        "        denom = tf.math.pow(math_ops.sqrt(v / bias_correct2), self.hessian_power) + coefficients['epsilon']\n",
        "\n",
        "\n",
        "        var.assign_sub( coefficients['lr_t'] * m / bias_correct1 / denom  )\n",
        "\n",
        "    @def_function.function(experimental_compile=True)\n",
        "    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
        "        raise Exception('For now, we do not support sparse update yet.')\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(AdaHessian, self).get_config()\n",
        "        config.update({\n",
        "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
        "            'decay': self._serialize_hyperparameter('decay'),\n",
        "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
        "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
        "            'epsilon': self.epsilon,\n",
        "            'weight_decay': self.weight_decay\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def _distributed_apply(self, distribution, grads_hessian_and_vars, name, apply_state):\n",
        "        \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n",
        "\n",
        "        def apply_grad_to_update_var(var, grad, hess):\n",
        "            \"\"\"Apply gradient to variable.\"\"\"\n",
        "            if isinstance(var, ops.Tensor):\n",
        "                raise NotImplementedError(\"Trying to update a Tensor \", var)\n",
        "\n",
        "            apply_kwargs = {}\n",
        "\n",
        "            if \"apply_state\" in self._dense_apply_args:\n",
        "                apply_kwargs[\"apply_state\"] = apply_state\n",
        "            update_op = self._resource_apply_dense(grad, hess, var, **apply_kwargs)\n",
        "\n",
        "            if var.constraint is not None:\n",
        "                with ops.control_dependencies([update_op]):\n",
        "                    return var.assign(var.constraint(var))\n",
        "            else:\n",
        "                return update_op\n",
        "\n",
        "        eagerly_outside_functions = ops.executing_eagerly_outside_functions()\n",
        "        update_ops = []\n",
        "        with ops.name_scope(name or self._name, skip_on_eager=True):\n",
        "            for grad, hess, var in grads_hessian_and_vars:\n",
        "\n",
        "                def _assume_mirrored(grad, hess):\n",
        "                    if isinstance(grad, ds_values.PerReplica):\n",
        "                        return ds_values.Mirrored(grad.values), ds_values.Mirrored(hess.values)\n",
        "                    return grad, hess\n",
        "\n",
        "                grad, hess = nest.map_structure(_assume_mirrored, grad, hess)\n",
        "                # Colocate the update with variables to avoid unnecessary communication\n",
        "                # delays. See b/136304694.\n",
        "                with distribution.extended.colocate_vars_with(var):\n",
        "                    with ops.name_scope(\"update\" if eagerly_outside_functions else\n",
        "                                  \"update_\" + var.op.name, skip_on_eager=True):\n",
        "                        update_ops.extend(distribution.extended.update(\n",
        "                                var, apply_grad_to_update_var, args=(grad, hess), group=False))\n",
        "\n",
        "            any_symbolic = any(isinstance(i, ops.Operation) or\n",
        "                             tf_utils.is_symbolic_tensor(i) for i in update_ops)\n",
        "            if not context.executing_eagerly() or any_symbolic:\n",
        "                # If the current context is graph mode or any of the update ops are\n",
        "                # symbolic then the step update should be carried out under a graph\n",
        "                # context. (eager updates execute immediately)\n",
        "                with ops._get_graph_from_inputs(update_ops).as_default():  # pylint: disable=protected-access\n",
        "                    with ops.control_dependencies(update_ops):\n",
        "                        return self._iterations.assign_add(1, read_value=False)\n",
        "\n",
        "            return self._iterations.assign_add(1)"
      ],
      "metadata": {
        "id": "Lb2NpVZT0LZ4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "pkpVMn5L0RmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hls4ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haScapO0f9_O",
        "outputId": "729c7fac-3091-40a6-c324-c85e9ca1c8cf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hls4ml\n",
            "  Downloading hls4ml-0.6.0-py3-none-any.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 13.8 MB/s \n",
            "\u001b[?25hCollecting onnx>=1.4.0\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hls4ml) (1.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from hls4ml) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from hls4ml) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hls4ml) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.0->hls4ml) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.0->hls4ml) (4.2.0)\n",
            "Installing collected packages: onnx, hls4ml\n",
            "Successfully installed hls4ml-0.6.0 onnx-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hls4ml\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Activation, MaxPool1D, AvgPool1D, Flatten, Dense\n",
        "from tensorflow.nn import selu"
      ],
      "metadata": {
        "id": "tGCqgr5RgLVT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.set_floatx('float64')"
      ],
      "metadata": {
        "id": "yhMkhYhWFc0V"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hidden_X1(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Hidden_X1, self).__init__()\n",
        "    self.conv1_a = Conv1D(8, 7, padding='same')\n",
        "    self.selu_a = Activation(selu)\n",
        "    self.conv1_b = Conv1D(6, 7, padding='same')\n",
        "    self.selu_b = Activation(selu)\n",
        "    self.conv1_c = Conv1D(4, 5, padding='same')\n",
        "    self.selu_c = Activation(selu)\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.conv1_a(x)\n",
        "    x = self.selu_a(x)\n",
        "    x = self.conv1_b(x)\n",
        "    x = self.selu_b(x)\n",
        "    x = self.conv1_c(x)\n",
        "    x = self.selu_c(x)\n",
        "    return x\n",
        "  \n",
        "class Hidden_XFC(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Hidden_XFC, self).__init__()\n",
        "    self.dense_a = Dense(20, selu)\n",
        "    self.dense_b = Dense(20, selu)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense_a(x)\n",
        "    x = self.dense_b(x)\n",
        "    return x\n",
        "\n",
        "class Hidden_X2(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Hidden_X2, self).__init__()\n",
        "    self.max_pool_a = MaxPool1D(2, padding='same')\n",
        "    self.conv1_d = Conv1D(2, 5, padding='same')\n",
        "    self.selu_d = Activation(selu)\n",
        "    self.conv1_e = Conv1D(4, 5, padding='same')\n",
        "    self.selu_e = Activation(selu)\n",
        "    self.conv1_f = Conv1D(4, 5, padding='same')\n",
        "    self.selu_f = Activation(selu)\n",
        "    self.conv1_g = Conv1D(4, 5, padding='same')\n",
        "    self.selu_g = Activation(selu)\n",
        "    self.conv1_h = Conv1D(4, 5, padding='same')\n",
        "    self.selu_h = Activation(selu)\n",
        "    self.conv1_i = Conv1D(4, 5, padding='same')\n",
        "    self.selu_i = Activation(selu)\n",
        "    self.avg_pool_a = AvgPool1D(2, padding='same')\n",
        "    self.conv1_j = Conv1D(4, 3, padding='same')\n",
        "    self.selu_j = Activation(selu)\n",
        "    self.avg_pool_b = AvgPool1D(2, padding='same')\n",
        "    self.conv1_k = Conv1D(4, 3, padding='same')\n",
        "    self.selu_k = Activation(selu)\n",
        "    self.avg_pool_c = AvgPool1D(2, padding='same')\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.max_pool_a(x)\n",
        "    x = self.conv1_d(x)\n",
        "    x = self.selu_d(x)\n",
        "    x = self.conv1_e(x)\n",
        "    x = self.selu_e(x)\n",
        "    x = self.conv1_f(x)\n",
        "    x = self.selu_f(x)\n",
        "    x = self.conv1_g(x)\n",
        "    x = self.selu_g(x)\n",
        "    x = self.conv1_h(x)\n",
        "    x = self.selu_h(x)\n",
        "    x = self.conv1_i(x)\n",
        "    x = self.selu_i(x)\n",
        "    x = self.avg_pool_a(x)\n",
        "    x = self.conv1_j(x)\n",
        "    x = self.selu_j(x)\n",
        "    x = self.avg_pool_b(x)\n",
        "    x = self.conv1_k(x)\n",
        "    x = self.selu_k(x)\n",
        "    x = self.avg_pool_c(x)\n",
        "    return x\n",
        "  \n",
        "class Hidden_Embedding(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Hidden_Embedding, self).__init__()\n",
        "    self.dense_c = Dense(20, selu)\n",
        "    self.dense_d = Dense(8, selu)\n",
        "    self.dense_e = Dense(4, selu)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense_c(x)\n",
        "    x = self.dense_d(x)\n",
        "    x = self.dense_e(x)\n",
        "    return x\n",
        "\n",
        "class SHO_Model(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(SHO_Model, self).__init__()\n",
        "    self.hidden_x1 = Hidden_X1()\n",
        "    self.hidden_xfc = Hidden_XFC()\n",
        "    self.hidden_x2 = Hidden_X2()\n",
        "    self.flatten = Flatten()\n",
        "    self.hidden_embedding = Hidden_Embedding()\n",
        "\n",
        "  def call(self, x):\n",
        "    # x = tf.transpose(x, perm=[0, 2, 1])\n",
        "    x = self.hidden_x1(x)\n",
        "    # print(x.shape)\n",
        "    xfc = tf.reshape(x, [-1, 320])\n",
        "    xfc = self.hidden_xfc(xfc)\n",
        "    # x = tf.reshape(x, [-1, 2, 128])\n",
        "    # print(xfc.shape)\n",
        "    x = self.hidden_x2(x)\n",
        "    cnn_flat = self.flatten(x)\n",
        "    # print(cnn_flat.shape)\n",
        "    # print(xfc.shape)\n",
        "    encoded = tf.concat([cnn_flat, xfc], 1)\n",
        "    embedding = self.hidden_embedding(encoded)\n",
        "    # unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(np.sqrt(params_scaler.var_))),\\\n",
        "    #                     tf.convert_to_tensor(params_scaler.mean_))\n",
        "    \n",
        "    return embedding"
      ],
      "metadata": {
        "id": "hxvPllPDp7tl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SHO_Model()\n",
        "optimizer = AdaHessian(0.1)\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((data_train, params_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((data_test, params_test))\n",
        "\n",
        "BATCH_SIZE = 200\n",
        "SHUFFLE_BUFFER_SIZE = 516\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.fit(train_dataset, epochs=5, batch_size=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J0xddmsi8Bt",
        "outputId": "86642f2d-72a1-4e0a-ccb8-23808b5484b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5530/5530 [==============================] - 54s 7ms/step - loss: 0.1570\n",
            "Epoch 2/5\n",
            "5530/5530 [==============================] - 38s 7ms/step - loss: 0.1273\n",
            "Epoch 3/5\n",
            "5530/5530 [==============================] - 37s 7ms/step - loss: 0.1208\n",
            "Epoch 4/5\n",
            "5530/5530 [==============================] - 37s 7ms/step - loss: 0.1170\n",
            "Epoch 5/5\n",
            "5530/5530 [==============================] - 37s 7ms/step - loss: 0.1134\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f05a626d0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq9LPAuk7faV",
        "outputId": "10d0e468-fb36-4f3a-8b1a-237a3ea684f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while time.sleep(10):\n",
        "  print(gc.collect())"
      ],
      "metadata": {
        "id": "87Jop7ps8BLH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQRMXGi3f0fc"
      },
      "source": [
        "## PyTorch/Brevitas Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPaisGmqf0fn"
      },
      "source": [
        "### Reconstruction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID8hRuFgasMT",
        "outputId": "a62901e0-c39d-4719-ced0-bcc731dc6ca6"
      },
      "source": [
        "# installing brevitas\n",
        "!pip install brevitas"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting brevitas\n",
            "  Downloading brevitas-0.7.1-py3-none-any.whl (403 kB)\n",
            "\u001b[K     |████████████████████████████████| 403 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting future-annotations\n",
            "  Downloading future_annotations-1.0.0-py2.py3-none-any.whl (5.6 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5PUKW-FaxrH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "48e08ff0-77d6-462a-887d-a0b9e5d78353"
      },
      "source": [
        "# all components used from brevitas\n",
        "import brevitas.nn as qnn\n",
        "import brevitas\n",
        "from brevitas.quant import Int8Bias as BiasQuant\n",
        "from brevitas.quant_tensor import QuantTensor"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-15461ee2de42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# all components used from brevitas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbrevitas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mqnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrevitas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbrevitas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInt8Bias\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mBiasQuant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbrevitas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'brevitas'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTda5Y0xGGtF"
      },
      "source": [
        "We observed that model doesn't train well with bit_width equal to 4 and 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMzriJ9XATWw"
      },
      "source": [
        "# hyperparameters for the quantized model (feel free to fine-tune them)\n",
        "bit_width = 16\n",
        "weight_bit_width = 16\n",
        "scale = 0.1 # set randomly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRMnOSqMarKr"
      },
      "source": [
        "# quantized reconstruction model\n",
        "# only 3 Conv1D layers were kept from the initial model, and now they pass to 4 dense layers\n",
        "\n",
        "class SHO_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SHO_Model, self).__init__()\n",
        "\n",
        "        self.q_input = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.conv1 = qnn.QuantConv1d(in_channels=2, out_channels=8, kernel_size=7, weight_bit_width=weight_bit_width, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu1 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.conv2 = qnn.QuantConv1d(in_channels=8, out_channels=6, kernel_size=7, weight_bit_width=weight_bit_width, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu2 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.conv3 = qnn.QuantConv1d(in_channels=6, out_channels=4, kernel_size=5, weight_bit_width=weight_bit_width, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu3 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "\n",
        "        self.fc1 = qnn.QuantLinear(256, 20, weight_bit_width=weight_bit_width, bias=True, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu4 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.fc2 = qnn.QuantLinear(20, 16, weight_bit_width=weight_bit_width, bias=True, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu5 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.fc3 = qnn.QuantLinear(16, 8, weight_bit_width=weight_bit_width, bias=True, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu6 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.fc4 = qnn.QuantLinear(8, 4, weight_bit_width=weight_bit_width, bias=False)\n",
        "\n",
        "    def forward(self, x, n=-1):\n",
        "      x = self.q_input(x)\n",
        "      x = torch.swapaxes(x, 1, 2) # output shape - samples, (real, imag), frequency\n",
        "      x = self.relu1(self.conv1(QuantTensor(x, scale=torch.tensor(scale).cuda())))\n",
        "      x = self.relu2(self.conv2(QuantTensor(x, scale=torch.tensor(scale).cuda())))\n",
        "      x = self.relu3(self.conv3(QuantTensor(x, scale=torch.tensor(scale).cuda())))\n",
        "      x = torch.reshape(x, (n, 256)) # batch size, features\n",
        "      embedding = self.relu4(self.fc1(QuantTensor(x, scale=torch.tensor(scale).cuda())))\n",
        "      embedding = self.relu5(self.fc2(QuantTensor(embedding, scale=torch.tensor(scale).cuda())))\n",
        "      embedding = self.relu6(self.fc3(QuantTensor(embedding, scale=torch.tensor(scale).cuda())))\n",
        "      embedding = self.fc4(QuantTensor(embedding, scale=torch.tensor(scale).cuda()))\n",
        "\n",
        "      # corrects the scaling of the parameters\n",
        "      unscaled_param = embedding*torch.tensor(params_scaler.var_[0:4]**0.5).cuda() \\\n",
        "                              + torch.tensor(params_scaler.mean_[0:4]).cuda()\n",
        "\n",
        "      # passes to the pytorch fitting function \n",
        "      fits = SHO_fit_func_torch(unscaled_param, wvec_freq, device='cuda')\n",
        "\n",
        "      # extract and return real and imaginary      \n",
        "      real = torch.real(fits)\n",
        "      real_scaled = (real - torch.tensor(scaler_real.mean).cuda())\\\n",
        "                                        /torch.tensor(scaler_real.std).cuda()\n",
        "      imag = torch.imag(fits)\n",
        "      imag_scaled = (imag - torch.tensor(scaler_imag.mean).cuda())\\\n",
        "                                        /torch.tensor(scaler_imag.std).cuda()\n",
        "      out = torch.stack((real_scaled, imag_scaled), 2)\n",
        "      return out.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1QyxaNZf0fp"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64tESzxi4eOZ"
      },
      "source": [
        "model = SHO_Model().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIzoDUS5f0fp"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "loss_func = torch.nn.MSELoss()\n",
        "batch_size = 512\n",
        "\n",
        "# uncomment the code below if you want to try Adam\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
        "optimizer = AdaHessian(model.parameters(), lr=0.1) #0.1\n",
        "\n",
        "train_dataloader = DataLoader(data_train, batch_size=batch_size)\n",
        "\n",
        "epochs = 5 # feel free to change it\n",
        "    \n",
        "for epoch in range(epochs):\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_loss = 0.\n",
        "  total_num = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for train_batch in train_dataloader:\n",
        "      \n",
        "    pred = model(train_batch.cuda())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = loss_func(train_batch.cuda(), pred)\n",
        "    loss.backward(create_graph=True)\n",
        "    train_loss += loss.item() * pred.shape[0]\n",
        "    total_num += pred.shape[0]\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= total_num\n",
        "  # torch.save(model, '/content/drive/MyDrive/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/Trained Models/SHO Fitting/model_AdaHessian_quant.pt')\n",
        "  torch.save(model.state_dict(), '/content/drive/MyDrive/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/Trained Models/SHO Fitting/model_AdaHessian_quant.pt')\n",
        "\n",
        "\n",
        "  print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, epochs, train_loss))\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJc35R3emiN4"
      },
      "source": [
        "### Inference model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txUy_uy1ml-5"
      },
      "source": [
        "# quantized model for inference (with cut-off emperical function\n",
        "\n",
        "class SHO_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SHO_Model, self).__init__()\n",
        "\n",
        "        self.q_input = qnn.QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.conv1 = qnn.QuantConv1d(in_channels=2, out_channels=8, kernel_size=7, weight_bit_width=weight_bit_width, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu1 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.conv2 = qnn.QuantConv1d(in_channels=8, out_channels=6, kernel_size=7, weight_bit_width=weight_bit_width, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu2 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.conv3 = qnn.QuantConv1d(in_channels=6, out_channels=4, kernel_size=5, weight_bit_width=weight_bit_width, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu3 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "\n",
        "        self.fc1 = qnn.QuantLinear(256, 20, weight_bit_width=weight_bit_width, bias=True, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu4 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.fc2 = qnn.QuantLinear(20, 16, weight_bit_width=weight_bit_width, bias=True, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu5 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.fc3 = qnn.QuantLinear(16, 8, weight_bit_width=weight_bit_width, bias=True, bias_quant=BiasQuant, return_quant_tensor=True)\n",
        "        self.relu6 = brevitas.nn.QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
        "        self.fc4 = qnn.QuantLinear(8, 4, weight_bit_width=weight_bit_width, bias=False)\n",
        "\n",
        "    def forward(self, x, n=-1):\n",
        "      x = self.q_input(x)\n",
        "      x = torch.swapaxes(x, 1, 2) # output shape - samples, (real, imag), frequency\n",
        "      x = self.relu1(self.conv1(QuantTensor(x, scale=torch.tensor(1.0).cuda())))\n",
        "      x = self.relu2(self.conv2(QuantTensor(x, scale=torch.tensor(1.0).cuda())))\n",
        "      x = self.relu3(self.conv3(QuantTensor(x, scale=torch.tensor(1.0).cuda())))\n",
        "      x = torch.reshape(x, (n, 256)) # batch size, features\n",
        "      embedding = self.relu4(self.fc1(QuantTensor(x, scale=torch.tensor(1.0).cuda())))\n",
        "      embedding = self.relu5(self.fc2(QuantTensor(embedding, scale=torch.tensor(1.0).cuda())))\n",
        "      embedding = self.relu6(self.fc3(QuantTensor(embedding, scale=torch.tensor(1.0).cuda())))\n",
        "      embedding = self.fc4(QuantTensor(embedding, scale=torch.tensor(1.0).cuda()))\n",
        "\n",
        "      # corrects the scaling of the parameters\n",
        "      unscaled_param = embedding*torch.tensor(params_scaler.var_[0:4]**0.5).cuda() \\\n",
        "                              + torch.tensor(params_scaler.mean_[0:4]).cuda()\n",
        "\n",
        "      return unscaled_param.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x3uWlpjntKm"
      },
      "source": [
        "model_parameters = SHO_Model().cuda()\n",
        "# loads prior trained model\n",
        "model_parameters.load_state_dict(torch.load('/content/drive/MyDrive/Pre-trained-Deep-Learning-Models-For-Rapid-Analysis-Of-Piezoelectric-Hysteresis-Loops-SHO-Fitting/Trained Models/SHO Fitting/model_AdaHessian_quant.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDng7ZcIqxUo"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvytKHSDPKPP"
      },
      "source": [
        "#### Prediction on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h68xQUlIcUdL"
      },
      "source": [
        "# function to transform parameters to \n",
        "def transform_params(params_real, params_pred):\n",
        "    params_pred[:, 2] = np.where(\n",
        "        params_pred[:, 2] > 0, params_pred[:, 2], params_pred[:, 2]*-1)\n",
        "    params_real[:, 2] = np.where(\n",
        "        params_real[:, 2] > 0, params_real[:, 2], params_real[:, 2]*-1)\n",
        "    \n",
        "    params_pred[:, 0] = np.abs(params_pred[:, 0])\n",
        "\n",
        "    return params_real, params_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3TdL1zwFcmn"
      },
      "source": [
        "# prediction of parameters\n",
        "batch_size = 100000\n",
        "train_dataloader = DataLoader(data_, batch_size=batch_size)\n",
        "\n",
        "num_elements = len(train_dataloader.dataset)\n",
        "num_batches = len(train_dataloader)\n",
        "all_pred_params = torch.zeros_like(torch.tensor(params))\n",
        "\n",
        "for i, train_batch in enumerate(train_dataloader):\n",
        "  start = i*batch_size\n",
        "  end = start + batch_size\n",
        "\n",
        "  if i == num_batches - 1:\n",
        "    end = num_elements\n",
        "\n",
        "  pred_batch = model_parameters(train_batch.cuda())\n",
        "  all_pred_params[start:end] = pred_batch.cpu().detach()\n",
        "\n",
        "  del pred_batch\n",
        "  del train_batch\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "print(all_pred_params.shape)\n",
        "all_pred_params = all_pred_params.cpu().detach().numpy()\n",
        "print(all_pred_params.shape)\n",
        "\n",
        "params_copy = np.copy(params)\n",
        "all_pred_params_copy = np.copy(all_pred_params)\n",
        "\n",
        "all_params_transformed, all_pred_params_transformed = transform_params(params_copy, all_pred_params_copy)\n",
        "\n",
        "all_pred_params_scaled = params_scaler.transform(all_params_transformed)\n",
        "all_params_scaled = params_scaler.transform(all_pred_params_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-9hZhszFkQ5"
      },
      "source": [
        "print('Total MSE: ' + str(mean_squared_error(all_params_scaled, all_pred_params_scaled)))\n",
        "print('MSE of Amplitude: ' + str(mean_squared_error(all_params_scaled[:, 0], all_pred_params_scaled[:, 0])))\n",
        "print('MSE of Resonance: ' + str(mean_squared_error(all_params_scaled[:, 1], all_pred_params_scaled[:, 1])))\n",
        "print('MSE of Quality-Factor: ' + str(mean_squared_error(all_params_scaled[:, 2], all_pred_params_scaled[:, 2])))\n",
        "print('MSE of Phase: ' + str(mean_squared_error(all_params_scaled[:, 3], all_pred_params_scaled[:, 3])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcG7PtN_7Fg"
      },
      "source": [
        "### Check visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKVkMzMkqsMh"
      },
      "source": [
        "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
        "\n",
        "i = np.random.randint(0, data_.shape[0])\n",
        "out_pred = SHO_fit_func_torch(torch.tensor(np.atleast_2d(all_pred_params[i])), wvec_freq)\n",
        "magnitude_graph_pred, phase_graph_pred = convert_real_imag(out_pred)\n",
        "\n",
        "axs.plot(wvec_freq, magnitude_graph_initial[i, :], 'o', color='b')\n",
        "axs.plot(wvec_freq, magnitude_graph_pred[0, :], color='b')\n",
        "ax2 = axs.twinx()\n",
        "ax2.plot(wvec_freq, phase_graph_initial[i, :], 'o', color='r')\n",
        "ax2.plot(wvec_freq, phase_graph_pred[0, :], color='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o0xx5ko2Yha"
      },
      "source": [
        "#### Visualizations of the worst, best and medium loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UdKR7gfvHX9"
      },
      "source": [
        "data_real = np.copy(scaled_data_real)\n",
        "data_imag = np.copy(scaled_data_imag)\n",
        "\n",
        "data_real = scaler_real.inverse_transform(data_real)\n",
        "data_imag = scaler_imag.inverse_transform(data_imag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwc2dlxKu3yX"
      },
      "source": [
        "out_sho_nn = SHO_fit_func_torch(torch.tensor(\n",
        "                              all_pred_params[:]), wvec_freq)\n",
        "out_sho_lsqf = SHO_fit_func_torch(torch.tensor(\n",
        "                              params[:]), wvec_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM_oK0P8IUV0"
      },
      "source": [
        "mse_sho_nn = np.mean(np.square(scaled_data_real - scaler_real.transform(tf.convert_to_tensor(torch.real(out_sho_nn)))), 1) + np.mean(np.square(scaled_data_imag - scaler_imag.transform(tf.convert_to_tensor(torch.imag(out_sho_nn)))), 1)\n",
        "mse_sho_lsqf = np.mean(np.square(scaled_data_real - scaler_real.transform(tf.convert_to_tensor(torch.real(out_sho_lsqf)))), 1) + np.mean(np.square(scaled_data_imag - scaler_imag.transform(tf.convert_to_tensor(torch.imag(out_sho_lsqf)))), 1)\n",
        "highest_sho_nn = (-mse_sho_nn).argsort()[:]\n",
        "highest_sho_lsqf = (-mse_sho_lsqf).argsort()[:]\n",
        "print('MSE of SHO with NN: ' + str(np.mean(mse_sho_nn)))\n",
        "print('MSE of SHO with LSQF: ' + str(np.mean(mse_sho_lsqf)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXN0Z69J_9ua"
      },
      "source": [
        "plot_reconstruction_comparison_SHO('Worst', 0, wvec_freq, [params, all_pred_params], [highest_sho_lsqf, highest_sho_nn], [magnitude_graph_initial, phase_graph_initial])\n",
        "plt.savefig('Assets/Figures/worst_SHO.png', bbox_inches='tight', pad_inches=1)\n",
        "plt.savefig('Assets/Figures/worst_SHO.svg', bbox_inches='tight', pad_inches=1)\n",
        "plot_reconstruction_comparison_SHO('Medium', data_real.shape[0] // 2, wvec_freq, [params, all_pred_params], [highest_sho_lsqf, highest_sho_nn], [magnitude_graph_initial, phase_graph_initial])\n",
        "plt.savefig('Assets/Figures/medium_SHO.png', bbox_inches='tight', pad_inches=1)\n",
        "plt.savefig('Assets/Figures/medium_SHO.svg', bbox_inches='tight', pad_inches=1)\n",
        "plot_reconstruction_comparison_SHO('Best', -1, wvec_freq, [params, all_pred_params], [highest_sho_lsqf, highest_sho_nn], [magnitude_graph_initial, phase_graph_initial])\n",
        "plt.savefig('Assets/Figures/best_SHO.png', bbox_inches='tight', pad_inches=1)\n",
        "plt.savefig('Assets/Figures/best_SHO.svg', bbox_inches='tight', pad_inches=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}