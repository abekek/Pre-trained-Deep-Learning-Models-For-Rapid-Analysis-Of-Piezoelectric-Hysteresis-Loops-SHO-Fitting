{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abekek/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/blob/main/notebooks/benchmarking/Hysteresis_Loops_TRCG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_bMEJp3TVU9"
      },
      "source": [
        "## Initialization Code (code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BYn5s4SZNKd"
      },
      "source": [
        "### Mounting Google Drive (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0pjoQu69jTt",
        "outputId": "69d30d89-3bd5-43c2-90ec-b559e312a124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# if running on collaboratory set = True\n",
        "collaboratory = True\n",
        "\n",
        "if collaboratory:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else: \n",
        "    print('Running on local systems, if running on collaboratory please change above')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b6qZAWUuDj0",
        "outputId": "b1440f8e-52aa-45ec-dc24-c8950439f221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "# changes directory to your main google drive folder\n",
        "%cd drive/My\\ Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAz1uF_Q-LQG"
      },
      "outputs": [],
      "source": [
        "# Checks if the directory exists\n",
        "import os\n",
        "if os.path.exists(\"./Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks\"):\n",
        "    pass\n",
        "else:\n",
        "    !git clone https://github.com/abekek/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOTaD9Ko_0Xr",
        "outputId": "84feee4d-2414-4648-e1d3-07ab19c2ef42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks\n"
          ]
        }
      ],
      "source": [
        "# moves to the right directory\n",
        "%cd Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ewX6wa_3sy",
        "outputId": "d266a340-84e7-44d3-b26b-d52792507d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating 52b267f..8306052\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian.pt\n",
            "Please commit your changes or stash them before you merge.\n",
            "error: The following untracked working tree files would be overwritten by merge:\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian_noise_2.0_bs128.pt\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian_noise_4.0_bs128.pt\n",
            "\tTrained Models/SHO Fitting/model_AdaHessian_noise_7.0_bs128.pt\n",
            "Please move or remove them before you merge.\n",
            "Aborting\n"
          ]
        }
      ],
      "source": [
        "# checks if the directory is up to date\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVye1WEEZttP"
      },
      "source": [
        "### Installing Packages (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OlmHtCc-H8-",
        "outputId": "b84aa87b-21ed-46e4-ecf2-29cb4f6c269d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting atomicwrites==1.4.0\n",
            "  Downloading atomicwrites-1.4.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting attrs==20.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting autopep8==1.5.4\n",
            "  Downloading autopep8-1.5.4.tar.gz (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 29.0 MB/s \n",
            "\u001b[?25hCollecting certifi==2020.12.5\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 67.0 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.4\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting coverage==5.3.1\n",
            "  Downloading coverage-5.3.1-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.6.2)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.1.1)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 56.2 MB/s \n",
            "\u001b[?25hCollecting packaging==20.8\n",
            "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
            "Collecting Pillow==8.1.0\n",
            "  Downloading Pillow-8.1.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 52.7 MB/s \n",
            "\u001b[?25hCollecting pipreqs==0.4.10\n",
            "  Downloading pipreqs-0.4.10-py2.py3-none-any.whl (25 kB)\n",
            "Collecting pluggy==0.13.1\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting py==1.10.0\n",
            "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting pycodestyle==2.6.0\n",
            "  Downloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 343 kB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting pytest==6.2.1\n",
            "  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting pytest-cov==2.11.1\n",
            "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting toml==0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting urllib3==1.26.2\n",
            "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 61.1 MB/s \n",
            "\u001b[?25hCollecting yarg==0.1.9\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Collecting numpy_groupies==0.9.7\n",
            "  Downloading numpy_groupies-0.9.7.tar.gz (22 kB)\n",
            "Requirement already satisfied: dask==2.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 28)) (2.12.0)\n",
            "Requirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (1.1.0)\n",
            "Collecting tensorflow==2.4.1\n",
            "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3 MB 14 kB/s \n",
            "\u001b[?25hCollecting sidpy==0.0.5\n",
            "  Downloading sidpy-0.0.5-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.9 MB/s \n",
            "\u001b[?25hCollecting ipywidgets==7.6.3\n",
            "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 75.6 MB/s \n",
            "\u001b[?25hCollecting scikit_image==0.16.2\n",
            "  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.5 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting joblib==1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 76.3 MB/s \n",
            "\u001b[?25hCollecting pip==19.3.1\n",
            "  Downloading pip-19.3.1-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 49.9 MB/s \n",
            "\u001b[?25hCollecting ipython==7.22.0\n",
            "  Downloading ipython-7.22.0-py3-none-any.whl (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting pyqtgraph==0.12.1\n",
            "  Downloading pyqtgraph-0.12.1-py3-none-any.whl (939 kB)\n",
            "\u001b[K     |████████████████████████████████| 939 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting pyUSID==0.0.10\n",
            "  Downloading pyUSID-0.0.10-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting scikit_learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sphinx_rtd_theme==0.5.2\n",
            "  Downloading sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 58.2 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting BGlib==0.0.3\n",
            "  Downloading BGlib-0.0.3-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting pycroscopy==0.60.7\n",
            "  Downloading pycroscopy-0.60.7-py2.py3-none-any.whl (354 kB)\n",
            "\u001b[K     |████████████████████████████████| 354 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (3.2.2)\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (0.2.3.5)\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 12.4 MB/s \n",
            "\u001b[?25hCollecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting pygame==2.0.1\n",
            "  Downloading pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 60.3 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0.dev20210415+cu101 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0.dev20210415+cu101\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installs all of the requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Me0OrE2gO3Tq",
        "outputId": "37e997be-c56a-4759-ef66-128ceb5f62eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycroscopy==0.60.7\n",
            "  Using cached pycroscopy-0.60.7-py2.py3-none-any.whl (354 kB)\n",
            "Collecting sidpy>=0.0.1\n",
            "  Downloading sidpy-0.10-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.15.0)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.1.0)\n",
            "Collecting ipython>=6.0\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 26.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (0.18.3)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.1.0)\n",
            "Collecting pyUSID>=0.0.8\n",
            "  Using cached pyUSID-0.0.10-py2.py3-none-any.whl (66 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (7.1.2)\n",
            "Collecting igor\n",
            "  Downloading igor-0.3.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 256 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (3.1.0)\n",
            "Collecting gwyfile\n",
            "  Downloading gwyfile-0.2.0-py2.py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (5.4.8)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (3.2.2)\n",
            "Requirement already satisfied: ipywidgets>=5.2.2 in /usr/local/lib/python3.7/dist-packages (from pycroscopy==0.60.7) (7.7.1)\n",
            "Collecting numpy-groupies==0.9.7\n",
            "  Using cached numpy_groupies-0.9.7.tar.gz (22 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.6.0->pycroscopy==0.60.7) (1.5.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=6.0->pycroscopy==0.60.7) (57.4.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.30-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 29.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.1.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.10.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.6.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.3.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=6.0->pycroscopy==0.60.7) (0.8.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pycroscopy==0.60.7) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->pycroscopy==0.60.7) (4.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=6.0->pycroscopy==0.60.7) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->pycroscopy==0.60.7) (0.2.5)\n",
            "Collecting cytoolz\n",
            "  Downloading cytoolz-0.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (0.12.0)\n",
            "Requirement already satisfied: dask>=0.10 in /usr/local/lib/python3.7/dist-packages (from pyUSID>=0.0.8->pycroscopy==0.60.7) (2.12.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12.3->pycroscopy==0.60.7) (2.6.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->pycroscopy==0.60.7) (3.1.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from sidpy>=0.0.1->pycroscopy==0.60.7) (0.0)\n",
            "Collecting ipyfilechooser>=0.0.6\n",
            "  Downloading ipyfilechooser-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting distributed>=2.0.0psutil\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.2.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.7.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (3.13)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (21.3)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.0.4)\n",
            "Collecting dask>=0.10\n",
            "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.3.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.4.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.6.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.11.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=5.2.2->pycroscopy==0.60.7) (23.2.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.0.0psutil->sidpy>=0.0.1->pycroscopy==0.60.7) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (5.8.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (3.8.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=5.2.2->pycroscopy==0.60.7) (0.5.1)\n",
            "Building wheels for collected packages: numpy-groupies, igor\n",
            "  Building wheel for numpy-groupies (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy-groupies: filename=numpy_groupies-0.9.7-py3-none-any.whl size=21315 sha256=4110b547b3ea2afe600e322d5bdc280aa8e3c9149438e6d83ff7a33987e46db5\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/05/e1/250ebec6656f2a0bc1141d5c185876dcd74e7e47740613c9d6\n",
            "  Building wheel for igor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for igor: filename=igor-0.3-py3-none-any.whl size=52116 sha256=c3cbf09049c76d63e87af9f01400e8d04128e52c4fc87af97aa58a8f2d00af7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/27/d2/31051f074caeea50e0d11890508c40e9456af990a9350d0fb6\n",
            "Successfully built numpy-groupies igor\n",
            "Installing collected packages: prompt-toolkit, ipython, locket, pyyaml, partd, fsspec, cloudpickle, dask, ipyfilechooser, distributed, cytoolz, sidpy, pyUSID, numpy-groupies, igor, gwyfile, pycroscopy\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.30 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.1.0 cytoolz-0.12.0 dask-2022.2.0 distributed-2022.2.0 fsspec-2022.5.0 gwyfile-0.2.0 igor-0.3 ipyfilechooser-0.6.0 ipython-7.34.0 locket-1.0.0 numpy-groupies-0.9.7 partd-1.2.0 prompt-toolkit-3.0.30 pyUSID-0.0.10 pycroscopy-0.60.7 pyyaml-6.0 sidpy-0.10\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 16.7 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.6 six-1.16.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# installing PyTorch's Nightly version\n",
        "!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html -U\n",
        "\n",
        "!pip install pycroscopy==0.60.7\n",
        "\n",
        "if os.path.exists(\"./BGlib\"):\n",
        "    pass\n",
        "else:\n",
        "    !git clone https://github.com/pycroscopy/BGlib.git\n",
        "    %cd BGlib/\n",
        "    !git tag -l\n",
        "    !git checkout 0.0.3\n",
        "    !git branch -D master\n",
        "    !git checkout -b master\n",
        "    %cd ..\n",
        "\n",
        "# downgrading the h5py version\n",
        "!pip install 'h5py==2.10.0' --force-reinstal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ff9GT0VaP-y"
      },
      "source": [
        "### Importing Packages (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "eb6fWK4LaTAw",
        "outputId": "08815b70-f3f0-474a-fde6-c6120452457b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1949696/45929032 bytes (4.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4161536/45929032 bytes (9.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7200768/45929032 bytes (15.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9904128/45929032 bytes (21.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12058624/45929032 bytes (26.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14663680/45929032 bytes (31.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17121280/45929032 bytes (37.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19537920/45929032 bytes (42.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22274048/45929032 bytes (48.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24715264/45929032 bytes (53.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27639808/45929032 bytes (60.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29777920/45929032 bytes (64.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32120832/45929032 bytes (69.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34537472/45929032 bytes (75.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36757504/45929032 bytes (80.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38330368/45929032 bytes (83.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41156608/45929032 bytes (89.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43458560/45929032 bytes (94.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numba import jit\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import argparse\n",
        "import seaborn as sns\n",
        "from scipy.signal import resample\n",
        "from scipy import fftpack\n",
        "from scipy import io\n",
        "from scipy import special\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.layers import (Attention, Dense, Conv1D, Convolution2D, \n",
        "                                     GRU, LSTM, Bidirectional, TimeDistributed,\n",
        "                                     Dropout, Flatten, LayerNormalization, \n",
        "                                     RepeatVector, Reshape, MaxPooling1D, \n",
        "                                     UpSampling1D, BatchNormalization, Activation)\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from multiprocessing import Pool, Process\n",
        "import multiprocessing as mp\n",
        "from moviepy.editor import *\n",
        "import glob\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import gc\n",
        "import sidpy\n",
        "from BGlib.BGlib import be as belib\n",
        " \n",
        "# set up notebook to show plots within the notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "\n",
        "# Import necessary libraries:\n",
        "# General utilities:\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Computation:\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization:\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "# Finally, pycroscopy itself\n",
        "sys.path.append('../../../')\n",
        "import pyUSID as usid\n",
        "from codes.util.preprocessing_global_standard_scaler import global_standard_scaler\n",
        "from sidpy.hdf.hdf_utils import write_simple_attrs, get_attr\n",
        "from pyUSID.io.hdf_utils import create_results_group, write_main_dataset, write_reduced_anc_dsets, create_empty_dataset, reshape_to_n_dims, get_auxiliary_datasets\n",
        "from pyUSID.io.usi_data import USIDataset\n",
        "from pyUSID.io import Dimension\n",
        "\n",
        "from codes.util.file import print_tree\n",
        "from codes.util.core import SHO_fit_func_torch, loop_fitting_function, loop_fitting_function_tf, computeDotProducts, normOfVar, fit_loop_function, computeTime, conventional_fit_loop_function\n",
        "from codes.viz.plot import plot_best_worst_SHO, make_movie, plot_best_worst_loops, plot_reconstruction_comparison_SHO, plot_reconstruction_comparison_loops\n",
        "from codes.util.postprocessing import transform_params, convert_real_imag\n",
        "from codes.util.preprocessing_global_scaler import global_scaler\n",
        "from codes.processing.filters import range_filter, clean_interpolate, interpolate_missing_points\n",
        "from codes.algorithm.TRPCGOptimizerv2 import TRPCGOptimizerv2\n",
        "from codes.algorithm.AdaHessian import AdaHessian\n",
        "\n",
        "import numpy.lib.recfunctions as rfn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8kofx1Ta8tA"
      },
      "source": [
        "### Setting Defaults (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yuu-vFWOFE8"
      },
      "outputs": [],
      "source": [
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsXoBN6cNuRB",
        "outputId": "4b34fc70-0df9-482b-cb29-20ad3f85470c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jul 19 17:32:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# shows the GPU that is available and the resources\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_n-dq6La9Po",
        "outputId": "880ca312-22bc-4148-d057-7d63bff8280a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "gpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol1F9fy7Mo1v"
      },
      "source": [
        "### Loading data for SHO fitting (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "Or1SY2WdMrkA",
        "outputId": "9eaea157-6bcb-4ebc-ca46-85ba93e3115a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1Q2Qo_1VGlCsVOTjQpZlE5tjoIV1etVe2\n",
            "To: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/data_file.h5\n",
            "100%|██████████| 1.80G/1.80G [00:20<00:00, 87.8MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data_file.h5'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#TODO place this file on Zenodo before publication \n",
        "# downloads the original experiment file\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1Q2Qo_1VGlCsVOTjQpZlE5tjoIV1etVe2', 'data_file.h5', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRIcp94ufPLz",
        "outputId": "d3b371ab-fb41-4f95-aa34-47528e560c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n",
            "├ Measurement_000\n",
            "  ---------------\n",
            "  ├ Channel_000\n",
            "    -----------\n",
            "    ├ Bin_FFT\n",
            "    ├ Bin_Frequencies\n",
            "    ├ Bin_Indices\n",
            "    ├ Bin_Step\n",
            "    ├ Bin_Wfm_Type\n",
            "    ├ Excitation_Waveform\n",
            "    ├ Noise_Floor\n",
            "    ├ Position_Indices\n",
            "    ├ Position_Values\n",
            "    ├ Raw_Data\n",
            "    ├ Raw_Data-SHO_Fit_000\n",
            "      --------------------\n",
            "      ├ Fit\n",
            "      ├ Guess\n",
            "      ├ Spectroscopic_Indices\n",
            "      ├ Spectroscopic_Values\n",
            "      ├ completed_fit_positions\n",
            "      ├ completed_guess_positions\n",
            "    ├ Spatially_Averaged_Plot_Group_000\n",
            "      ---------------------------------\n",
            "      ├ Bin_Frequencies\n",
            "      ├ Max_Response\n",
            "      ├ Mean_Spectrogram\n",
            "      ├ Min_Response\n",
            "      ├ Spectroscopic_Parameter\n",
            "      ├ Step_Averaged_Response\n",
            "    ├ Spatially_Averaged_Plot_Group_001\n",
            "      ---------------------------------\n",
            "      ├ Bin_Frequencies\n",
            "      ├ Max_Response\n",
            "      ├ Mean_Spectrogram\n",
            "      ├ Min_Response\n",
            "      ├ Spectroscopic_Parameter\n",
            "      ├ Step_Averaged_Response\n",
            "    ├ Spectroscopic_Indices\n",
            "    ├ Spectroscopic_Values\n",
            "    ├ UDVS\n",
            "    ├ UDVS_Indices\n"
          ]
        }
      ],
      "source": [
        "# Opens the translated file\n",
        "h5_f = h5py.File('./data_file.h5', 'r+')\n",
        "\n",
        "#Inspects the h5 file\n",
        "usid.hdf_utils.print_tree(h5_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49u7-Tiqf1eD",
        "outputId": "eb2c69db-8378-458e-8b6f-0eb4a6e1f5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets and datagroups within the file:\n",
            "------------------------------------\n",
            "/\n",
            "/Measurement_000\n",
            "/Measurement_000/Channel_000\n",
            "/Measurement_000/Channel_000/Bin_FFT\n",
            "/Measurement_000/Channel_000/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Bin_Indices\n",
            "/Measurement_000/Channel_000/Bin_Step\n",
            "/Measurement_000/Channel_000/Bin_Wfm_Type\n",
            "/Measurement_000/Channel_000/Excitation_Waveform\n",
            "/Measurement_000/Channel_000/Noise_Floor\n",
            "/Measurement_000/Channel_000/Position_Indices\n",
            "/Measurement_000/Channel_000/Position_Values\n",
            "/Measurement_000/Channel_000/Raw_Data\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Fit\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Indices\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_fit_positions\n",
            "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/completed_guess_positions\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Max_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Mean_Spectrogram\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Min_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Spectroscopic_Parameter\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Step_Averaged_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Bin_Frequencies\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Max_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Mean_Spectrogram\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Min_Response\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Spectroscopic_Parameter\n",
            "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Step_Averaged_Response\n",
            "/Measurement_000/Channel_000/Spectroscopic_Indices\n",
            "/Measurement_000/Channel_000/Spectroscopic_Values\n",
            "/Measurement_000/Channel_000/UDVS\n",
            "/Measurement_000/Channel_000/UDVS_Indices\n",
            "\n",
            "The main dataset:\n",
            "------------------------------------\n",
            "<HDF5 file \"data_file.h5\" (mode r+)>\n",
            "\n",
            "The ancillary datasets:\n",
            "------------------------------------\n",
            "<HDF5 dataset \"Position_Indices\": shape (3600, 2), type \"<u4\">\n",
            "<HDF5 dataset \"Position_Values\": shape (3600, 2), type \"<f4\">\n",
            "<HDF5 dataset \"Spectroscopic_Indices\": shape (4, 63360), type \"<u4\">\n",
            "<HDF5 dataset \"Spectroscopic_Values\": shape (4, 63360), type \"<f4\">\n",
            "\n",
            "Metadata or attributes in a datagroup\n",
            "------------------------------------\n",
            "BE_actual_duration_[s] : 0.004\n",
            "BE_amplitude_[V] : 1\n",
            "BE_auto_smoothing : auto smoothing on\n",
            "BE_band_edge_smoothing_[s] : 4832.1\n",
            "BE_band_edge_trim : 0.094742\n",
            "BE_band_width_[Hz] : 200000\n",
            "BE_bins_per_band : 0\n",
            "BE_center_frequency_[Hz] : 1310000\n",
            "BE_desired_duration_[s] : 0.004\n",
            "BE_phase_content : chirp-sinc hybrid\n",
            "BE_phase_variation : 1\n",
            "BE_points_per_BE_wave : 0\n",
            "BE_repeats : 4\n",
            "FORC_V_high1_[V] : 1\n",
            "FORC_V_high2_[V] : 10\n",
            "FORC_V_low1_[V] : -1\n",
            "FORC_V_low2_[V] : -10\n",
            "FORC_num_of_FORC_cycles : 1\n",
            "FORC_num_of_FORC_repeats : 1\n",
            "File_MDAQ_version : MDAQ_VS_090915_01\n",
            "File_date_and_time : 18-Sep-2015 18:32:14\n",
            "File_file_name : SP128_NSO\n",
            "File_file_path : C:\\Users\\Asylum User\\Documents\\Users\\Agar\\SP128_NSO\\\n",
            "File_file_suffix : 99\n",
            "IO_AO_amplifier : 10\n",
            "IO_AO_range_[V] : +/- 10\n",
            "IO_Analog_Input_1 : +/- .1V, FFT\n",
            "IO_Analog_Input_2 : off\n",
            "IO_Analog_Input_3 : off\n",
            "IO_Analog_Input_4 : off\n",
            "IO_DAQ_platform : NI 6115\n",
            "IO_rate_[Hz] : 4000000\n",
            "VS_amplitude_[V] : 16\n",
            "VS_cycle_fraction : full\n",
            "VS_cycle_phase_shift : 0\n",
            "VS_measure_in_field_loops : in and out-of-field\n",
            "VS_mode : DC modulation mode\n",
            "VS_number_of_cycles : 2\n",
            "VS_offset_[V] : 0\n",
            "VS_read_voltage_[V] : 0\n",
            "VS_set_pulse_amplitude[V] : 0\n",
            "VS_set_pulse_duration[s] : 0.002\n",
            "VS_step_edge_smoothing_[s] : 0.001\n",
            "VS_steps_per_full_cycle : 96\n",
            "data_type : BEPSData\n",
            "grid_/single : grid\n",
            "grid_contact_set_point_[V] : 1\n",
            "grid_current_col : 1\n",
            "grid_current_row : 1\n",
            "grid_cycle_time_[s] : 10\n",
            "grid_measuring : 0\n",
            "grid_moving : 0\n",
            "grid_num_cols : 60\n",
            "grid_num_rows : 60\n",
            "grid_settle_time_[s] : 0.15\n",
            "grid_time_remaining_[h;m;s] : 10\n",
            "grid_total_time_[h;m;s] : 10\n",
            "grid_transit_set_point_[V] : 0.1\n",
            "grid_transit_time_[s] : 0.15\n",
            "num_bins : 165\n",
            "num_pix : 3600\n",
            "num_udvs_steps : 384\n"
          ]
        }
      ],
      "source": [
        "print('Datasets and datagroups within the file:\\n------------------------------------')\n",
        "print_tree(h5_f.file)\n",
        " \n",
        "print('\\nThe main dataset:\\n------------------------------------')\n",
        "print(h5_f)\n",
        "print('\\nThe ancillary datasets:\\n------------------------------------')\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Position_Indices'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Position_Values'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Indices'])\n",
        "print(h5_f.file['/Measurement_000/Channel_000/Spectroscopic_Values'])\n",
        "\n",
        "print('\\nMetadata or attributes in a datagroup\\n------------------------------------')\n",
        "for key in h5_f.file['/Measurement_000'].attrs:\n",
        "    print('{} : {}'.format(key, h5_f.file['/Measurement_000'].attrs[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyxiBxNNXMs8"
      },
      "outputs": [],
      "source": [
        "# This sets the cores for computing the conventional fits.\n",
        "# Data included has been fit with this method. \n",
        "\n",
        "# Maximum memory to use, in Mbs. Default = 1024\n",
        "max_mem = 1024 * 8 \n",
        "\n",
        "# Number of logical cores to use in fitting.  None uses all but 2 available cores.\n",
        "max_cores = None \n",
        "\n",
        "# Note that if you set this to True, visualization is unlikely to work!\n",
        "results_to_new_file = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJAHHp4HCRV-"
      },
      "source": [
        "# Main Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzqhoXwCOd3z"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"./Trained Models/Piezoresponse\"):\n",
        "    pass\n",
        "else:\n",
        "    os.makedirs(\"Trained Models/Piezoresponse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4gQ0b2rn4kI"
      },
      "source": [
        "## Extracting Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-6EgZqZn3dg"
      },
      "outputs": [],
      "source": [
        "# number of samples per SHO fit\n",
        "num_bins = h5_f['Measurement_000'].attrs['num_bins'] \n",
        "\n",
        "# number of pixels in the image\n",
        "num_pix = h5_f['Measurement_000'].attrs['num_pix'] \n",
        "\n",
        "# number of pixels in x and y dimensions\n",
        "num_pix_1d = int(np.sqrt(num_pix)) \n",
        "\n",
        "# number of DC voltage steps \n",
        "voltage_steps = h5_f['Measurement_000'].attrs['num_udvs_steps']\n",
        "\n",
        "# sampling rate\n",
        "sampling_rate = h5_f['Measurement_000'].attrs['IO_rate_[Hz]']\n",
        "\n",
        "# BE bandwidth\n",
        "be_bandwidth = h5_f['Measurement_000'].attrs['BE_band_width_[Hz]']\n",
        "\n",
        "# BE center frequency\n",
        "be_center_frequency = h5_f['Measurement_000'].attrs['BE_center_frequency_[Hz]']\n",
        "\n",
        "# Frequency Vector in Hz\n",
        "frequency_bin = h5_f['Measurement_000']['Channel_000']['Bin_Frequencies'][:]\n",
        "\n",
        "# Resampled frequency vector\n",
        "wvec_freq = resample(frequency_bin, 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1lWNU9CAl11"
      },
      "outputs": [],
      "source": [
        "# get raw data (real and imaginary combined)\n",
        "raw_data = h5_f['Measurement_000']['Channel_000']['Raw_Data']\n",
        "raw_data_resampled = resample(np.array(raw_data).reshape(-1 , 165), 80, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yULSaZnyREWD"
      },
      "outputs": [],
      "source": [
        "# extracting the excitation waveform\n",
        "be_waveform = h5_f['Measurement_000']['Channel_000']['Excitation_Waveform']\n",
        "\n",
        "# extracting spectroscopic values\n",
        "spectroscopic_values = h5_f['Measurement_000']['Channel_000']['Spectroscopic_Values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PldHXtVFOyhH",
        "outputId": "bd24b10c-85f2-4797-c1ed-f4e8e0118ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consider calling test() to check results before calling compute() which computes on the entire dataset and writes results to the HDF5 file\n",
            "\tThis class (likely) supports interruption and resuming of computations!\n",
            "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
            "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
            "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
            "\n",
            "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
            "Finished processing the entire dataset!\n",
            "\n",
            "Note: Loop_Fit has already been performed with the same parameters before. These results will be returned by compute() by default. Set override to True to force fresh computation\n",
            "\n",
            "[<HDF5 group \"/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess-Loop_Fit_000\" (7 members)>]\n",
            "Resuming computation. 0% completed already\n",
            "\tThis class (likely) supports interruption and resuming of computations!\n",
            "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
            "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
            "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
            "\n",
            "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
            "Finished processing the entire dataset!\n"
          ]
        }
      ],
      "source": [
        "h5_main = h5_f['Measurement_000']['Channel_000']['Raw_Data-SHO_Fit_000']['Guess']\n",
        "h5_loop_fit, h5_loop_group = fit_loop_function(h5_f, h5_main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeQr9r8VFx1S",
        "outputId": "250c137d-cd31-46da-860f-d354224c408d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No spectroscopic datasets found as attributes of /Measurement_000/Channel_000/Position_Indices\n",
            "No position datasets found as attributes of /Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values\n"
          ]
        }
      ],
      "source": [
        "proj_nd_shifted = conventional_fit_loop_function(h5_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFdLN0VZrCSr"
      },
      "outputs": [],
      "source": [
        "# getting parameters for the hysteresis loops\n",
        "params = np.zeros((num_pix, 9))\n",
        "params_names = ['a_0', 'a_1', 'a_2', 'a_3', 'a_4', 'b_0', 'b_1', 'b_2', 'b_3']\n",
        "\n",
        "for i in range(9):\n",
        "  params[:, i] = np.array(h5_f['Measurement_000']['Channel_000']['Raw_Data-SHO_Fit_000']['Guess-Loop_Fit_000']['Fit'][params_names[i]][:, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xsM6xKrhxEz"
      },
      "outputs": [],
      "source": [
        "# voltage vector\n",
        "V = np.swapaxes(np.atleast_2d(h5_f['Measurement_000']['Channel_000']['UDVS'][::2][:, 1][24:120]), 0, 1).astype(np.float64)\n",
        "\n",
        "# to set up a type of loop_fitting function to use. Possible options: ['9 parameters', '13 parameters']\n",
        "func_type = '9 parameters'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLXJAPK171xb"
      },
      "outputs": [],
      "source": [
        "# creating fits from fitting function and preparing for visualization\n",
        "loop_fit_results = np.reshape(loop_fitting_function(func_type, V, params), (96, num_pix_1d, num_pix_1d))\n",
        "proj_nd_shifted_transposed = np.transpose(proj_nd_shifted,(1,0,2,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "hrHRndQYKfM1",
        "outputId": "924ca602-6a51-42b5-8227-5713d96a2c5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2547c39a50>]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAD4CAYAAAAkRnsLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e+RLooFEbHQFLsuSkRxbauAggULq2ADRREVUXFFUdfCumsHG1KkiEpAQMXIKtVVQUU2NEURjdgoKyC9BZKc3x/n5pcQJn2SO+V8nmeemblz550zKXPm7aKqOOecc9GwR9gBOOecSxyeVJxzzkWNJxXnnHNR40nFOedc1HhScc45FzVVww4gTAcccIA2btw47DCccy6uzJ07d42q1ov0WFInlcaNG5Oenh52GM45F1dE5JfCHvPmL+ecc1HjScU551zUeFJxzjkXNZ5UnHPORY0nFeecc1HjScU551zUeFJxzjkXNZ5UnHNJJzsbRo2CP/4IO5LE40nFOZd0nnkGunaFa64B31IqujypOOeSyvz58PDD0LQpTJkCAweGHVFi8aTinIspU6bAXXdBZmb0y96+Ha69Fg44AObMgXbt4N57YfHi6L9WtH3wAXz4YdhRFM+TinMuZrz9Nlx0EbzwAtxyS/Sbph54AL79FkaOhLp1YcQI2GsvawbbsSO6rxUtO3dC795w4YV2GTky7IiKFpWkIiIXiMgSEckQkfsjPF5DRN4KHv9SRBrne6xvcHyJiJxfXJki0jM4piJyQL7jIiIvBo99JSInR+O9OecqR2oqXHUVtGwJffpYR/ozz0Sv/I8+ggED4Pbb4fzgk+agg+DVV61J7JFHovda0fL779CmTV7cbdvCjTfC0KFhR1YEVS3XBagC/Ag0BaoDC4FjC5xzGzA4uN0JeCu4fWxwfg2gSVBOlaLKBE4CGgM/Awfke432wIeAAKcBXxYXe4sWLdQ5F77hw1VFVM85R3XTJtWcHNVOnezYu++Wv/x161QPO0z1yCNVt2zZ/fGbbrLX+vTT0pe9erXq4MH2HqZMUV20yF4vJ6d8MX/xherBB6vWqqX6xht2bNs21QsvVAXVl14qX/nlAaRrYTmhsAdKegFaAVPy3e8L9C1wzhSgVXC7KrAm+PDf5dzc80pYZsGkMgTonO/+EqBBUbF7UnEufK+8Yp9Ebdvu+oG/datqy5aqe+6pOn9++V7j2mtVq1RRnTMn8uObNqkefrhqo0aq69eXrMx581RvuEG1Rg2Lv+Cldm1LYu3bqz74oOqECaoZGcUnm5wc+5lUq6batKnqggW7Pp6ZqXrppfYa/fsXXdb27eVPbpEUlVSisZ/KIcBv+e4vA04t7BxVzRKRDUDd4PjsAs89JLhdXJklieMQYGX+k0SkO9AdoGHDhsUU6ZyrSAMGWH/BxRfDuHFQs2beY7VqwcSJ1hx28cXWsd6gQelfY9w4ePNNeOwxOOWUyOfstZedc8YZcMcd8Prrkc/buRPeeQdeegk++wz23BNuuAFuvdXKWL4877JsmV0vXmyDD7KzrYw6daB5czjmGKhWDUTssscedr10Kbz3ng0iGD0a9ttv1xiqV7f3dPXV9rPbsQPuu88ey8qCL7+EqVPtMmcOHHKINZu1aQPnnWeDFCpS0m3SpapDgaEAKSkpPkLduZA88YR1nF9xhfWnVK+++zkNGkBamn3YX3opfPyxJZuS+vln6NHDEtMDDxR97mmnwUMPWfLZay9LGJs3w5YtedeLFsHKlTYcuX9/Syj77ptXRtOmkcvets2eu2CB9d/Mn2+DEnJy7JK/jlOlivXvPPywJZpIqlWDMWPs+v77ISPDJnLOmAEbN9rzWra0kW0ZGfZaw4db0mrRwhLMBRfAWWeV/GdZUtFIKsuBw/LdPzQ4FumcZSJSFdgH+KOY5xZXZlnicM6FTBX69YNHH7Vv26NGQdUiPolOOsm+sV92mX2Ip6YW/mGba8MGePppqwlVqQJvvFH0a+R66CH4/HMYNMiSyl57Qe3adr3XXtCqlXWUt2tXfAz51apltaTCakplUbWqva9q1WDYMGjY0AY6tG0L554L++8P338PjRtbrOnpMG2a1WCeftoGLsyeXezLlF5h7WIlvWCJaSnW0Z7bqX5cgXNuZ9eO+nHB7ePYtaN+KdZJX5Iyf2bXPpUL2bWjfk5xsXufinOVKyfH+hdAtWtX1ayskj/3iSfseQ0bqvbpY/0sBfsLtm9XHTBAtW5dO7dzZ9Uffyx9jNnZpXtO2Nas2fVnkZmp+o9/qFavbj+3gjZsUF28uOyvR0V21Fv5tAe+x0ZsPRgc6wdcEtyuCYwHMoA5QNN8z30weN4SoF1RZQbHe2H9JVnACmBYcFyAgcH5XwMpxcXtScW50lmxQnXz5rI9NyfHkgHYaKvSfnDn5KiOHWsd31WrWjlHH63ar5/qkiWqo0erNmlix1u3Vp07t2xxxrvZs1WPP95+DldeqbpyZfRfo8KTSrxePKk4V7zly1Wff161VSv7xDj9dNWdO0tXRk6O6t132/NvvbX8NYHVq1UHDVI96ywbCpzbI9G8uQ3rTUabNqn26mU/j0MPVU1Lq7jX8qTiScW5Ulm1yoa1nn123of2n/5kNQxQfeSRkpeVk6Pas6c9r1ev6A9x/e031RdesFpMvDVbRcsHH1izoIjq7bdb81ZFKiqpJN3oL+dc0dLS4K9/taGqxxxjHepXXQVHHWWPZ2bCP/5hI4j+/Oeiy8rJgdtugyFD4J57bIa8SHTjPfRQ6NUrumXGk3Hj7PdzzDEwc2bxv5OKJpZ0klNKSoqmp6eHHYZzMWPaNFt7q3lzG1F0/PG7J4GNG21EVnY2LFwI++wTuaysLOjWzeZ83H8//Otf0U8ozoZCb9hgw5Vr1Kic1xSRuaqaEukxX1DSOQfYt9wOHewb7+TJcMIJkZNAnTo2xHfZMquFRLJjB3TqZAmlXz9PKBVlwQKb7NijR+UllOJ4UnEuzmVm2sq1F15oH/IjRlgNYufOkpeRnm7Pb9jQ5jEUnMVd0Gmn2QS91FSbiZ7ftm02UfHtt+G55+Dvf/eEUlGGDLFVCK6/PuxI8njzlzd/uTi1Zg0MHgwvv2yr2TZtasc2brTHa9a0ZqwWLWwyXLt2kWejf/01nHOONWPNnGnLepREdrY9b+FC+8bctCls2gSXXAKffGKxde8erXfrCtq0CQ4+2FYkeO21yn1tb/5yLg5kZdkyG7feah8WuWs23XWXLc/++efWdr5kiZ3TsKHVAk4+GaZPt+U41q2zWdSpqVZrqV7dZqxfcQXUq2cduhMm2JIjYOe2aWPJZsaMkicUsJnqb75ps7WvvRZWr7ayZs60md6eUCrWmDG2fEyPHmFHsiuvqXhNxYVo505bLmPCBHj3XVu/ac89oX17u/7mG1uQcOvWXZ9XowZcdx3cfTcce2zRr5GVZTWHCRNsMcRVqyyJtG9vCw5u3w6ffgpHH1229zB2LHTubGtgbd1q9y+7rGxluZJRtRpodrbVEiu7ebGomooPKXYuJJMn2zf8P/6Avfe2lXg7drQNpPbcM++8nBz45RdLMN98Y8e6doX69Uv2OlWr2uq0551nTWUzZ8L48dbnkZvUyppQwDrkp061ZJKWlrcBlqs46em2KOUrr8Ref5XXVLym4kKQmgpdutiQ3X79rNko/7LvlSE72zr58yewssrJsTb+woYXu+jq1g3eegtWrLDReJXNayrOxZAXXrB+kr/8xfYLCeNDAaxPJBoJBaxfxRNK5Vi/3mqF11wT3t9OUbyj3rlKogoPPmgJ5fLL4YMPYvNDwcW2N9+0vqtbbgk7ksi8puJcJcjKshFbw4bZqKhXXrGagnOloWpDtVNSrKM+FnlNxbkKkpVlc0aWL7e1tIYNs02gBg/2hOIKt2JF4RNXP//cBmvE2jDi/Lym4lwULFsGN91kW8Zu2WLNEzt27HrOCy8k98KHrng//ADHHWcj+3r1gptv3nW74sGDrcm0U6fwYiyOJxXnyunjj+HKK215kr/+NW9/89q1865POCH81WNd7HvhBRsifMQR0KePjQzs1g3uvNOSyfjx9uWldu2wIy2cJxXnykgVnn8e7r0XmjWzyYvlme/hktvatbaG2zXX2Ppt8+bBgAEwcCC89JJNcs3MjN0O+lzep+JcGWzZYv/8vXvbWldffukJxZXP0KHWbHr33Xb/5JNtuZuffrIvLsuW2RpuJ5wQbpzF8cmPPvnRldKPP9oyJIsWwT//aXuFxNqsZhdfduyAJk2sP2Xq1MjnZGbadSwsce+TH52LksWL4fTTbbLf5Mm24KNz5ZU7O3748MLPiYVkUhKeVJwrIVW44w6rlaSn2zdL58pLFfr3tz6TRFg3zZOKcyX07ru2PPzLL3tCcdHz8ce20vCwYYnRjOod9c6VwLZtcM891kka66NvXHzp39/2urnmmrAjiQ6vqThXAs8+Cz//bMvEV/X/GhclS5bApEnw6KOVv0p1RfGainPF+O03eOIJ2+vkL38JOxqXSJ5/3jrgb7017Eiix5OKc8Xo08c6U599NuxIXCJZs8a2er7uOjjwwLCjiR5PKs4V4dNPbe+K++6DRo3CjsYlkiFDrK/urrvCjiS6PKk4V4jsbFvU77DDrLbiXLRkZtoowgsusAmPicS7HJ0rxKuvwsKFMG5c9HZIdMll1CjbVGvLll0vmzbZde/eYUcYfZ5UnItg7Vrb++Tss62D3rnSWrHChp8fcgg0bWr9JrVr512aNIHWrcOOMvo8qThXwNat0LUrrFsHL76YGBPSXOV79lnbqG3aNEsqycKTinP5rFxpqw7PnWt7W5x4YtgRuXi0apVtqHXttcmVUMCTinP/b+FCuOgiq6FMnGjJxbmyGDAAtm+Hvn3DjqTy+egv54D337edGVVh1ixPKK7s1q61kV1XXglHHRV2NJUvKklFRC4QkSUikiEi90d4vIaIvBU8/qWINM73WN/g+BIROb+4MkWkSVBGRlBm9eB4VxFZLSILgstN0XhvLrHl7t7YoYNtsjVnDjRvHnZULp69+CJs3gwPPhh2JOEod1IRkSrAQKAdcCzQWUSOLXBaN2Cdqh4BDACeCp57LNAJOA64AHhFRKoUU+ZTwICgrHVB2bneUtXmwWVYed+bS3z33GM77V12GXzyCRx8cNgRuXi2caP1xV16aezv0FhRolFTaQlkqOpSVd0BjAU6FDinAzAquD0BOE9EJDg+VlUzVfUnICMoL2KZwXPODcogKPPSKLwHl4T+9z9r+77xRhg/3oZ5OlceAwfC+vU2HD1ZRSOpHAL8lu/+suBYxHNUNQvYANQt4rmFHa8LrA/KiPRaV4jIVyIyQUQOixSsiHQXkXQRSV+9enXJ36VLOB98YNd33mk7OTpXHlu22DL27dpBixZhRxOeRPpXeh9orKonAtPIqxntQlWHqmqKqqbUq1evUgN0seX9920JlmRtpnDRNWSILRKZzLUUiE5SWQ7krxUcGhyLeI6IVAX2Af4o4rmFHf8D2DcoY5fXUtU/VDUzOD4MSOLvCq4427fbpLSLLvLJja78tm+HZ56Bc8+F008PO5pwRSOp/BdoFozKqo51vKcVOCcN6BLc7gh8pKoaHO8UjA5rAjQD5hRWZvCc/wRlEJT5HoCINMj3epcAi6Pw3lyC+uQTa6646KKwI3GJYPhw66NL9loKRGHyo6pmiUhPYApQBRihqt+ISD8gXVXTgOHAGyKSAazFkgTBeeOAb4Es4HZVzQaIVGbwkvcBY0XkcWB+UDZALxG5JChnLdC1vO/NJa5Jk6BWLd90y5XfypXw1FM2z+mcc8KOJnxiX/6TU0pKiqanp4cdhqtkqrZ0xgknQFrBOrVzpZCWZqMHt26FKVPgzDPDjqhyiMhcVU2J9FgiddQ7VyLffmv7zV98cdiRuHi1dattAdyhAzRsCPPmJU9CKY4nFZd03n/frtu3DzcOF5/mz4eTT7bRXn36wOzZthqDM55UXNKZNMk+FA4pOJvKuSLk5Nhy9qeeaptsTZ9ufSnVq4cdWWzxpOKSypo18MUXPurLld4LL8C991qz6Vdf2fBhtztf+t4llcmT7RunJxVXGjk5tvLwmWfChAk+t6koXlNxSWXSJDjooOReRsOV3owZsHSpdc57QimaJxWXNHbutJrKhRf6Wl+udIYMgQMOgMsvDzuS2Of/Wi5pzJoFGzZ405crnZUrbSfQrl2hRo2wo4l9nlRc0pg0yUbqtG4ddiQunowYAdnZ0L172JHEB08qLmlMmmTLsuy1V9iRuHiRnQ2vvmojvZo1Czua+OBJxSWF77+3izd9udKYOhV++QVuuSXsSOKHJxWXFP79b7v2pOJKY8gQOPBA2x7YlYwnFZcUJk2C44+Hxo3DjsTFi+XL7e/mxht91nxpeFJxCW/DBvj0U6+luNIZPtz6VG6+OexI4osnFZfwZsyArCxfQNKVXFaWddC3bWvbJLiS86TiEt7MmVCzpi0E6FxJfPghLFvmHfRl4UnFJbxZsyyheLu4K6khQ2w5H99zp/Q8qbiEtnmz7X/hGyi5kvr1V6updOsG1aqFHU388aTiEtrs2dbZesYZYUfi4sWrr9qW095BXzaeVFxCmznTFo9s1SrsSFw8eO89ePppuOQSaNQo7GjikycVl9BmzYI//Qnq1Ak7Ehfrxo+Hjh3hpJPgtdfCjiZ+eVJxCWvnTmv+8v4UV5w334ROneC002xpln33DTui+OVJxSWs+fNh61bvT3FFGzECrr8ezjnH9tvxWm35eFJxCWvWLLv2pOIKM2iQjfI6/3xbkqV27bAjin+eVFzCmjkTDj8cGjQIOxIXi55/Hm67zeaiTJwItWqFHVFi8KTiEpKq1VS8P8VFMm0a3H03XHEFTJjgOzpGkycVl5CWLIE1a7zpy+1u+3aroTRrZh30vtJCdFUNOwDnKoL3p7jCPPEEZGRYbaVmzbCjSTxeU3EJaeZMqFcPjjwy7EhcLFmyBJ58Eq6+Glq3DjuaxORJxSWkWbOsliISdiQuVqhas1etWtC/f9jRJC5PKi7hrFgBS5d6J73bVWoqfPSR1VTq1w87msTlScUlHO9PcQWtWwe9e9sWCN27hx1NYvOOepdwZs60SWwnnRR2JC5W9O1rowGnTLEFRl3F8R+vSzizZtkaTlX9K5MDvvjCNt26805o3jzsaBKfJxWXUDZsgIULvT/Fmaws6NEDDj0UHnss7GiSQ1SSiohcICJLRCRDRO6P8HgNEXkrePxLEWmc77G+wfElInJ+cWWKSJOgjIygzOrFvYZLHl98YaN8vD8ludx7rw0fb9zYluWpWxf22stGen31Fbz4Iuy9d9hRJodyNxCISBVgINAGWAb8V0TSVPXbfKd1A9ap6hEi0gl4CrhKRI4FOgHHAQcD00Ukd2ZBYWU+BQxQ1bEiMjgoe1Bhr1He9+fiy8yZUKWKNX+55LB6NQwYYE1bxx1nS65Ur27XNWrAMcfApZeGHWXyiEarc0sgQ1WXAojIWKADkD+pdAAeDW5PAF4WEQmOj1XVTOAnEckIyiNSmSKyGDgXuDo4Z1RQ7qDCXkNVNQrv0cWJWbPg5JN9tdlkMm6cbRk9ciSccELY0bhoNH8dAvyW7/6y4FjEc1Q1C9gA1C3iuYUdrwusD8oo+FqFvcYuRKS7iKSLSPrq1atL9UZdbMvMhC+/9P6UZJOaCscf7wklViRdR72qDlXVFFVNqVevXtjhuCiaO9cSi/enJI+ffoLPP4drrgk7EpcrGkllOXBYvvuHBsciniMiVYF9gD+KeG5hx/8A9g3KKPhahb2GSxIzZ9q1J5XkMWaMXXfuHG4cLk80ksp/gWbBqKzqWMd7WoFz0oAuwe2OwEdBX0ca0CkYudUEaAbMKazM4Dn/CcogKPO9Yl7DJQFVeOstawLxCmhyUIXRo+1LRKNGYUfjcpW7o15Vs0SkJzAFqAKMUNVvRKQfkK6qacBw4I2gI34tliQIzhuHdepnAberajZApDKDl7wPGCsijwPzg7Ip7DVccpgyxfakHz68+HNdYvjqK/j2W3jllbAjcflJMn+ZT0lJ0fT09LDDcFFw1lnWvv7jj77pUrLo08eGEq9cCQccEHY0yUVE5qpqSqTHfCELF/dmzbL+lOef94SSLHJyrD/l/PM9ocSapBv95RLPE0/YB8tNN4Udiasss2bBsmU+6isWeVJxcW3BAvjgA1ss0Cc8Jo/Ro+33fcklYUfiCvKk4uLak0/amk633x52JK6y7NgB48fb0iv+RSL2eFJxceuHH+zD5dZbYb/9wo7GVZbJk23TrauvLv5cV/k8qbi49fTTUK0a3H132JG4ypSaan1obdqEHYmLxJOKi0vLl8OoUXDjjXDQQWFH4yrLpk2QlgZXXmlfKFzs8aTi4lL//jas9N57w47EVaaJE2HbNm/6imWeVFzc+eMP2x62c2do0iTsaFxlGj3aNuI6/fSwI3GF8aTi4s5LL8GWLXD/bnuMukS2YgVMn25fJkTCjsYVxpOKiyvbt1tSueQS2+XPJY8nnrBrn+Qa2zypuLgycSKsXQt33BF2JK4y/forDB1qAzOaNg07GlcUTyourrz2Ghx2GJx7btiRuMr0z3/a9UMPhRuHK54nFRc3li2DqVOhSxfYw/9yk8bSpTBiBNx8MzRsGHY0rjj+r+nixuuv28ZMXbuGHYmrTP36QdWq8MADYUfiSsKTiosLqjBypO2bcvjhYUfjKsuSJfDGG7YUz8EHhx2NKwlPKi4ufPYZZGTADTeEHYmrTI89BjVr+vDxeOJJxcWFkSNtRdqOHcOOxFWWRYtg7Fgb6XfggWFH40rKk4qLeVu2wLhxtt7TXnuFHY2rLI8+ar9vX4onvnhScTHv7bdh82Zv+komCxbY7/2uu6Bu3bCjcaXhScXFvJEj4Ygj4Iwzwo7EVZaHH4Z994XevcOOxJWWJxUX05YuhY8/tmHEvt5TcvjiC3j/ffjb3yyxuPjiScXFtFGjLJlcf33YkbjKMHs2XHSRDR/u1SvsaFxZeFJxMSsnx5JK69a2NItLbB9+aMvv7LcfzJwJe+8ddkSuLDypuJj18cfwyy/eQZ8M3njDVp4++mibk+SLRsYvTyouZo0cCfvsA5deGnYkriI995w1b551ln2RqF8/7IhceXhScTFpwwYbUtq5M9SqFXY0riKoQp8+1iHfsSN88AHUqRN2VK68PKm4mDRhgu1F7otHJq5eveCZZ+C222zmfI0aYUfkoqFq2AE4F0lqKjRrBi1bhh2JqwjffQcDB1pCefllHy6eSLym4mLO8uXwn//ANdf4h02ievppWyjy0Uf9d5xoPKm4mDN2rLW3X3112JG4ivDbb/Dmm7bXfL16YUfjos2Tios5o0fDKadY85dLPP3725eGe+4JOxJXETypuJiyeDHMn29NXy7xrFkDQ4daLbRRo7CjcRXBk4qLKamptv/8VVeFHYmrCC+/DFu32lBil5jKlVREZH8RmSYiPwTX+xVyXpfgnB9EpEu+4y1E5GsRyRCRF0Wsy66wcsW8GJz/lYicnK+sbBFZEFzSyvO+XDhULamcdx4cdFDY0bho27wZXnwROnSA444LOxpXUcpbU7kfmKGqzYAZwf1diMj+wCPAqUBL4JF8yWcQcDPQLLhcUEy57fKd2z14fq5tqto8uFxSzvflQjB7tq1K7E1fienVV2HdOt8aONGVN6l0AEYFt0cBkRbUOB+YpqprVXUdMA24QEQaAHVUdbaqKvB6vucXVm4H4HU1s4F9g3JcAkhNtWGml10WdiQu2jIzbTmWc86B004LOxpXkcqbVOqr6srg9v+ASKv2HAL8lu/+suDYIcHtgseLKrewsgBqiki6iMwWEV8tKs7s3AlvvQUXX+xLdSSi0aNt/lHfvmFH4ipasTPqRWQ6EKmF+8H8d1RVRUSjFVgZym2kqstFpCnwkYh8rao/FjxJRLpjTWc0bNgwytG6spo+HVav9qavRJSdDU89BSedBG3ahB2Nq2jFJhVVbV3YYyLyu4g0UNWVQTPUqginLQfOyXf/UODj4PihBY4vD24XVu5y4LBIz1HV3OulIvIxcBKwW1JR1aHAUICUlJSoJ0FXNqNH2z4a7dqFHYmLtokT4fvvYdw4nz2fDMrb/JUG5I7m6gK8F+GcKUBbEdkv6KBvC0wJmrc2ishpwaiv6/M9v7By04Drg1FgpwEbgsSzn4jUABCRA4A/A9+W8725SrJli33wdOwI1auHHY2LJlV44gmbyHr55WFH4ypDeReUfBIYJyLdgF+AKwFEJAXooao3qepaEfkH8N/gOf1UdW1w+zbgNaAW8GFwKbRc4AOgPZABbAVyt286BhgiIjlYonxSVT2pxIm0NEss3vSVeF55BebOtZFfVaqEHY2rDGIDr5JTSkqKpqenhx1G0rvoIli40HZ53MOn4yaMjz6Ctm2hfXurifrvNnGIyFxVTYn0mP+aXajWrIEpU2wzLv/QSRxLl8Jf/wpHHWWLR/rvNnn4r9qFavx4yMrypq9EsmmT7TevCu+950PEk41v0uVClZoKxx4LJ54YdiQuGnJy4LrrbBOuyZPhiCPCjshVNq+puND88gvMmuWbcSWSRx6x2kn//tC60MkILpF5UnGhGTvWrjt3DjcOFx3jxsHjj8ONN8Idd4QdjQuLJxUXmjFjbB2oJk3CjsSV18KF0LUrnH66DSP2mmfy8qTiQvHNN/ZB5FsGxz9V6NnTOuTfeQdq1Ag7Ihcm76h3oRgzxoaZXnll8ee62DZlivWNDRwI9SMtKeuSitdUXKVTtaRy3nn+IRTvVOHBB6FxY7jpprCjcbHAk4qrdHPm2OQ4b/qKf++8A/Pm2agvX7fNgScVF4LUVGt398244lt2Nvz973D00XDttWFH42KF96m4SpWVZZtxXXQR7LNP2NG48khNhcWLbShxVf8kcQGvqbhK9Z//wO+/+9yUeLdjBzz6KDRvDldcEXY0Lpb49wtXqcaMsaGn7duHHYkrjxEjrF/s3//2xSLdrvzPwVWa7dvh7bdts6ZatcKOxpXVtm3wj3/YREffqdMV5Fos/A0AABBISURBVDUVV2k++AA2bvSmr3g3aBCsWGFbQPvMeVeQJxVXacaMgQMPhHPPDTsSV5QffoC777bf1Z//bDWSo4+2BLJpk20P3KYNnHNO2JG6WORJxVWKjRvh/fehe3cfKRTLfv8dzj8f/vgDqlWDkSPt+P77W3LZYw/bWO3xx8ON08Uu//d2leLddyEz0yc8xrJNm2wAxe+/2yi9U06B77+Hzz6Dzz+36+++g44doWXLsKN1scqTiqsUqam2GvGpp4YdiYtk507b/nfhQtsPJTdpHHWUXW680e6vXw+1a4cXp4t9PvrLVaicHLj/fpg6Fa6/3jt2Y5GqNUtOmQKDB8OFFxZ+7r77WrOYc4XxmoqrMJs22fIdaWnQo4ctPOhiz8MPw2uv2WRGXxTSlZfXVFyF+OUXGzk0aRK89JJt3OTfcCvO+vXwr39ZJ3ppDB5sne433WTJxbny8pqKi7rPPrPFInfsgA8/hLZtw44oseX2h0yfDtOm2aUkI+zeew9uv9065wcN8qZJFx1eU3FRNWqUzUPZZx+YPdsTSkVTtf3gp0+3kXUffwx9+hT/vM8+g06doEULXxDSRZcnFRc1/fvbPuVnnAFffmkT5lzFGjAAhgyxwRCjR1uCGTDARtsVZvFiuPhiOOwwW7vLR3O5aPKk4qLilVfgnnusGWbyZJss5ypWWhr87W+2SvA//2nHnnsOzjzT+kgWLNj9OcuX2+TG6tVttFe9epUbs0t8nlRcuY0caW3zl1xi35a9Q77izZtna6ilpMDrr+etFFytGowfb0n98sttZnyu9ettAcj1662vq0mTcGJ3ic2TiiuXMWOgWzfrO3nrLU8olWH5cmu+qlvXOtv33HPXx+vXt9Wgly+3xJOdbStEX3qpzYh/5x046aRwYneJz7vnXJm9+y5cdx2cdZbdrlkz7IgS34YNllA2brTO9gYNIp936qnWJHnTTdC3L/z8M3zyidUkW7eu1JBdkvGk4srkww/hqqtsfaj339/923I8WLYMnn0WjjjCJmfG0giorVvh22/t8s03ebd/+smG/qalwYknFl1Gt27w3//CM8/Y/Wef9bXXXMWLoX8jFy9mzLD2+uOPt+Sy995hR1Q6O3bA889Dv37WLJSdbSOoXn4Zzj678OetW2eTBdPSoFUr6yBv1Sp6Ox+uWWNlT5xoy9pkZtrxatVs/a2UFFvq5rzzbIRdSbzwgpV7wgk2kMK5CqeqSXtp0aKFutJ5913VGjVUjz9edfXqsKMpvRkzVI8+WhVUO3RQ/ekne0+NGtmxzp1Vly3b9Tk//6x6552qtWvbOc2bq1avbrcPPli1Z0/Vjz9Wzcoq+rVzclR37FDdskV1wwbVP/5Q/f571QEDVM8+W3WPPazMhg3t9d5+W/W771R37qygH4ZzZQSkayGfq6F/sId58aSimpGh+txzqmvXFn/usGH2wXfaaapr1lR8bNG0bJlqp072F9+0qeqkSbs+vnWr6iOPWMKsXVv1ySdVZ8+251Spolq1qup116kuXGjnb9igOnq06uWXq9asaeUeeKBqixaqxx5rr9Gggep++6nWqpWXMAq7HH+86t//rjpvniUf52JZUUlF7PHklJKSounp6WGHEYqdO62NPbcJqF49u3/ddbsv16EKTz9tE+zOP99GFsXLhLmff7ZmrSFD7D337WszzmvVinz+0qXQu7eNqgJr2rvlFujVyyYLRrJ5szUDTpxoHek1a9qlVq286+rVrRmratVdL7Vq2QoERxxRIW/fuQohInNVNSXig4Vlm5JcgP2BacAPwfV+hZzXJTjnB6BLvuMtgK+BDOBF+P8kF7Fc4GjgCyAT+FuB17gAWBKUdX9J4k/Wmsrnn9s3Y7Bv2lOnqrZqZffPPFP166/zzs3OVr3nHv3/pqHMzOjGsm6dxbNiRfS+oefkqM6apdqxo9UQqlSxGkdGRsnLmDpVddAg1fXroxOTc4mEimr+Ap7O/QAH7geeinDO/sDS4Hq/4HZukpgDnAYI8CHQrqhygQOBU4B/5k8qQBXgR6ApUB1YCBxbXPzJllTWr1e99VZVEdVDD1V97728x7KzrXlr//2tqefee+0Dv0sX+yvp2dPOiabvvlM97LC8JqA997Rk16GDau/eqsOHW7NUSe3YYU1Sp5xi5e27r2qfPqq//hrduJ1LdhWZVJYADYLbDYAlEc7pDAzJd39IcKwB8F2k84orF3i0QFJpBUzJd78v0Le4+JMpqUyebG38e+yhetddqhs3Rj5v9WrVbt3sL6NGDbvu1y/67fzz56vWq2eX0aNVX3rJ4rr4YuuTyO2nOPxw1SlTii4rJ0c1LU21WTN7zpFHqg4cqLp5c3Rjds6ZopJKeYcU11fVlcHt/wH1I5xzCPBbvvvLgmOHBLcLHi9pucW9RsSNa0WkO9AdoGHDhsUUmxjmz7el6A8/3IaspuRrCV292voCunSxdv8DDoBhw2z72IcftrkoN98c3Xg+/9yWW69Tx5ZpP+qo3c/JybGhy7ffbv04V11lCyUWnOz31VfWBzJjhpUzcaJNDozWMF/nXOkU+68nItNFZFGES4f85wXZK+q9/tEuV1WHqmqKqqbUS4LV9FatsuU56ta15dHzJ5TNm20tqO7dbYfG7Oy8x04/3c6PdkKZNg3atIEDD4RZsyInFLCk0KaNJY3HHrNkcfTR1umenW3vq0cPW25k3jx48UX4+mvo0METinNhKvbfT1Vbq+rxES7vAb+LSAOA4HpVhCKWA/nHzRwaHFse3C54nBKWW5LXSGo7dkDHjvYBPHGirQmVa+dOe2zBAptQN368JZCcnLK91rx5tj/HPvvAX/5iq+bOng1ZWXnnvPMOXHSRjXSaORNKUlGsWdNqTF9/bUuP3HGHJZJmzaxG1bMnZGTYcV93zLkYUFi7WEkuwDPs2qH+dIRz9gd+wjrp9wtu76+RO+rbl6Rcdu9TqYoNAGhCXkf9ccXFn+h9Kj16WB9Dauqux3NyVK+/3h4bNsyOPfyw3e/Vq+T9Jzk5qtOmqbZubc+tU8fmcjRvntf5XqeO6iWX2AiyPfawUWYlmRNT2OuNGWN9JhdeqPrtt2UrxzlXPlRgR31dYAY29Hd6vmSRAgzLd96N2FDfDOCGfMdTgEXYyK2XyRtSXFi5B2H9JRuB9cHtOsFj7YHvg7IeLEn8iZxUBg2y3+599+3+2AMP2GOPPZZ3LCfHOspB9aGHii47K0v1rbdsoh+oHnSQTRbMP/x21So7p3t3mwgIlnw2bYrO+3POhafCkkq8XxI1qXz6qQ0Lbtdu96VDBg6033r37rvXSHJy8kZ+PfXU7o/NnWtDfQ8+2M5p1kx16FDVbduKj2nVKp8p7lyiKCqp+IKSCebXX22hw6ZNbUvZKlXyHnv3XeuDuPhiGDhw95nzIjbzfPNmuO8+G53Vtq2VM3q07cVRrZp17nftapty5S+/KEkwJsI5h69SHLOys21plNwO6latoHlzW9pjxw7bTyMjA77/HubOhTfftB39cq1fb1v71q9vl733hieftLLGji18mfcqVeCNN2DLFrj11rzjZ50Fd91lnft161bse3fOxS9PKjEmO9uWPW/fPu/YmDGlK+Pww21Z+t9/t8Tz+++wbRscd1zJ9j7J3ZL2vvvg4INt98AkmdLjnCsnTypl9MsvVgOIxm6Hy5fD5MkwZYp9mOfXrZvtMz5vnjVtFXT44fahf9ZZ0LKlDektSBU2bbJFIEvaXFWzpu3F4ZxzpeFJpQyysqwmUKsW3HabNRPVL27OfwSLFsETT1hzVMH5IZMmwYUXRn7e9u2wYgU0blyyiX4i1j/inHMVzecel0HVqtbfsXq1zfZu1MhqFIsWlez56em2bMoJJ9gS63feaZ3fAH/+M6xcWXhCAatFNG3qM8edc7HHaypldOut1v9xxx227esbb8CIEba0yGWXwb77Wud4nTp5l19/tZrJ1KlWRuPGlkxmz4YvvrAyn3/e1uByzrl45EmlHHr2tMRy111w5pm2v/ngwba+VUn8/DMMHWq1juHDbRFH55yLZ55UyunOO60/pHdvW+H3p59g7Vob8vv99/D44zBnTt75N9wAJ55oa1cdeaTVVnzNKudcovCkEgV33201lnvvtX6Ov//dmrFGjcpbuPHee210lnPOJTJPKlHyt79ZjeW++2w0V40a1pzVu7fVSpxzLhl4UomiPn1snsjKlWUfZuycc/HMk0qU3XJL2BE451x4fKaDc865qPGk4pxzLmo8qTjnnIsaTyrOOeeixpOKc865qPGk4pxzLmo8qTjnnIsaTyrOOeeiRlQ17BhCIyKrgV8q+GUOANZU8GtUpHiOP55jh/iOP55jB4+/OI1UtV6kB5I6qVQGEUlX1ZSw4yireI4/nmOH+I4/nmMHj788vPnLOedc1HhScc45FzWeVCre0LADKKd4jj+eY4f4jj+eYwePv8y8T8U551zUeE3FOedc1HhScc45FzWeVCqIiPxVRL4RkRwRScl3vLGIbBORBcFlcJhxRlJY7MFjfUUkQ0SWiMj5YcVYUiLyqIgsz/fzbh92TMURkQuCn2+GiNwfdjylJSI/i8jXwc87Pex4iiMiI0RklYgsyndsfxGZJiI/BNf7hRljYQqJPdS/eU8qFWcRcDnwaYTHflTV5sGlRyXHVRIRYxeRY4FOwHHABcArIlKl8sMrtQH5ft4fhB1MUYKf50CgHXAs0Dn4ucebvwQ/73iY6/Ea9vec3/3ADFVtBswI7sei19g9dgjxb96TSgVR1cWquiTsOMqiiNg7AGNVNVNVfwIygJaVG13CawlkqOpSVd0BjMV+7q6CqOqnwNoChzsAo4Lbo4BLKzWoEiok9lB5UglHExGZLyKfiMiZYQdTCocAv+W7vyw4Fut6ishXQVNBTDZj5BOvP+P8FJgqInNFpHvYwZRRfVVdGdz+H1A/zGDKILS/eU8q5SAi00VkUYRLUd8sVwINVfUkoDeQKiJ1KifiPGWMPSYV814GAYcDzbGf/XOhBpsczlDVk7EmvNtF5KywAyoPtXkX8TT3ItS/+aqV+WKJRlVbl+E5mUBmcHuuiPwIHAlUaodmWWIHlgOH5bt/aHAsVCV9LyLyKjCpgsMpr5j8GZeGqi4PrleJyLtYk16kvsVY9ruINFDVlSLSAFgVdkAlpaq/594O42/eayqVTETq5XZui0hToBmwNNyoSiwN6CQiNUSkCRb7nJBjKlLwgZDrMmwQQiz7L9BMRJqISHVsYERayDGVmIjUFpG9c28DbYn9n3kkaUCX4HYX4L0QYymVsP/mvaZSQUTkMuAloB7wbxFZoKrnA2cB/URkJ5AD9FDVmOpoKyx2Vf1GRMYB3wJZwO2qmh1mrCXwtIg0x5ovfgZuCTecoqlqloj0BKYAVYARqvpNyGGVRn3gXREB+3xJVdXJ4YZUNBEZA5wDHCAiy4BHgCeBcSLSDdse48rwIixcIbGfE+bfvC/T4pxzLmq8+cs551zUeFJxzjkXNZ5UnHPORY0nFeecc1HjScU551zUeFJxzjkXNZ5UnHPORc3/AU2IB7bqbI03AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "i = np.random.randint(0,num_pix_1d,2)\n",
        "plt.plot(V, proj_nd_shifted_transposed[i[0], i[1], :, 3],'blue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-DZrvCavZ7Q"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0OFyC2DylqN"
      },
      "outputs": [],
      "source": [
        "real_loops = clean_interpolate(proj_nd_shifted_transposed[:, :, :, 3].reshape(num_pix,-1)).astype(np.float64)\n",
        "\n",
        "real_loops_scaler = global_scaler()\n",
        "\n",
        "real_scaled_loops = real_loops_scaler.fit_transform(real_loops).astype(np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1spu3xjysDG"
      },
      "outputs": [],
      "source": [
        "real_parms_scaler = StandardScaler()\n",
        "real_parms_scaled = real_parms_scaler.fit_transform(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7bYEwndy8Ch"
      },
      "outputs": [],
      "source": [
        "params_mean = real_parms_scaler.mean_\n",
        "params_std = np.sqrt(real_parms_scaler.var_)\n",
        "\n",
        "data_mean = real_loops_scaler.mean.astype(np.float64)\n",
        "data_std = real_loops_scaler.std.astype(np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N_Bd-qby2uO"
      },
      "source": [
        "## Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsMjUdsS-Rw2"
      },
      "outputs": [],
      "source": [
        "class TRPCGOptimizerv2:\n",
        "\n",
        "    cgopttol = 1e-7\n",
        "    c0tr = 0.2\n",
        "    c1tr = 0.25\n",
        "    c2tr = 0.75  # when to accept\n",
        "    t1tr = 0.75\n",
        "    t2tr = 2.0\n",
        "    radius_max = 5.0  # max radius\n",
        "    radius_initial = 1.0\n",
        "    radius = radius_initial\n",
        "\n",
        "    @tf.function\n",
        "    def computeHessianProduct(self, x, y, v):\n",
        "        with tf.GradientTape() as tape:\n",
        "            with tf.GradientTape() as tape2:\n",
        "                out = self.model(x)\n",
        "                loss = tf.keras.losses.mean_squared_error(out, y)\n",
        "                loss = tf.reduce_mean(loss)\n",
        "            grad = tape2.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "            gradSum = tf.reduce_sum([tf.reduce_sum(g*p0i)\n",
        "                                     for g, p0i in zip(grad, v)])\n",
        "        Hp = tape.gradient(gradSum, self.model.trainable_variables)\n",
        "        return Hp\n",
        "\n",
        "    def __init__(self, model, radius, precondition,\n",
        "                 cgopttol=1e-7, c0tr=0.0001, c1tr=0.1, c2tr=0.75, t1tr=0.25, t2tr=2.0, radius_max=2.0,\n",
        "                 radius_initial=0.1):\n",
        "\n",
        "        self.model = model\n",
        "        self.cgopttol = cgopttol\n",
        "        self.c0tr = c0tr\n",
        "        self.c1tr = c1tr\n",
        "        self.c2tr = c2tr\n",
        "        self.t1tr = t1tr\n",
        "        self.t2tr = t2tr\n",
        "        self.radius_max = radius_max\n",
        "        self.radius_initial = radius_initial\n",
        "        self.radius = radius\n",
        "        self.cgmaxiter = sum([tf.size(w).numpy()\n",
        "                              for w in self.model.trainable_weights])\n",
        "        self.d = self.cgmaxiter\n",
        "        self.cgmaxiter = min(120, self.cgmaxiter)\n",
        "        self.iterationCounterForAdamTypePreconditioning = 0\n",
        "        self.precondition = precondition\n",
        "        if self.precondition != 0:\n",
        "            self.DiagPrecond = [w.data*0.0 for w in self.model.parameters()]\n",
        "            self.DiagScale = 0.0\n",
        "\n",
        "    def findroot(self, x, p):\n",
        "        aa = 0.0\n",
        "        bb = 0.0\n",
        "        cc = 0.0\n",
        "        for e in range(len(x)):\n",
        "            aa += tf.reduce_sum(p[e]*p[e])\n",
        "            bb += tf.reduce_sum(p[e]*x[e])\n",
        "            cc += tf.reduce_sum(x[e]*x[e])\n",
        "        bb = bb*2.0\n",
        "        cc = cc - self.radius**2\n",
        "        alpha = (-2.0*cc)/(bb + tf.sqrt(bb**2-(4.0*aa*cc)))\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def computeListNorm(self, lst):\n",
        "        return np.sum([tf.reduce_sum(ri*ri) for ri in lst])**0.5\n",
        "\n",
        "    def computeListNormSq(self, lst):\n",
        "        return np.sum([tf.reduce_sum(ri*ri) for ri in lst])\n",
        "\n",
        "    def computeDotProducts(self, u, v):\n",
        "        return tf.reduce_sum(tf.stack([tf.reduce_sum(ui * vi) for ui, vi in zip(u, v)], 0))\n",
        "\n",
        "    def normOfVar(self, x):\n",
        "        return tf.sqrt(self.computeDotProducts(x, x))\n",
        "\n",
        "    def CGSolver(self, loss_grad, x, y):\n",
        "        cg_iter = 0  # iteration counter\n",
        "        x0 = [w.numpy()*0.0 for w in self.model.trainable_weights]\n",
        "        if self.precondition == 0:\n",
        "            r0 = [i+0.0 for i in loss_grad]  # set initial residual to gradient\n",
        "            normGrad = self.normOfVar(r0)\n",
        "            # set initial conjugate direction to -r0\n",
        "            p0 = [-i+0.0 for i in loss_grad]\n",
        "            self.cgopttol = self.computeListNormSq(loss_grad)\n",
        "            self.cgopttol = self.cgopttol**0.5\n",
        "            self.cgopttol = (min(0.5, self.cgopttol**0.5))*self.cgopttol\n",
        "        else:\n",
        "            r0 = [(i.data+0.0)*pr.data for i,\n",
        "                  pr in zip(loss_grad, self.SquaredPreconditioner)]\n",
        "            p0 = [-(i.data+0.0)*pr.data for i,\n",
        "                  pr in zip(loss_grad, self.SquaredPreconditioner)]\n",
        "            self.cgopttol = self.computeListNormSq(r0)\n",
        "            self.cgopttol = self.cgopttol.data.item()**0.5\n",
        "            self.cgopttol = (min(0.5, self.cgopttol**0.5))*self.cgopttol\n",
        "\n",
        "        cg_term = 0\n",
        "        j = 0\n",
        "\n",
        "        while 1:\n",
        "            j += 1\n",
        "            self.CG_STEPS_TOOK = j\n",
        "            # if CG does not solve model within max allowable iterations\n",
        "            if j > self.cgmaxiter:\n",
        "                j = j-1\n",
        "                p1 = x0\n",
        "                print('\\n\\nCG has issues !!!\\n\\n')\n",
        "                break\n",
        "            # hessian vector product\n",
        "            if self.precondition == 0:\n",
        "                Hp = self.computeHessianProduct(x, y, p0)\n",
        "            else:\n",
        "                loss_grad_direct \\\n",
        "                    = np.sum([(gi*(si*pr.data)).sum() for gi, si, pr in zip(loss_grad, p0, self.SquaredPreconditioner)])\n",
        "                Hp = torch.autograd.grad(loss_grad_direct, self.model.parameters(\n",
        "                ), retain_graph=True)  # hessian-vector in tuple\n",
        "                Hp = [g*pr.data for g,\n",
        "                      pr in zip(Hp, self.SquaredPreconditioner)]\n",
        "\n",
        "            pHp = tf.reduce_sum([tf.reduce_sum(Hpi*p0i)\n",
        "                                 for Hpi, p0i in zip(Hp, p0)])\n",
        "\n",
        "            # if nonpositive curvature detected, go for the boundary of trust region\n",
        "            if pHp <= 0:\n",
        "                tau = self.findroot(x0, p0)\n",
        "                p1 = [xi+tau*p0i for xi, p0i in zip(x0, p0)]\n",
        "                cg_term = 1\n",
        "                break\n",
        "\n",
        "            # if positive curvature\n",
        "            # vector product\n",
        "            rr0 = self.computeListNormSq(r0)\n",
        "            # update alpha\n",
        "            alpha = (rr0/pHp)\n",
        "\n",
        "            x1 = [xi+alpha*pi for xi, pi in zip(x0, p0)]\n",
        "            norm_x1 = self.computeListNorm(x1)\n",
        "\n",
        "            if norm_x1 >= self.radius:\n",
        "                tau = self.findroot(x0, p0)\n",
        "\n",
        "                p1 = [xi+tau*pi for xi, pi in zip(x0, p0)]\n",
        "                cg_term = 2\n",
        "                break\n",
        "\n",
        "            # update residual\n",
        "            r1 = [ri+alpha*Hpi for ri, Hpi in zip(r0, Hp)]\n",
        "            norm_r1 = self.computeListNorm(r1)\n",
        "\n",
        "            if norm_r1 < self.cgopttol:\n",
        "                p1 = x1\n",
        "                cg_term = 3\n",
        "                break\n",
        "\n",
        "            rr1 = self.computeListNormSq(r1)\n",
        "            beta = (rr1/rr0)\n",
        "\n",
        "            # update conjugate direction for next iterate\n",
        "            p1 = [-ri+beta*pi for ri, pi in zip(r1, p0)]\n",
        "\n",
        "            p0 = p1\n",
        "            x0 = x1\n",
        "            r0 = r1\n",
        "\n",
        "        cg_iter = j\n",
        "        if self.precondition != 0:\n",
        "            p1 = [pi*pr.data for pi, pr in zip(p1, self.SquaredPreconditioner)]\n",
        "\n",
        "        d = p1\n",
        "\n",
        "        return d, cg_iter, cg_term\n",
        "\n",
        "    def assignToModel(self, newX):\n",
        "        for w, nw in zip(self.model.trainable_weights, newX):\n",
        "            w.assign(nw)\n",
        "\n",
        "    def addToModel(self, d):\n",
        "        for w, di in zip(self.model.trainable_weights, d):\n",
        "            w.assign_add(di)\n",
        "\n",
        "    def computeLoss(self, x, y):\n",
        "        out = self.model(x)\n",
        "        loss = tf.keras.losses.mean_squared_error(out, y)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        return loss\n",
        "\n",
        "    def computeLossAndGrad(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.computeLoss(x, y)\n",
        "        grad = tape.gradient(loss, self.model.trainable_variables)\n",
        "        return loss, grad\n",
        "\n",
        "    def step(self, x, y):\n",
        "\n",
        "        loss, grad = self.computeLossAndGrad(x, y)\n",
        "        w0 = [w.numpy()+0.0 for w in self.model.trainable_weights]\n",
        "        update = 3\n",
        "\n",
        "        while update == 3:\n",
        "            update = 2\n",
        "            # Conjugate Gradient Method\n",
        "\n",
        "            d, cg_iter, cg_term = self.CGSolver(grad, x, y)\n",
        "            Hd = self.computeHessianProduct(x, y, d)\n",
        "            dHd = tf.reduce_sum([tf.reduce_sum(Hdi*di)\n",
        "                                 for Hdi, di in zip(Hd, d)])\n",
        "            gd = tf.reduce_sum([tf.reduce_sum(gi*di)\n",
        "                                for gi, di in zip(grad, d)])\n",
        "            norm_d = self.computeListNorm(d)\n",
        "\n",
        "            denominator = -gd - 0.5*(dHd)\n",
        "            self.addToModel(d)\n",
        "            loss_new = self.computeLoss(x, y)\n",
        "            numerator = loss - loss_new\n",
        "\n",
        "            # ratio\n",
        "            rho = numerator/denominator\n",
        "\n",
        "            if rho < self.c1tr:  # shrink radius\n",
        "                self.radius = self.t1tr*self.radius\n",
        "                update = 0\n",
        "            # and np.abs(norm_d.data.item() - self.radius) < 1e-10: # enlarge radius\n",
        "            if rho > self.c2tr:\n",
        "                self.radius = min(self.t2tr*self.radius, self.radius_max)\n",
        "                update = 1\n",
        "            # otherwise, radius remains the same\n",
        "            if rho <= self.c0tr:  # reject d\n",
        "                update = 3\n",
        "\n",
        "                self.assignToModel(w0)\n",
        "\n",
        "                lossTMP, grad = self.computeLossAndGrad(x, y)\n",
        "\n",
        "                # print('rejecting .... radius: %1.6e   FVALNew %1.6e,  DeltaF %1.6e ' % (\n",
        "                #     self.radius, lossTMP, numerator))\n",
        "            if self.radius < 1e-15:\n",
        "                break\n",
        "\n",
        "        return loss, d, rho, update, cg_iter, cg_term, grad, norm_d, numerator, denominator, self.radius\n",
        "\n",
        "    def stepMAE(self, loss, MAE, Coor, AtomTypes, Grid, Label):\n",
        "\n",
        "        update = 3\n",
        "        w0 = [a.data+0.0 for a in self.model.parameters()]\n",
        "        loss_grad = torch.autograd.grad(\n",
        "            loss, self.model.parameters(), create_graph=True, retain_graph=True)\n",
        "        if self.precondition == 1:\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(di.data*self.DiagScale+(1-self.DiagScale)*gi*gi)\n",
        "                di.data[di.data == 0] += 1.0\n",
        "            self.DiagScale = 0.95\n",
        "        if self.precondition == 2:  # Martens paper\n",
        "            self.DiagScale = 0.001  # set lambda to what value?\n",
        "            self.exponent = 0.75  # based on paper\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_((gi*gi + self.DiagScale)**self.exponent)\n",
        "        if self.precondition == 3:\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(1.0-self.DiagScale+self.DiagScale*gi*gi)\n",
        "            self.DiagScale = 1e-2\n",
        "        if self.precondition == 4:\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(di.data*self.DiagScale+(1-self.DiagScale)*gi*gi)\n",
        "                di.data[di.data == 0] += 1.0\n",
        "            self.DiagScale = 0.99\n",
        "        if self.precondition == 5:\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(di.data*self.DiagScale+(1-self.DiagScale)*gi*gi)\n",
        "                di.data[di.data == 0] += 1.0\n",
        "            self.DiagScale = 0.90\n",
        "        if self.precondition == 6:\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(di.data*self.DiagScale +\n",
        "                             (1-self.DiagScale)*torch.abs(gi))\n",
        "                di.data[di.data == 0] += 1.0\n",
        "            self.DiagScale = 0.95\n",
        "\n",
        "        if self.precondition == 6:\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(di.data*self.DiagScale +\n",
        "                             (1-self.DiagScale)*torch.abs(gi))\n",
        "                di.data[di.data == 0] += 1.0\n",
        "            self.DiagScale = 0.95\n",
        "\n",
        "        if self.precondition in [7, 8, 9]:\n",
        "            if self.precondition == 7:\n",
        "                self.DiagScale = 0.99\n",
        "            if self.precondition == 8:\n",
        "                self.DiagScale = 0.95\n",
        "            if self.precondition == 9:\n",
        "                self.DiagScale = 0.90\n",
        "\n",
        "            self.iterationCounterForAdamTypePreconditioning += 1\n",
        "\n",
        "            for gi, di in zip(loss_grad, self.DiagPrecond):\n",
        "                di.data.set_(di.data*self.DiagScale +\n",
        "                             (1-self.DiagScale)*torch.abs(gi))\n",
        "                di.data[di.data == 0] += 1.0\n",
        "\n",
        "        while update == 3:\n",
        "            update = 2\n",
        "            # Conjugate Gradient Method\n",
        "            d, cg_iter, cg_term = self.CGSolver(loss_grad)\n",
        "\n",
        "            for wi, di in zip(self.model.parameters(), d):\n",
        "                wi.data.set_(wi.data+0.0+di)\n",
        "\n",
        "            # MSE loss plus penalty term\n",
        "            with torch.no_grad():\n",
        "                loss_new = Projection_Error(XData, YData, idx, n_steps)\n",
        "\n",
        "            numerator = loss.data.item() - loss_new.data.item()\n",
        "\n",
        "            loss_grad_direct = np.sum([(gi*di).sum()\n",
        "                                       for gi, di in zip(loss_grad, d)])\n",
        "\n",
        "            Hd = torch.autograd.grad(loss_grad_direct, self.model.parameters(\n",
        "            ), retain_graph=True)  # hessian-vector in tuple\n",
        "\n",
        "            dHd = np.sum([(Hdi*di).sum() for Hdi, di in zip(Hd, d)])\n",
        "\n",
        "            gd = np.sum([(gi*di).sum() for gi, di in zip(loss_grad, d)])\n",
        "\n",
        "            norm_d = self.computeListNorm(d)\n",
        "\n",
        "            denominator = -gd.data.item() - 0.5*(dHd.data.item())\n",
        "\n",
        "            # ratio\n",
        "            rho = numerator/denominator\n",
        "\n",
        "            if rho < self.c1tr:  # shrink radius\n",
        "                self.radius = self.t1tr*self.radius\n",
        "                update = 0\n",
        "            # and np.abs(norm_d.data.item() - self.radius) < 1e-10: # enlarge radius\n",
        "            if rho > self.c2tr:\n",
        "                self.radius = min(self.t2tr*self.radius, self.radius_max)\n",
        "                update = 1\n",
        "            # otherwise, radius remains the same\n",
        "            if rho <= self.c0tr:  # reject d\n",
        "                update = 3\n",
        "                for wi, w0i in zip(self.model.parameters(), w0):\n",
        "                    wi.data.set_(w0i.data)\n",
        "\n",
        "        return d, rho, update, cg_iter, cg_term, loss_grad, norm_d, numerator, denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xZ83a0jy3UX"
      },
      "outputs": [],
      "source": [
        "ACTIVATION = tf.nn.leaky_relu\n",
        "\n",
        "def Identity_Block(X, time_step, kernel_size, drop_out):\n",
        "    \n",
        "    x = Conv1D(time_step, kernel_size, padding='same')(X)\n",
        "    x = LayerNormalization(axis=1)(x)\n",
        "    x = Activation(ACTIVATION)(x)\n",
        "    x = tf.transpose(x, (0, 2, 1)) \n",
        "    x = Attention()([x,x])\n",
        "    x = tf.transpose(x, (0, 2, 1))\n",
        "    x = Dropout(drop_out)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def Conv1D_Block(X, time_step, kernel_size, drop_out):\n",
        "    x = Conv1D(time_step, kernel_size, padding='same')(X)\n",
        "    x = Activation(ACTIVATION)(x)      \n",
        "    return x\n",
        "\n",
        "def Conv1D_Pie(size = 120, time_step = 128//8, kernel_size = 3, lr = 3e-5, embedding = 8, \n",
        "                  n_step = 96, drop_out=0.2, l2_norm = 5e-6):\n",
        "    X_input = layers.Input(shape=(n_step, 1))\n",
        "    X = X_input\n",
        "    Embedding_out = layers.Input(shape=(8,))\n",
        "    \n",
        "    x = X\n",
        "    x = Conv1D_Block(x, 16, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    x = Conv1D_Block(x, 32, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "    x = Conv1D_Block(x, 16, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    \n",
        "    time_step = 8 \n",
        "    \n",
        "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    \n",
        "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    \n",
        "    encoded = Flatten()(x)\n",
        "    \n",
        "    embedding = Dense(9, activation='linear')(encoded)\n",
        "    embedding = tf.cast(embedding, dtype='float64')\n",
        "    \n",
        "    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
        "                            tf.convert_to_tensor(params_mean))\n",
        "    scaled_loops = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
        "                            tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "\n",
        "    model = Model(X_input, scaled_loops, name = 'Convolutional_1D_with_Attention')\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQQ9WJsJJKrx"
      },
      "outputs": [],
      "source": [
        "model = Conv1D_Pie()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPz9EcN9C85j"
      },
      "source": [
        "## Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHucCAj3DMsa",
        "outputId": "6c02f238-27e7-450f-9917-978bf12e5480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MANUAL SEED: 9\n",
            "Traning with batch size = 64\n",
            "Training time: 326.69265842437744 seconds\n",
            "Number of epochs: 9\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs64/assets\n",
            "Trust Region Inference time: 6.564451588524713e-05 seconds\n",
            "Adam Inference time: 9.494688775804307e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.005398969201054389\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.017673274502158165\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.6830449104309082, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.3884546756744385, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 128\n",
            "Training time: 318.10376048088074 seconds\n",
            "Number of epochs: 15\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs128/assets\n",
            "Trust Region Inference time: 9.481350580851237e-05 seconds\n",
            "Adam Inference time: 9.461853239271376e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.004111095633887535\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.013578874059021473\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.5272481441497803, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.3375916481018066, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 256\n",
            "Training time: 306.88377356529236 seconds\n",
            "Number of epochs: 29\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs256/assets\n",
            "Trust Region Inference time: 6.559696462419298e-05 seconds\n",
            "Adam Inference time: 6.656726201375325e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.004103595920914205\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.009896645322442055\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.6142799258232117, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.357056140899658, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 512\n",
            "Training time: 305.04489827156067 seconds\n",
            "Number of epochs: 63\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs512/assets\n",
            "Trust Region Inference time: 9.456952412923176e-05 seconds\n",
            "Adam Inference time: 6.626751687791613e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.004389639941292618\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.01603853702545166\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.7726337313652039, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.6187520027160645, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 1024\n",
            "Training time: 303.6442677974701 seconds\n",
            "Number of epochs: 108\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs1024/assets\n",
            "Trust Region Inference time: 9.523948033650716e-05 seconds\n",
            "Adam Inference time: 6.467488076951769e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.0038814263450492772\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.011128590442240238\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.4872352182865143, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.151737928390503, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "MANUAL SEED: 10\n",
            "Traning with batch size = 64\n",
            "Training time: 313.8427360057831 seconds\n",
            "Number of epochs: 9\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs64/assets\n",
            "Trust Region Inference time: 9.624600410461426e-05 seconds\n",
            "Adam Inference time: 9.474886788262262e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.006233044532921986\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.009515413083136082\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.8639565706253052, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(3.734093189239502, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 128\n",
            "Training time: 314.43668031692505 seconds\n",
            "Number of epochs: 17\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs128/assets\n",
            "Trust Region Inference time: 9.64359442392985e-05 seconds\n",
            "Adam Inference time: 9.452952278984917e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.0045371815228792516\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.014583582989871502\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.5535010099411011, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(1.8156057596206665, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 256\n",
            "Training time: 302.34251379966736 seconds\n",
            "Number of epochs: 30\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs256/assets\n",
            "Trust Region Inference time: 6.646010610792371e-05 seconds\n",
            "Adam Inference time: 9.437382221221923e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.004638537134459594\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.011107869446277618\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.49717894196510315, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.1780333518981934, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 512\n",
            "Training time: 301.7392988204956 seconds\n",
            "Number of epochs: 47\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs512/assets\n",
            "Trust Region Inference time: 6.661123699612088e-05 seconds\n",
            "Adam Inference time: 9.620308876037598e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.003731098055337274\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.01574772596359253\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.5001608729362488, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(1.8464099168777466, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n",
            "Traning with batch size = 1024\n",
            "Training time: 304.7229781150818 seconds\n",
            "Number of epochs: 109\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Rapid-Fitting-of-BEPFM-and-Hysteresis-Loops-Using-Physics-Constrained-Unsupervised-Neural-Networks/Benchmarking/Trained Models/Loops/model_bs1024/assets\n",
            "Trust Region Inference time: 9.417083528306749e-05 seconds\n",
            "Adam Inference time: 9.59883795844184e-05 seconds\n",
            "MSE of hysteresis loops with Trust Region CG: 0.003645371534479106\n",
            "MSE of hysteresis loops with LSQF: 0.008488220453739382\n",
            "Adam Reconstruction MSE: 0.011721043847501278\n",
            "Params Predictions MSE (scaled): tf.Tensor(0.5012620687484741, shape=(), dtype=float64)\n",
            "Params Predictions MSE (unscaled): tf.Tensor(2.2386586666107178, shape=(), dtype=float64)\n",
            "-----------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for seed in range(9, 11):\n",
        "  print(f'MANUAL SEED: {seed}')\n",
        "  tf.random.set_seed(seed)  \n",
        "  for n in range(6, 11):\n",
        "    batch_size = 2**n\n",
        "    tf.keras.backend.set_floatx('float64')\n",
        "    model = Conv1D_Pie()\n",
        "    model.compile(optimizer=Adam(0.00001), loss='mse')\n",
        "\n",
        "    FVAL = []\n",
        "    earlyPredictor = tf.keras.Model(model.inputs,model.layers[23].output)\n",
        "    embedding = real_parms_scaled\n",
        "    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
        "                        tf.convert_to_tensor(params_mean))\n",
        "    scaled_loops_ = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
        "                        tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "    \n",
        "    print(f'Traning with batch size = {batch_size}')\n",
        "\n",
        "    cgopttol = 1e-7\n",
        "    c0tr = 0.2\n",
        "    c1tr = 0.25\n",
        "    c2tr = 0.75  # when to accept\n",
        "    t1tr = 0.75\n",
        "    t2tr = 2.0\n",
        "    radius_max = 5.0  # max radius\n",
        "    radius_initial = 1.0\n",
        "    radius = radius_initial\n",
        "\n",
        "    optimizer = TRPCGOptimizerv2(model,radius_initial,0)\n",
        "    # print(\"d = \",optimizer.d)\n",
        "\n",
        "    allsamples=[i for i in range(num_pix)]\n",
        "\n",
        "    st = time.time()\n",
        "    for epoch in range(1000):\n",
        "\n",
        "        np.random.shuffle(allsamples)\n",
        "\n",
        "        BS = batch_size\n",
        "        for it in range(num_pix//BS):\n",
        "\n",
        "          x = real_scaled_loops[allsamples[it*BS:(it+1)*BS]]\n",
        "          y = real_scaled_loops[allsamples[it*BS:(it+1)*BS]]\n",
        "\n",
        "\n",
        "          loss, d, rho, update, cg_iter, cg_term, loss_grad, norm_d, numerator, denominator, rad = \\\n",
        "          optimizer.step(x,y)\n",
        "          \n",
        "          parm_pred = earlyPredictor.predict(real_scaled_loops)\n",
        "          embedding = parm_pred\n",
        "          unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
        "                              tf.convert_to_tensor(params_mean))\n",
        "          scaled_loops_DNN = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
        "                              tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "\n",
        "          \n",
        "          err = tf.reduce_mean(tf.abs(scaled_loops_DNN - real_scaled_loops)).numpy()\n",
        "          \n",
        "          FVAL.append([loss.numpy(),err])\n",
        "          # print(epoch,time.time()-st,\"sec\",FVAL[-1], cg_iter , optimizer.radius)\n",
        "          if optimizer.radius < 1e-15:\n",
        "              break\n",
        "\n",
        "        if(time.time() - st > 300):\n",
        "            print('Training time: ' + str(time.time() - st) + ' seconds')\n",
        "            print('Number of epochs: ' + str(epoch))\n",
        "            break;\n",
        "\n",
        "    model.save(f'./Trained Models/Piezoresponse/model_bs{batch_size}')\n",
        "    unscaled_param_trust = tf.identity(unscaled_param)\n",
        "    scaled_params_trust = tf.identity(embedding)\n",
        "\n",
        "    start_time_inference = time.time()\n",
        "    earlyPredictor.predict(real_scaled_loops)\n",
        "    print('Trust Region Inference time: ' + str((time.time() - start_time_inference) / real_scaled_loops.shape[0]) + ' seconds')\n",
        "\n",
        "    scaled_loops_DNN_trust = scaled_loops_DNN\n",
        "    real_scaled_loops_trust = real_scaled_loops\n",
        "    scaled_loops_trust = scaled_loops_\n",
        "\n",
        "    model = Conv1D_Pie()\n",
        "    model.compile(Adam(3e-5), loss='mse')\n",
        "    model.fit(real_scaled_loops,real_scaled_loops, batch_size=1200, epochs=1000, verbose=0)\n",
        "    earlyPredictor = tf.keras.Model(model.inputs,model.layers[23].output)\n",
        "    parm_pred = earlyPredictor.predict(real_scaled_loops)\n",
        "    embedding = parm_pred\n",
        "    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
        "                        tf.convert_to_tensor(params_mean))\n",
        "    scaled_loops_DNN = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
        "                        tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "    err = tf.reduce_mean(tf.abs(scaled_loops_DNN - real_scaled_loops)).numpy()\n",
        "\n",
        "    earlyPredictor = tf.keras.Model(model.inputs,model.layers[24].output)\n",
        "    parm_pred = earlyPredictor.predict(real_scaled_loops)\n",
        "    embedding = real_parms_scaled\n",
        "\n",
        "    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
        "                        tf.convert_to_tensor(params_mean))\n",
        "    scaled_loops_ = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
        "                        tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "    \n",
        "    embedding = parm_pred\n",
        "    unscaled_param = tf.add(tf.multiply(embedding, tf.convert_to_tensor(params_std)),\\\n",
        "                        tf.convert_to_tensor(params_mean))\n",
        "    scaled_loops_DNN = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, unscaled_param), \\\n",
        "                        tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "    \n",
        "    start_time_inference = time.time()\n",
        "    earlyPredictor.predict(real_scaled_loops)\n",
        "    print('Adam Inference time: ' + str((time.time() - start_time_inference) / real_scaled_loops.shape[0]) + ' seconds')\n",
        "  \n",
        "    errors = tf.reduce_mean(tf.abs(scaled_loops_DNN_trust - real_scaled_loops) + tf.abs(scaled_loops_DNN - real_scaled_loops), 1)\n",
        "\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    adam_error = mse(scaled_loops_DNN, real_scaled_loops).numpy()\n",
        "    trust_region_error = mse(scaled_loops_DNN_trust, real_scaled_loops).numpy()\n",
        "\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    adam_mae = mae(scaled_loops_DNN, real_scaled_loops).numpy()\n",
        "    trust_region_mae = mae(scaled_loops_DNN_trust, real_scaled_loops).numpy()\n",
        "\n",
        "    errors = np.asarray(errors)\n",
        "    adam_error = np.asarray(adam_error)\n",
        "    trust_region_error = np.asarray(trust_region_error)\n",
        "\n",
        "    unscaled_loops_lsqf = loop_fitting_function_tf(func_type, V, params)\n",
        "    scaled_loops_lsqf = tf.divide(tf.subtract(loop_fitting_function_tf(func_type, V, params), \\\n",
        "                                tf.convert_to_tensor(data_mean)), tf.convert_to_tensor(data_std))\n",
        "    \n",
        "    mse_loops_trust = np.mean(np.square((scaled_loops_DNN_trust - real_scaled_loops)), 1)\n",
        "    mse_loops_lsqf = np.mean(np.square((scaled_loops_lsqf - real_scaled_loops)), 1)\n",
        "    highest_loops_trust = (-mse_loops_trust).argsort()[:]\n",
        "    highest_loops_lsqf = (-mse_loops_lsqf).argsort()[:]\n",
        "    print('MSE of hysteresis loops with Trust Region CG: ' + str(np.mean(mse_loops_trust)))\n",
        "    print('MSE of hysteresis loops with LSQF: ' + str(np.mean(mse_loops_lsqf)))\n",
        "\n",
        "    print('Adam Reconstruction MSE: ' + str(adam_error))\n",
        "    # print('Trust Region Reconstruction CG MSE: ' + str(trust_region_error))\n",
        "    print('Params Predictions MSE (scaled): ' + str(mse(scaled_params_trust, real_parms_scaled)))\n",
        "    print('Params Predictions MSE (unscaled): ' + str(mse(unscaled_param_trust, params)))\n",
        "    print('-----------------------------')\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kliyRC2f4DNa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPKnJUsvOk+3GhmdNBfg/be",
      "include_colab_link": true,
      "name": "Hysteresis Loops TRCG.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c1715a88d06923266e8248b82b3e765c4bf779c2d6765688e4035450030da9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
